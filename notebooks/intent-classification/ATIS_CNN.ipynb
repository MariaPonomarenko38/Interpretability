{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATIS-CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdtkqPJrlq6l",
        "outputId": "adb9e12e-cc74-481f-dd85-37d0551e4f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from intent_classification_helper import *"
      ],
      "metadata": {
        "id": "IsjjIe35l6lb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atis_train = pd.read_csv('/content/drive/MyDrive/nlp_datasets/ATIS dataset/atis_intents_train.csv', error_bad_lines=False, \n",
        "                   engine='python', encoding='utf-8', names=['intent', 'message'])\n",
        "atis_test = pd.read_csv('/content/drive/MyDrive/nlp_datasets/ATIS dataset/atis_intents_test.csv', error_bad_lines=False, \n",
        "                   engine='python', encoding='utf-8', names=['intent', 'message'])\n",
        "assert(set(atis_train['intent']) == set(atis_test['intent']))\n",
        "atis_train = balance_class(atis_train, 'message', 'intent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u1O6WOrl9Af",
        "outputId": "a8b3fe71-2a3a-43db-cddf-ff3f9dfdc0f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode label\n",
        "le = LabelEncoder()\n",
        "atis_train['intent'] = le.fit_transform(atis_train['intent'])\n",
        "atis_test['intent'] = le.fit_transform(atis_test['intent'])\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "y_train = onehot_encoder.fit_transform(atis_train['intent'].values.reshape(-1, 1)).todense()\n",
        "y_test = onehot_encoder.fit_transform(atis_test['intent'].values.reshape(-1, 1)).todense()"
      ],
      "metadata": {
        "id": "j6QRRcuEmE3R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "print('Preprocessing text on training set...')\n",
        "preprocess_text(atis_train, 'message')\n",
        "\n",
        "print('Preprocessing text on test set...')\n",
        "preprocess_text(atis_test, 'message')\n",
        "\n",
        "x_train = atis_train['message'].tolist()\n",
        "x_test = atis_test['message'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsPKJog6mIAL",
        "outputId": "4777e9b6-1262-4449-b6c2-99ae31caa37c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text on training set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing completed.\n",
            "\n",
            "\n",
            "Preprocessing text on test set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n",
            "Text preprocessing completed.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# try different number of features\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=40)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=100)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=200)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=1000)\n",
        "tv = TfidfVectorizer(max_df=1.0, min_df=0)\n",
        "\n",
        "tv.fit(x_train)\n",
        "x_train = tv.transform(x_train).toarray()\n",
        "x_test = tv.transform(x_test).toarray()\n",
        "\n",
        "vocab = tv.get_feature_names_out()\n",
        "\n",
        "print(pd.DataFrame(x_train, columns=vocab))\n",
        "print('TF-IDF vocabulary size: ', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhMIVfbKmNcT",
        "outputId": "d6ecfb3b-55c5-45e1-94f3-dab7f0dd4e5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0900   10  100      1000  1020  1024  1026  1030  1039  1045  ...  \\\n",
            "0       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "1       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "2       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "3       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "4       0.0  0.0  0.0  0.504342   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "...     ...  ...  ...       ...   ...   ...   ...   ...   ...   ...  ...   \n",
            "29323   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29324   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29325   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29326   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29327   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "\n",
            "       without  working  world  worth  would  year  yes   yn  york  yyz  \n",
            "0          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "1          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "2          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "3          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "4          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "...        ...      ...    ...    ...    ...   ...  ...  ...   ...  ...  \n",
            "29323      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29324      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29325      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29326      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29327      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "\n",
            "[29328 rows x 748 columns]\n",
            "TF-IDF vocabulary size:  748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvVM8P0am68M",
        "outputId": "339eaa67-a27c-459f-8171-e3edcf750511"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29328, 748)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_atis(x_train_cnn, y_train_cnn, batch_size, epochs, validation_data, feature_numbers):\n",
        "  model = Sequential()\n",
        "  #model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, input_length=input_length))\n",
        "  #model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
        "  model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(x_train_cnn.shape[1], 1)))\n",
        "  #model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=32, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(units=8, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  # path = '/content/drive/MyDrive/nlp_datasets/CLINC150/models/'\n",
        "  # path = os.path.join(path, 'cnn_{}_features'.format(feature_numbers))\n",
        "  # if not os.path.isdir(path):\n",
        "  #   os.mkdir(path)\n",
        "\n",
        "  #checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy')\n",
        "\n",
        "  #model.fit(x_train_cnn, y_train_cnn, batch_size=batch_size, epochs=epochs, validation_data=validation_data, callbacks=[checkpoint])\n",
        "  model.fit(x_train_cnn, y_train_cnn, batch_size=batch_size, epochs=epochs, validation_data=validation_data)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "0j2Kro5jnH19"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_atis(x_train_cnn=x_train, y_train_cnn=y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test), feature_numbers=len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ECi-HJcmmw2",
        "outputId": "718fe857-22e7-4985-85be-3e6fcde5dfa9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_2 (Conv1D)           (None, 746, 128)          512       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 746, 128)         512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 8)                 264       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,416\n",
            "Trainable params: 5,160\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "917/917 [==============================] - 82s 89ms/step - loss: 1.9976 - accuracy: 0.1913 - val_loss: 2.0118 - val_accuracy: 0.0362\n",
            "Epoch 2/20\n",
            "917/917 [==============================] - 83s 91ms/step - loss: 1.9172 - accuracy: 0.2285 - val_loss: 2.0180 - val_accuracy: 0.0362\n",
            "Epoch 3/20\n",
            "917/917 [==============================] - 84s 92ms/step - loss: 1.9081 - accuracy: 0.2386 - val_loss: 2.0549 - val_accuracy: 0.0325\n",
            "Epoch 4/20\n",
            "917/917 [==============================] - 90s 98ms/step - loss: 1.9024 - accuracy: 0.2385 - val_loss: 1.9849 - val_accuracy: 0.0512\n",
            "Epoch 5/20\n",
            "917/917 [==============================] - 90s 98ms/step - loss: 1.8991 - accuracy: 0.2411 - val_loss: 1.9853 - val_accuracy: 0.0500\n",
            "Epoch 6/20\n",
            "917/917 [==============================] - 89s 97ms/step - loss: 1.8892 - accuracy: 0.2402 - val_loss: 2.0084 - val_accuracy: 0.0437\n",
            "Epoch 7/20\n",
            "917/917 [==============================] - 90s 98ms/step - loss: 1.8810 - accuracy: 0.2373 - val_loss: 1.9965 - val_accuracy: 0.0425\n",
            "Epoch 8/20\n",
            "917/917 [==============================] - 85s 93ms/step - loss: 1.8746 - accuracy: 0.2377 - val_loss: 2.0113 - val_accuracy: 0.0463\n",
            "Epoch 9/20\n",
            "917/917 [==============================] - 88s 96ms/step - loss: 1.8657 - accuracy: 0.2452 - val_loss: 1.9952 - val_accuracy: 0.0463\n",
            "Epoch 10/20\n",
            "917/917 [==============================] - 87s 95ms/step - loss: 1.8641 - accuracy: 0.2401 - val_loss: 1.9612 - val_accuracy: 0.0425\n",
            "Epoch 11/20\n",
            "917/917 [==============================] - 87s 95ms/step - loss: 1.8612 - accuracy: 0.2448 - val_loss: 2.0001 - val_accuracy: 0.0437\n",
            "Epoch 12/20\n",
            "917/917 [==============================] - 88s 96ms/step - loss: 1.8585 - accuracy: 0.2468 - val_loss: 1.9856 - val_accuracy: 0.0425\n",
            "Epoch 13/20\n",
            "917/917 [==============================] - 92s 100ms/step - loss: 1.8550 - accuracy: 0.2470 - val_loss: 2.0218 - val_accuracy: 0.0450\n",
            "Epoch 14/20\n",
            "917/917 [==============================] - 89s 97ms/step - loss: 1.8529 - accuracy: 0.2442 - val_loss: 2.0204 - val_accuracy: 0.0463\n",
            "Epoch 15/20\n",
            "917/917 [==============================] - 95s 103ms/step - loss: 1.8492 - accuracy: 0.2446 - val_loss: 1.9882 - val_accuracy: 0.0525\n",
            "Epoch 16/20\n",
            "917/917 [==============================] - 95s 104ms/step - loss: 1.8456 - accuracy: 0.2485 - val_loss: 2.0243 - val_accuracy: 0.0312\n",
            "Epoch 17/20\n",
            "917/917 [==============================] - 95s 104ms/step - loss: 1.8460 - accuracy: 0.2507 - val_loss: 2.0380 - val_accuracy: 0.0262\n",
            "Epoch 18/20\n",
            "917/917 [==============================] - 94s 103ms/step - loss: 1.8442 - accuracy: 0.2510 - val_loss: 2.0306 - val_accuracy: 0.0275\n",
            "Epoch 19/20\n",
            "917/917 [==============================] - 87s 95ms/step - loss: 1.8464 - accuracy: 0.2493 - val_loss: 2.0194 - val_accuracy: 0.0463\n",
            "Epoch 20/20\n",
            "917/917 [==============================] - 84s 92ms/step - loss: 1.8431 - accuracy: 0.2520 - val_loss: 2.0904 - val_accuracy: 0.0400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7feb49bc7750>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}