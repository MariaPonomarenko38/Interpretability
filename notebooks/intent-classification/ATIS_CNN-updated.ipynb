{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdtkqPJrlq6l",
        "outputId": "5188fac2-067b-4133-ace5-968648ab14fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from intent_classification_helper import *"
      ],
      "metadata": {
        "id": "IsjjIe35l6lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atis_train = pd.read_csv('/content/drive/MyDrive/nlp_datasets/ATIS dataset/atis_intents_train.csv', error_bad_lines=False, \n",
        "                   engine='python', encoding='utf-8', names=['intent', 'message'])\n",
        "atis_test = pd.read_csv('/content/drive/MyDrive/nlp_datasets/ATIS dataset/atis_intents_test.csv', error_bad_lines=False, \n",
        "                   engine='python', encoding='utf-8', names=['intent', 'message'])\n",
        "assert(set(atis_train['intent']) == set(atis_test['intent']))\n",
        "atis_train = balance_class(atis_train, 'message', 'intent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u1O6WOrl9Af",
        "outputId": "afd45c29-dd30-49c9-fd7c-31e3bf491937"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atis_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFteSOuqBL8h",
        "outputId": "dee96a78-7436-4dfd-cb84-177ec993419a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29328, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "atis_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY2f7JZhBqkY",
        "outputId": "b7d1c1e9-2559-45aa-a1d1-98814d1ff075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encode label\n",
        "le = LabelEncoder()\n",
        "atis_train['intent'] = le.fit_transform(atis_train['intent'])\n",
        "atis_test['intent'] = le.fit_transform(atis_test['intent'])\n",
        "\n",
        "onehot_encoder = OneHotEncoder()\n",
        "y_train = onehot_encoder.fit_transform(atis_train['intent'].values.reshape(-1, 1)).todense()\n",
        "y_test = onehot_encoder.fit_transform(atis_test['intent'].values.reshape(-1, 1)).todense()"
      ],
      "metadata": {
        "id": "j6QRRcuEmE3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "print('Preprocessing text on training set...')\n",
        "preprocess_text(atis_train, 'message')\n",
        "\n",
        "print('Preprocessing text on test set...')\n",
        "preprocess_text(atis_test, 'message')\n",
        "\n",
        "x_train = atis_train['message'].tolist()\n",
        "x_test = atis_test['message'].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsPKJog6mIAL",
        "outputId": "f8fa04b2-ed09-49c2-bb4f-cba581022c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing text on training set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text preprocessing completed.\n",
            "\n",
            "\n",
            "Preprocessing text on test set...\n",
            "\n",
            "\n",
            "Start text preprocessing: \n",
            "--------------------------\n",
            "Converting to lowercase...\n",
            "--------------------------\n",
            "Removing html tags...\n",
            "--------------------------\n",
            "Removing nonword characters...\n",
            "--------------------------\n",
            "Removing stopwords...\n",
            "Text preprocessing completed.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# try different number of features\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=40)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=100)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=200)\n",
        "#tv = TfidfVectorizer(max_df=1.0, min_df=0, max_features=1000)\n",
        "tv = TfidfVectorizer(max_df=1.0, min_df=0)\n",
        "\n",
        "tv.fit(x_train)\n",
        "x_train = tv.transform(x_train).toarray()\n",
        "x_test = tv.transform(x_test).toarray()\n",
        "\n",
        "vocab = tv.get_feature_names_out()\n",
        "\n",
        "print(pd.DataFrame(x_train, columns=vocab))\n",
        "print('TF-IDF vocabulary size: ', len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhMIVfbKmNcT",
        "outputId": "b3b91163-8ed9-4e51-ed7d-d7f2672f52ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       0900   10  100      1000  1020  1024  1026  1030  1039  1045  ...  \\\n",
            "0       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "1       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "2       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "3       0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "4       0.0  0.0  0.0  0.504342   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "...     ...  ...  ...       ...   ...   ...   ...   ...   ...   ...  ...   \n",
            "29323   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29324   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29325   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29326   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "29327   0.0  0.0  0.0  0.000000   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
            "\n",
            "       without  working  world  worth  would  year  yes   yn  york  yyz  \n",
            "0          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "1          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "2          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "3          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "4          0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "...        ...      ...    ...    ...    ...   ...  ...  ...   ...  ...  \n",
            "29323      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29324      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29325      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29326      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "29327      0.0      0.0    0.0    0.0    0.0   0.0  0.0  0.0   0.0  0.0  \n",
            "\n",
            "[29328 rows x 748 columns]\n",
            "TF-IDF vocabulary size:  748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvVM8P0am68M",
        "outputId": "c4d83434-7741-456d-fdec-eda3d05bbede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29328, 748)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import MaxPooling1D"
      ],
      "metadata": {
        "id": "z1z2yYoCsK1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_atis(x_train_cnn, y_train_cnn, batch_size, epochs, validation_data, feature_numbers):\n",
        "  model = Sequential()\n",
        "  # model.add(Embedding(input_dim=feature_numbers, output_dim=64, input_length=feature_numbers))\n",
        "  # model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(x_train_cnn.shape[1], 1)))\n",
        "\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dropout(0.5)) \n",
        "  model.add(MaxPooling1D(2))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(units=100, activation='relu'))\n",
        "  model.add(Dense(units=8, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  # path = '/content/drive/MyDrive/nlp_datasets/CLINC150/models/'\n",
        "  # path = os.path.join(path, 'cnn_{}_features'.format(feature_numbers))\n",
        "  # if not os.path.isdir(path):\n",
        "  #   os.mkdir(path)\n",
        "\n",
        "  #checkpoint = ModelCheckpoint(filepath=path, monitor='val_accuracy')\n",
        "\n",
        "  #model.fit(x_train_cnn, y_train_cnn, batch_size=batch_size, epochs=epochs, validation_data=validation_data, callbacks=[checkpoint])\n",
        "  #model.fit(x_train_cnn, y_train_cnn, batch_size=batch_size, epochs=epochs, validation_data=validation_data)\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "0j2Kro5jnH19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running cnn without word embedding layer\n",
        "model = cnn_atis(x_train_cnn=x_train, y_train_cnn=y_train, batch_size=8, epochs=20, validation_data=(x_test, y_test), feature_numbers=len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ECi-HJcmmw2",
        "outputId": "0a1b1d5c-749f-402d-e715-b7a18b2601ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_1 (Conv1D)           (None, 746, 32)           128       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 746, 32)          128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 746, 32)           0         \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 373, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 11936)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               1193700   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8)                 808       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,194,764\n",
            "Trainable params: 1,194,700\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=8, epochs=20, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifEoN85DsYOa",
        "outputId": "314215d2-28ad-489c-e1ec-93d07f1024b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3666/3666 [==============================] - 71s 19ms/step - loss: 0.0697 - accuracy: 0.9801 - val_loss: 0.1575 - val_accuracy: 0.9550\n",
            "Epoch 2/20\n",
            "3666/3666 [==============================] - 69s 19ms/step - loss: 0.0185 - accuracy: 0.9946 - val_loss: 0.2106 - val_accuracy: 0.9375\n",
            "Epoch 3/20\n",
            "3666/3666 [==============================] - 68s 19ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.3337 - val_accuracy: 0.9488\n",
            "Epoch 4/20\n",
            "3666/3666 [==============================] - 68s 18ms/step - loss: 0.0120 - accuracy: 0.9970 - val_loss: 0.2272 - val_accuracy: 0.9538\n",
            "Epoch 5/20\n",
            "3666/3666 [==============================] - 68s 19ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.1921 - val_accuracy: 0.9650\n",
            "Epoch 6/20\n",
            "3666/3666 [==============================] - 68s 18ms/step - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.4999 - val_accuracy: 0.9425\n",
            "Epoch 7/20\n",
            "3666/3666 [==============================] - 69s 19ms/step - loss: 0.0076 - accuracy: 0.9982 - val_loss: 0.1927 - val_accuracy: 0.9650\n",
            "Epoch 8/20\n",
            "3666/3666 [==============================] - 69s 19ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.2945 - val_accuracy: 0.9613\n",
            "Epoch 9/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.3357 - val_accuracy: 0.9475\n",
            "Epoch 10/20\n",
            "3666/3666 [==============================] - 71s 19ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.5661 - val_accuracy: 0.9100\n",
            "Epoch 11/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.3272 - val_accuracy: 0.9513\n",
            "Epoch 12/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 1.4944 - val_accuracy: 0.9550\n",
            "Epoch 13/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 1.4916 - val_accuracy: 0.8350\n",
            "Epoch 14/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.0216 - val_accuracy: 0.9312\n",
            "Epoch 15/20\n",
            "3666/3666 [==============================] - 71s 19ms/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 6.8775 - val_accuracy: 0.8888\n",
            "Epoch 16/20\n",
            "3666/3666 [==============================] - 69s 19ms/step - loss: 0.0052 - accuracy: 0.9990 - val_loss: 0.8314 - val_accuracy: 0.9563\n",
            "Epoch 17/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 2.6037 - val_accuracy: 0.8062\n",
            "Epoch 18/20\n",
            "3666/3666 [==============================] - 71s 19ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 1.0175 - val_accuracy: 0.9337\n",
            "Epoch 19/20\n",
            "3666/3666 [==============================] - 72s 20ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.6801 - val_accuracy: 0.9413\n",
            "Epoch 20/20\n",
            "3666/3666 [==============================] - 70s 19ms/step - loss: 0.0066 - accuracy: 0.9991 - val_loss: 2.5835 - val_accuracy: 0.7750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0243a425d0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}