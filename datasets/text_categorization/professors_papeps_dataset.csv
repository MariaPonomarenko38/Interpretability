,professor name,paper_abstract,title,date
0,Tarek Abdelrahman,we present a compilation flow for the generation of cnn inference accelerators on fpgas the flow translates a frozen model into opencl kernels with the tvm compiler and uses the intel opencl sdk to compile to an fpga bitstream we improve the quality of the generated hardware with optimizations applied to the base opencl kernels generated by tvm these optimizations increase parallelism reduce memory access latency increase concurrency and save onchip resources we automate these optimizations in tvm and evaluate them by generating accelerators for lenet mobilenetv and resnet on an intel stratixsx we show that the optimizations improve the performance of the generated accelerators by up to x over the base accelerators the performance of the optimized accelerators is up to x better than tensorflow on cpu x better than singlethreaded tvm and is only x compared to tvm with threads our optimized kernels also outperform ones generated by a similar approach that also uses highlevel synthesis while providing more functionality and flexibility however it underperforms an approach that utilizes handoptimized designs thus we view our approach as useful in preproduction environments that benefit from increased performance and fast prototyping realizing the benefits of fpgas without hardware design expertise less,A Compilation Flow for the Generation of CNN Inference Accelerators on FPGAs,"8 March, 2022"
1,Tarek Abdelrahman,"Field experiments were carried on cotton during season 2020 at Al-Dakahlia and Al-Fayoum governorates, Egypt. Abamectin, Fenpyroximate, Buprofezin, were compared to a natural pesticidal product (Anti-insect) against all movable stages (immature and adults) of Tetranychus urticae by using two sprayers, a Motorized Knapsack motor (Cifarilli)(20 L./Fed.) and a Hydraulic sprayer (Matabi)(56 L./Fed.). The residual behavior of the tested acaricides was evaluated in cotton seeds, and the intercrops, cucumber, and tomato fruits loaded under early cotton stages in the field. All tested compounds induced a significant negative efficacy on T. urticae survival. Abamectin and Fenpyroximate exhibited the highest acaricidal activity, followed by Buprofezin and Anti-insect. Cifarilli sprayer was more effective to control T. urticae on cotton according to the homogeneity of the droplet spectrum. The Cifarilli'rate of performance (Fed./day) was 4 times that Matabi sprayer. The lost spray-on ground for the Cifarilli motor was lower than Matabi sprayer with 40%. All cotton seed samples were free from residues of the pesticides. The recovery rate of Abamectin, Fenpyroximate, and Buprofezin from the fortified samples of cucumber, and tomatoes fruits ranged from 83.07% to 92%. The pre-harvest interval period was (10 and 4 days),(10 and 10 days) and (6-7 and 6 days) for tomatoes and cucumbers with Abamectin, Fenpyroximate, and Buprofezin, respectively. There were no significant differences between the effects of using the two sprayers on the safety period for the pesticides investigated, due to similarity in the pesticide degradation for sprayers and governorate",Toxicological evaluation and residual analysis of some acaricides against two-spotted spider mite Tetranychus urticae by using certain ground spraying equipment,2021-01-01 00:00:00
2,Tarek Abdelrahman,"Rice husk was used as an adsorbent for the removal of Cd (II) and Pb (II) from aqueous solution. The effects of contact time, pH, concentration, agitation speed, and dose on the removal of Cd (II) and Pb (II) have been evaluated. Our result shows that rice husk powder has high reduction efficiency for both metals. Results showed that low-cost adsorbents can be fruitfully used for the removal of heavy metals with a concentration range of 30–40 g/l. The percentage removal of heavy metals was dependent on the dose and concentration of Rice husk. The contact time necessary for maximum adsorption was found to be 35-40 min. The optimum pH range for heavy metal adsorption was 4–8",Adsorption Study of Cadmium (II) and Lead (II) On Rice Husk,2020-08-01 00:00:00
3,Tarek Abdelrahman,"the fungicidal residues of certain copper fungicides, i.e. copper oxychloride, copper hydroxide and copper salts of fatty and rosin acids used against downy mildew in lettuce were studied. Data showed the occurrence of Cu residues in the plant, at different values according to the forms and rates used. Copper compounds residues were digested from lettuce plants samples then the residues were determined by microwave plasma atomic emission spectrometry. Results indicated that the pre-harvest interval (PHI) for lettuce plants were 1, 3 and 4 days for copper salts, copper hydroxide and copper oxychloride, respectively.",Residual effect of certain copper fungicides used in controlling downy mildew of lettuce,2017-02-10 00:00:00
4,Tarek Abdelrahman,"This study explored the heavy metals contamination of drinking water samples from El-Gharbiya Governorate. Microwave plasma-nitrogen plasma-atomic emission spectrometry was used for the determination of heavy metals in 20 water samples (10 samples was collected from groundwater sources and 10 samples collected from the water purification station), Heavy metal ie (As, Cd, Cr, Cu, Mn, Ni, Pb, Mo, and Zn) contents in the collected drinking water samples were found at different levels. The obtained results showed that, the contamination percentage of groundwater sources and the water purification station samples were reached 100%. Samples were contaminated with different amount of heavy metals. Each sample was contaminated with one or more of heavy metals. All samples were free from any detectable residues of Cr and Mo. As, Cu, and Pb were recorded the highest contamination in all samples, followed by Mn and Zn, while Cd and Ni recorded the lowest level one.",Monitoring and minimizing levels of some heavy metals in groundwa-ter and water purification stations samples in El-Gharbiya Governorate- Egypt.,2017
5,Tarek Abdelrahman,"A total of 18 pesticides (12 organochlorines, and 6 pyrethroids) in 32 different imported animal liver samples collected from local markets in Cairo governorate in Egypt in different seasons, were detect the contamination of organochlorines and pyrethroids pesticides using Quick, Easy, Cheap, Effective, Rugged, and Safe (QuEChERS) method. The presence of organochlorines and pyrethroids pesticides residues were determined by gas chromatography with electron capture detector (GC-ECD). The results indicated that, the pesticides residues were found in all samples and 2 samples above the maximum residue levels (MRLs). Organochlorine residues had the highest percentage of contamination and violation (ie 100 and 6.25%, respectively) in imported animal liver samples, whereas synthetic pyrethroid residues had the lowest percentage of contamination (ie 6.25%) while their percentage of violation were 0%. However, the most frequently found pesticides were P, P'-DDE and heptachlor-epoxide while the lowest frequently found pesticides were aldrin, cypermethrin and deltamethrin. Furthermore, the health risk index for heptachlor-epoxide was the greatest which may be due to its physiochemical properties. A potential regular pesticides residues monitoring program in imported animal liver should be conducted to protect the consumers' health.","Dietary Intake of Pesticides Based on Import Animal Liver Consumption: A Case Study, Cairo, Egypt",2016-02-08 00:00:00
6,Tarek Abdelrahman,modern convolutional neural networks cnns are complex encompassing millions of parameters their deployment exerts computational storage and energy demands particularly on embedded platforms existing approaches to prune or sparsify cnns require retraining to maintain inference accuracy such retraining is not feasible in some contexts in this paper we explore the sparsification of cnns by proposing three modelindependent methods our methods are applied onthefly and require no retraining we show that the stateoftheart models weights can be reduced by up to compression factor of x without incurring more than loss in top accuracy additional finetuning gains only in sparsity which indicates that our fast onthefly methods are effective less,Fast On-the-fly Retraining-free Sparsification of Convolutional Neural Networks,"8 September, 2019"
7,Ishtiaque Ahmed,online app search optimization aso platforms that provide bulk installs and fake reviews for paying app developers in order to fraudulently boost their search rank in app stores were shown to employ diverse and complex strategies that successfully evade stateoftheart detection methods in this paper we introduce racketstore a platform to collect data from android devices of participating aso providers and regular users on their interactions with apps which they install from the google play store we present measurements from a study of installs of racketstore on unique devices controlled by aso providers and regular users that consists of data snapshots collected from these devices the apps installed on them and their google play reviews we reveal significant differences between aso providers and regular users in terms of the number and types of user accounts registered on their devices the number of apps they review and the intervals between the installation times of apps and their review times we leverage these insights to introduce features that model the usage of apps and devices and show that they can train supervised learning algorithms to detect paid app installs and fake reviews with an fmeasure of auc above and detect devices controlled by aso providers with an fmeasure of auc we discuss the costs associated with evading detection by our classifiers and also the potential for app stores to use our approach to detect aso work with privacy less,RacketStore: Measurements of ASO Deception in Google Play via Mobile and App Usage,"19 November, 2021"
8,Ishtiaque Ahmed,objective this study aims to identify the social determinants of mental health among undergraduate students in bangladesh a developing nation in south asia our goal is to identify the broader social determinants of mental health among this population study the manifestation of these determinants in their daytoday life and explore the feasibility of selfmonitoring tools in helping them identify the specific factors or relationships that impact their mental health methods we conducted a day study with undergraduate students from seven universities in bangladesh we conducted two semistructured interviews one prestudy and one poststudy during the day study participants used an android application to selfreport and selfmonitor their mood after each phone conversation the app prompted participants to report their mood after each phone conversation and provided graphs and charts so that participants could independently review their mood and conversation patterns results our results show that academics family job and economic condition romantic relationships and religion are the major social determinants of mental health among undergraduate students in bangladesh our app helped the participants pinpoint the specific issues related to these factors as participants could review the pattern of their moods and emotions from past conversation history although our app does not provide any explicit recommendation participants took certain steps on their own to improve their mental health eg reduced the frequency of communication with certain persons conclusions overall the findings from this study would provide better insights for the researchers to design better solutions to help the younger population from this part of the world less,Understanding the Social Determinants of Mental Health of the Undergraduate Students in Bangladesh: Interview Study,"6 September, 2021"
9,Ishtiaque Ahmed,mental health is a global epidemic affecting close to half a billion people worldwide chronic shortage of resources hamper detection and recovery of affected people effective sensing technologies can help fight the epidemic through early detection prediction and resulting proper treatment existing and novel technologies for sensing mental health state could address the aforementioned concerns by activating granular tracking of physiological behavioral and social signals pertaining to problems in mental health our paper focuses on the available methods of sensing mental health problems through direct and indirect measures we see how active and passive sensing by technologies as well as reporting from relevant sources can contribute toward these detection methods we also see available methods of therapeutic treatment available through digital means we highlight a few key intervention technologies that are being developed by researchers to fight against mental illness issues less,Mental Health and Sensing,"25 September, 2020"
10,Ishtiaque Ahmed,with the shortage of physicians and surgeons and increase in demand worldwide due to situations such as the covid pandemic there is a growing interest in finding solutions to help address the problem a solution to this problem would be to use neurotechnology to provide them augmented cognition senses and action for optimal diagnosis and treatment consequently doing so can negatively impact them and others we argue that applying neurotechnology for human enhancement in physicians and surgeons can cause injustices and harm to them and patients in this paper we will first describe the augmentations and neurotechnologies that can be used to achieve the relevant augmentations for physicians and surgeons we will then review selected ethical concerns discussed within literature discuss the neuroengineering behind using neurotechnology for augmentation purposes then conclude with an analysis on outcomes and ethical issues of implementing human augmentation via neurotechnology in medical and surgical practice less,Ethical Analysis on the Application of Neurotechnology for Human Augmentation in Physicians and Surgeons,"3 July, 2020"
11,Ishtiaque Ahmed,tracking sexual violence is a challenging task in this paper we present a supervised learningbased automated sexual violence report tracking model that is more scalable and reliable than its crowdsource based counterparts we define the sexual violence report tracking problem by considering victim perpetrator contexts and the nature of the violence we find that our model could identify sexual violence reports with a precision and recall of and respectively moreover we also applied the model during and after the metoo movement several interesting findings are discovered which are not easily identifiable from a shallow analysis less,Towards Automated Sexual Violence Report Tracking,"16 November, 2019"
12,Ishtiaque Ahmed,in october there happened the uprising of an unprecedented online movement on social media by women across the world who started publicly sharing their untold stories of being sexually harassed along with the hashtag metoo or some variants of it those stories did not only strike the silence that had long hid the perpetrators but also allowed women to discharge some of their bottledup grievances and revealed many important information surrounding sexual harassment in this paper we present our analysis of about one million such tweets collected between october and october that reveals some interesting patterns and attributes of the people place emotions actions and reactions related to the tweeted stories based on our analysis we also advance the discussion on the potential role of online social media in breaking the silence of women by factoring in the strengths and limitations of these platforms less,Can Women Break the Glass Ceiling?: An Analysis of #MeToo Hashtagged Posts on Twitter,"3 June, 2019"
13,Ashton Anderson,"In recent years, the “creator economy” has emerged as a disruptive force in creative industries. Independent creators can now reach large and diverse audiences through online platforms, and membership platforms have emerged to connect these creators with fans who are willing to financially support them. However, the structure and dynamics of how membership platforms function on a large scale remain poorly understood. In this work, we develop an analysis framework for the study of membership platforms and apply it to the complete set of Patreon pledges exceeding $2 billion since its inception in 2013 until the end of 2020. We analyze Patreon activity through three perspectives: patrons (demand), creators (supply), and the platform as a whole. We find several important phenomena that help explain how membership platforms operate. Patrons who pledge to a narrow set of creators are more loyal, but churn off the platform more often. High-earning creators attract large audiences, but these audiences are less likely to pledge to other creators. Over its history, Patreon diversified into many topics and launched higher-earning creators over time. Our analysis framework and results shed light on the functioning of membership platforms and have implications for the creator economy.",Quantifying the Creator Economy: A Large-Scale Analysis of Patreon,2022-05-31 00:00:00
14,Ashton Anderson,"Digital media platforms give users access to enormous amounts of content that they must explore to avoid boredom and satisfy their needs for heterogeneity. Existing strands of work across psychology, marketing, computer science, and music underscore the importance of the lifecycle to understanding exploratory behavior, but they are also often inconsistent with each other. In this study, we examine how users explore online content on Spotify over time, whether by discovering entirely novel music or by refreshing their listening habits from one time frame to the next. We find clear differences between users at different points of their off-platform lifecycles, with younger listeners consistently exploring unknown content less and exploiting known content more. Across their on-platform histories, users also explore in bursts by following seasonal cycles and exploratory phases. We also find that these patterns of exploration do not translate to other notions of heterogeneity like diversity; notably, younger listeners are more diverse in their consumption despite exploring less. Exploration and diversity thus capture different ways in which people find variety, potentially accounting for the inconsistencies in existing work. Together, these nuanced dynamics of exploration suggest that online platforms may be better poised to support users by incorporating different measures of heterogeneous consumption.",The Dynamics of Exploration on Spotify,2022-05-31 00:00:00
15,Ashton Anderson,"Social media reduces barriers for the formation of large, self-organizing grassroots communities. For political campaigns this poses significant opportunities to address declining party membership, but also reputational risks and potential loss of campaign coherence. While balancing these factors is often done informally, we adopt a behavioural approach by using neural community embeddings to evaluate online communities along cultural, political, and demographic dimensions. We apply this technique to the 2020 US Democratic presidential primaries and the website Reddit, providing novel insights into the important tension between campaigns and third-party actors. Using two benchmark comparison classes, we demonstrate that our embedding dimensions mirror their offline analogues, but more so the views of a candidate's supporters than the candidate's themselves. Finally, we introduce temporal aspects to our community embedding to evaluate the stability of political communities and their interrelations. These analyses serve as an exploration and application of our novel embedding methodology, and give insight into the relationship between online communities and the movements they support.",Measuring Alignment of Online Grassroots Political Communities with Political Campaigns,2022-05-31 00:00:00
16,Ashton Anderson,"Online recommendation systems are prone to create filter bubbles, whereby users are only recommended content narrowly aligned with their historical interests. In the case of media recommendation, this can reinforce political polarization by recommending topical content (eg, on the economy) at one extreme end of the political spectrum even though this topic has broad coverage from multiple political viewpoints that would provide a more balanced and informed perspective for the user. Historically, Maximal Marginal Relevance (MMR) has been used to diversify result lists and even mitigate filter bubbles, but suffers from three key drawbacks:(1) MMR directly sacrifices relevance for diversity,(2) MMR typically diversifies across all content and not just targeted dimensions (eg, political polarization), and (3) MMR is inefficient in practice due to the need to compute pairwise similarities between recommended items. To simultaneously address these limitations, we propose a novel methodology that trains Concept Activation Vectors (CAVs) for targeted topical dimensions (eg, political polarization). We then modulate the latent embeddings of user preferences in a state-of-the-art VAE-based recommender system to diversify along the targeted dimension while preserving topical relevance across orthogonal dimensions. Our experiments show that our Targeted Diversification VAE-based Collaborative Filtering (TDVAE-CF) methodology better preserves relevance of content to user preferences across a range of diversification levels in comparison",Mitigating the Filter Bubble while Maintaining Relevance: Targeted Diversification with VAE-based Recommender Systems,2022
17,Ashton Anderson,"The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them. To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data.",Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess,2021-12-06 00:00:00
18,Ashton Anderson,"Mass selection into groups of like-minded individuals may be fragmenting and polarizing online society, particularly with respect to partisan differences 1, 2, 3, 4. However, our ability to measure the social makeup of online communities and in turn, to understand the social organization of online platforms, is limited by the pseudonymous, unstructured and large-scale nature of digital discussion. Here we develop a neural-embedding methodology to quantify the positioning of online communities along social dimensions by leveraging large-scale patterns of aggregate behaviour. Applying our methodology to 5.1 billion comments made in 10,000 communities over 14 years on Reddit, we measure how the macroscale community structure is organized with respect to age, gender and US political partisanship. Examining political content, we find that Reddit underwent a significant polarization event around the 2016 US ",Quantifying social organization and political polarization in online platforms,2021/12
19,Ashton Anderson,"Many popular books and articles that purport to explain how people, companies, or ideas succeed highlight a few successes chosen to fit a particular narrative. We investigate what effect these highly selected “success narratives” have on readers’ beliefs and decisions. We conducted a large, randomized, pre-registered experiment, showing participants successful firms with founders that all either dropped out of or graduated college, and asked them to make incentive-compatible bets on a new firm. Despite acknowledging biases in the examples, participants’ decisions were very strongly influenced by them. People shown dropout founders were 55 percentage points more likely to bet on a dropout-founded company than people who were shown graduate founders. Most reported medium to high confidence in their bets, and many wrote causal explanations justifying their decision. In light of recent concerns about false information, our findings demonstrate how true but biased information can strongly alter beliefs and decisions.",Success stories cause false beliefs about success,2021/11
20,Ashton Anderson,"What makes written text appealing? In this registered report protocol, we propose to study the linguistic characteristics of news headline success using a large-scale dataset of field experiments (A/B tests) conducted on the popular website Upworthy comparing multiple headline variants for the same news articles. This unique setup allows us to control for factors that can have crucial confounding effects on headline success. Based on prior literature and a pilot partition of the data, we formulate hypotheses about the linguistic features that are associated with statistically superior headlines. We will test our hypotheses on a much larger partition of the data that will become available after the publication of this registered report protocol. Our results will contribute to resolving competing hypotheses about the linguistic features that affect the success of text and will provide avenues for research into the psychological mechanisms that are activated by those features.",Linguistic effects on news headline success: Evidence from thousands of online field experiments (Registered Report Protocol),2021-09-15 00:00:00
21,Ashton Anderson,"As online platforms become ubiquitous, there is growing concern that their use can potentially lead to negative outcomes in users' personal lives, such as disrupted sleep and impacted social relationships. A central question in the literature studying these problematic effects is whether they are associated with the amount of time users spend on online platforms. This is often addressed by either analyzing self-reported measures of time spent online, which are generally inaccurate, or using objective metrics derived from server logs or tracking software. Nonetheless, how the two types of time measures comparatively relate to problematic effects -- whether they complement or are redundant with each other in predicting problematicity -- remains unknown. Additionally, transparent research into this question is hindered by the literature's focus on closed platforms with inaccessible data, as well as selective analytical ",The Complementary Nature of Perceived and Actual Time Spent Online in Measuring Digital Well-being,2021-04-22 00:00:00
22,Ashton Anderson,"We consider the problem of predicting users’ preferences on online platforms. We build on recent findings suggesting that users’ preferences change over time, and that helping users expand their horizons is important in ensuring that they stay engaged. Most existing models of user preferences attempt to capture simultaneous preferences:“Users who like A tend to like B as well”. In this paper, we argue that these models fail to anticipate changing preferences. To overcome this issue, we seek to understand the structure that underlies the evolution of user preferences. To this end, we propose the Preference Transition Model (PTM), a dynamic model for user preferences towards classes of items. The model enables the estimation of transition probabilities between classes of items over time, which can be used to estimate how users’ tastes are expected to evolve based on their past history. We test our model’s",Where To Next? A Dynamic Model of User Preferences,2021-04-19 00:00:00
23,Ashton Anderson,"Music shapes our individual and collective identities, and in turn is shaped by the social and cultural contexts it occurs in. Qualitative approaches to the study of music have uncovered rich connections between music and social context but are limited in scale, while computational approaches process large amounts of musical data but lack information on the social contexts music is embedded in. In this work, we develop a set of neural embedding methods to understand the social contexts of online music sharing, and apply them to a novel dataset containing 1.3 M instances of music sharing in Reddit communities. We find that the patterns of how people share music in public are related to, but often differ from, how they listen to music in private. We cluster artists into social genres that are based entirely on aggregate sharing patterns and reflect where artists are invoked. We also characterize the social and cultural contexts music is shared in by measuring associations with social dimensions such as age and political affiliation. Finally, we observe that a significant amount of sharing is attributable to extra-musical factors—additional meanings that people have associated with songs. We develop two methods to quantify the extra-musicality of music sharing. Our methodology is widely applicable to the study of online social contexts, and our results reveal novel cultural associations that contribute to a better understanding of the online music ecosystem.",Imagine All the People: Characterizing Social Music Sharing on Reddit,2021
24,Ashton Anderson,"Optimism about the Internet’s potential to bring the world together has been tempered by concerns about its role in inflaming the “culture wars”. Via mass selection into like-minded groups, online society may be becoming more fragmented and polarized, particularly with respect to partisan differences. However, our ability to measure the cultural makeup of online communities, and in turn understand the cultural structure of online platforms, is limited by the pseudonymous, unstructured, and large-scale nature of digital discussion. Here we develop a neural embedding methodology to quantify the positioning of online communities along cultural dimensions by leveraging large-scale patterns of aggregate behaviour. Applying our methodology to 4.8 B Reddit comments made in 10K communities over 14 years, we find that the macro-scale community structure is organized along cultural lines, and that relationships between online cultural concepts are more complex than simply reflecting their offline analogues.",Community embeddings reveal large-scale cultural organization of online platforms,2020-10-01 00:00:00
25,Ashton Anderson,"In November 2017, Twitter doubled the maximum allowed tweet length from 140 to 280 characters, a drastic switch on one of the world's most influential social media platforms. In the first long-term study of how the new length limit was adopted by Twitter users, we ask: Does the effect of the new length limit resemble that of the old one? Or did the doubling of the limit fundamentally change how Twitter is shaped by the limited length of posted content? By analyzing Twitter's publicly available 1% sample over a period of around 3 years, we find that, when the length limit was raised from 140 to 280 characters, the prevalence of tweets around 140 characters dropped immediately, while the prevalence of tweets around 280 characters rose steadily for about 6 months. Despite this rise, tweets approaching the length limit have been far less frequent after than before the switch. We find widely different adoption rates across languages and client-device types. The prevalence of tweets around 140 characters before the switch in a given language is strongly correlated with the prevalence of tweets around 280 characters after the switch in the same language, and very long tweets are vastly more popular on Web clients than on mobile clients. ",Adoption of Twitter's New Length Limit: Is 280 the New 140?,2020-09-16 00:00:00
26,Ashton Anderson,the second gravitationalwave transient catalog reported on compact binary coalescences observed by the advanced ligo and advanced virgo detectors between april utc and october utc we present gwtc which reports on a deeper list of candidate events observed over the same period we analyze the final version of the strain data over this period with improved calibration and better subtraction of excess noise which has been publicly released we employ three matchedfilter search pipelines for candidate identification and estimate the astrophysical probability for each candidate event while gwtc used a false alarm rate threshold of per year we include in gwtc candidates that pass a false alarm rate threshold of per day we calculate the source properties of a subset of highsignificance candidates that have an astrophysical probability greater than of these candidates have been reported in gwtc if the additional highsignificance candidates presented here are astrophysical the mass range of events that are unambiguously identified as binary black holes both objects geq modot is increased compared to gwtc with total masses from sim modot for gw to sim modot for gw the primary components of two new candidate events gw and gw fall in the mass gap predicted by pair instability supernova theory we also expand the population of binaries with significantly asymmetric mass ratios reported in gwtc by an additional two events the mass ratio is less than and at probability for gw and gw respectively and find that of the new events have effective inspiral spins mathrmeff at credibility while no binary is consistent with mathrmeff at the same significance less,GWTC-2.1: Deep Extended Catalog of Compact Binary Coalescences Observed by LIGO and Virgo During the First Half of the Third Observing Run,"10 May, 2022"
27,Alán Aspuru-Guzik,quantum physics experiments produce interesting phenomena such as interference or entanglement which are core properties of numerous future quantum technologies the complex relationship between the setup structure of a quantum experiment and its entanglement properties is essential to fundamental research in quantum optics but is difficult to intuitively understand we present a deep generative model of quantum optics experiments where a variational autoencoder is trained on a dataset of quantum optics experimental setups in a series of computational experiments we investigate the learned representation of our quantum optics variational auto encoder qovae and its internal understanding of the quantum optics world we demonstrate that the qovae learns an interpretable representation of quantum optics experiments and the relationship between experiment structure and entanglement we show the qovae is able to generate novel experiments for highly entangled quantum states with specific distributions that match its training data the qovae can learn to generate specific entangled states and efficiently search the space of experiments that produce highly entangled quantum states importantly we are able to interpret how the qovae structures its latent space finding curious patterns that we can explain in terms of quantum physics the results demonstrate how we can use and understand the internal representations of deep generative models in a complex scientific domain the qovae and the insights from our investigations can be immediately applied to other physical systems less,Learning Interpretable Representations of Entanglement in Quantum Optics Experiments using Deep Generative Models,"16 June, 2022"
28,Alán Aspuru-Guzik,logic artificial intelligence ai is a subfield of ai where variables can take two defined arguments true or false and are arranged in clauses that follow the rules of formal logic several problems that span from physical systems to mathematical conjectures can be encoded into these clauses and solved by checking their satisfiability sat in contrast to machine learning approaches where the results can be approximations or local minima logic ai delivers formal and mathematically exact solutions to those problems in this work we propose the use of logic ai for the design of optical quantum experiments we show how to map into a sat problem the experimental preparation of an arbitrary quantum state and propose a logicbased algorithm called textscklaus to find an interpretable representation of the photonic setup that generates it we compare the performance of textscklaus with the stateoftheart algorithm for this purpose based on continuous optimization we also combine both logic and numeric strategies to find that the use of logic ai improves significantly the resolution of this problem paving the path to developing more formalbased approaches in the context of quantum physics experiments less,Design of quantum optical experiments with logic artificial intelligence,"13 May, 2022"
29,Alán Aspuru-Guzik,the alphafold computer program predicted protein structures for the whole human genome which has been considered as a remarkable breakthrough both in artificial intelligence ai application and structural biology despite the varying confidence level these predicted structures still could significantly contribute to structurebased drug design of novel targets especially the ones with no or limited structural information in this work we successfully applied alphafold in our endtoend aipowered drug discovery engines constituted of a biocomputational platform pandaomics and a generative chemistry platform chemistry to identify a firstinclass hit molecule of a novel target without an experimental structure starting from target selection towards hit identification in a cost and timeefficient manner pandaomics provided the targets of interest and chemistry generated the molecules based on the alphafold predicted structure and the selected molecules were synthesized and tested in biological assays through this approach we identified a small molecule hit compound for cdk with a kd value of um n within days from target selection and after only synthesizing compounds based on the available data the second round of aipowered compound generation was conducted and through which a more potent hit molecule ism was discovered with a kd value of nm n within days and after synthesizing compounds from the discovery of the first hit ism to the best of our knowledge this is the first reported small molecule targeting cdk and more importantly this work is the first demonstration of alphafold application in the hit identification process in early drug discovery less,AlphaFold Accelerates Artificial Intelligence Powered Drug Discovery: Efficient Discovery of a Novel Cyclin-dependent Kinase 20 (CDK20) Small Molecule Inhibitor,"12 February, 2022"
30,Alán Aspuru-Guzik,machine learning has the potential to automate molecular design and drastically accelerate the discovery of new functional compounds towards this goal generative models and reinforcement learning rl using string and graph representations have been successfully used to search for novel molecules however these approaches are limited since their representations ignore the threedimensional d structure of molecules in fact geometry plays an important role in many applications in inverse molecular design especially in drug discovery thus it is important to build models that can generate molecular structures in d space based on propertyoriented geometric constraints to address this one approach is to generate molecules as d point clouds by sequentially placing atoms at locations in space this allows the process to be guided by physical quantities such as energy or other properties however this approach is inefficient as placing individual atoms makes the exploration unnecessarily deep limiting the complexity of molecules that can be generated moreover when optimizing a molecule organic and medicinal chemists use known fragments and functional groups not single atoms we introduce a novel rl framework for scalable d design that uses a hierarchical agent to build molecules by placing molecular substructures sequentially in d space thus attempting to build on the existing human knowledge in the field of molecular design in a variety of experiments with different substructures we show that our agent guided only by energy considerations can efficiently learn to produce molecules with over atoms from many distributions including druglike molecules organic led molecules and biomolecules less,Scalable Fragment-Based 3D Molecular Design with Reinforcement Learning,"1 February, 2022"
31,Alán Aspuru-Guzik,the parameters of a quantum system grow exponentially with the number of involved quantum particles hence the associated memory requirement goes well beyond the limit of best classic computers for quantum systems composed of a few dozen particles leading to huge challenges in their numerical simulation this implied that verification let alone design of new quantum devices and experiments is fundamentally limited to small system size it is not clear how the full potential of large quantum systems can be exploited here we present the concept of quantum computer designed quantum hardware and apply it to the field of quantum optics specifically we map complex experimental hardware for highdimensional manybody entangled photons into a gatebased quantum circuit we show explicitly how digital quantum simulation of boson sampling experiments can be realized then we illustrate how to design quantumoptical setups for complex entangled photon systems such as highdimensional greenbergerhornezeilinger states and their derivatives since photonic hardware is already on the edge of quantum supremacy the limit beyond which systems can no longer be calculated classically and the development of gatebased quantum computers is rapidly advancing our approach promises to be an useful tool for the future of quantum device design less,Quantum Computer-Aided design of Quantum Optics Hardware,"3 May, 2021"
32,Alán Aspuru-Guzik,the potential advantage of machine learning in quantum computers is a topic of intense discussion in the literature theoretical numerical and experimental explorations will most likely be required to understand its power there has been different algorithms proposed to exploit the probabilistic nature of variational quantum circuits for generative modelling in this paper we employ a hybrid architecture for quantum generative adversarial networks qgans and study their robustness in the presence of noise we devise a simple way of adding different types of noise to the quantum generator circuit and numerically simulate the noisy hybrid quantum generative adversarial networks hqgans to learn continuous probability distributions and show that the performance of hqgans remain unaffected we also investigate the effect of different parameters on the training time to reduce the computational scaling of the algorithm and simplify its deployment on a quantum computer we then perform the training on rigettis aspenqa quantum processing unit and present the results from the training our results pave the way for experimental exploration of different quantum machine learning algorithms on noisy intermediate scale quantum devices less,Noise robustness and experimental demonstration of a quantum generative adversarial network for continuous distributions,"29 March, 2021"
33,Alán Aspuru-Guzik,bayesian optimization has emerged as a powerful strategy to accelerate scientific discovery by means of autonomous experimentation however expensive measurements are required to accurately estimate materials properties and can quickly become a hindrance to exhaustive materials discovery campaigns here we introduce gemini a datadriven model capable of using inexpensive measurements as proxies for expensive measurements by correcting systematic biases between property evaluation methods we recommend using gemini for regression tasks with sparse data and in an autonomous workflow setting where its predictions of expensive to evaluate objectives can be used to construct a more informative acquisition function thus reducing the number of expensive evaluations an optimizer needs to achieve desired target values in a regression setting we showcase the ability of our method to make accurate predictions of dft calculated bandgaps of hybrid organicinorganic perovskite materials we further demonstrate the benefits that gemini provides to autonomous workflows by augmenting the bayesian optimizer phoenics to yeild a scalable optimization framework leveraging multiple sources of measurement finally we simulate an autonomous materials discovery platform for optimizing the activity of electrocatalysts for the oxygen evolution reaction realizing autonomous workflows with gemini we show that the number of measurements of a composition space comprising expensive and rare metals needed to achieve a target overpotential is significantly reduced when measurements from a proxy composition system with less expensive metals are available less,Gemini: Dynamic Bias Correction for Autonomous Experimentation and Molecular Simulation,"4 March, 2021"
34,Alán Aspuru-Guzik,in this work we present a linear optical implementation for analog quantum simulation of molecular vibronic spectra incorporating the noncondon scattering operation with a quadratically small truncation error thus far analog and digital quantum algorithms for achieving quantum speedup have been suggested only in the condon regime which refers to a transition dipole moment that is independent of nuclear coordinates for analog quantum optical simulation beyond the condon regime ie noncondon transitions the resulting nonunitary scattering operations must be handled appropriately in a linear optical network in this paper we consider the first and secondorder herzbergteller expansions of the transition dipole moment operator for the noncondon effect for implementation on linear optical quantum hardware we believe the method opens a new way to approximate arbitrary nonunitary operations in analog and digital quantum simulations we report insilico simulations of the vibronic spectra for naphthalene phenanthrene and benzene to support our findings less,Analog quantum simulation of non-Condon effects in molecular spectroscopy,"11 November, 2020"
35,Alán Aspuru-Guzik,generative models are becoming a tool of choice for exploring the molecular space these models learn on a large training dataset and produce novel molecular structures with similar properties generated structures can be utilized for virtual screening or training semisupervised predictive models in the downstream tasks while there are plenty of generative models it is unclear how to compare and rank them in this work we introduce a benchmarking platform called molecular sets moses to standardize training and comparison of molecular generative models moses provides a training and testing datasets and a set of metrics to evaluate the quality and diversity of generated structures we have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research the platform and source code are available at httpsgithubcommolecularsetsmoses less,Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models,"28 October, 2020"
36,Jimmy Ba,dataset distillation aims to learn a small synthetic dataset that preserves most of the information from the original dataset dataset distillation can be formulated as a bilevel metalearning problem where the outer loop optimizes the metadataset and the inner loop trains a model on the distilled data metagradient computation is one of the key challenges in this formulation as differentiating through the inner loop learning procedure introduces significant computation and memory costs in this paper we address these challenges using neural feature regression with pooling frepo achieving the stateoftheart performance with an order of magnitude less memory requirement and two orders of magnitude faster training than previous methods the proposed algorithm is analogous to truncated backpropagation through time with a pool of models to alleviate various types of overfitting in dataset distillation frepo significantly outperforms the previous methods on cifar tiny imagenet and imagenetk furthermore we show that highquality distilled data can greatly improve various downstream applications such as continual learning and membership inference defense less,Dataset Distillation using Neural Feature Regression,"1 June, 2022"
37,Jimmy Ba,we study the first gradient descent step on the firstlayer parameters boldsymbolw in a twolayer neural network fboldsymbolx fracsqrtnboldsymbolatopboldsymbolwtopboldsymbolx where boldsymbolwinmathbbrdtimes n boldsymbolainmathbbrn are randomly initialized and the training objective is the empirical mse loss fracnsumin fboldsymbolxiyi in the proportional asymptotic limit where ndntoinfty at the same rate and an idealized studentteacher setting we show that the first gradient update contains a rank spike which results in an alignment between the firstlayer weights and the linear component of the teacher model f to characterize the impact of this alignment we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on boldsymbolw with learning rate when f is a singleindex model we consider two scalings of the first step learning rate for small we establish a gaussian equivalence property for the trained feature map and prove that the learned kernel improves upon the initial random features model but cannot defeat the best linear model on the input whereas for sufficiently large we prove that for certain f the same ridge estimator on trained features can go beyond this linear regime and outperform a wide range of random features and rotationally invariant kernels our results demonstrate that even one gradient step can lead to a considerable advantage over random features and highlight the role of learning rate scaling in the initial phase of training less,High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation,"3 May, 2022"
38,Jimmy Ba,noisy labels are inevitable in large realworld datasets in this work we explore an area understudied by previous works how the networks architecture impacts its robustness to noisy labels we provide a formal framework connecting the robustness of a network to the alignments between its architecture and targetnoise functions our framework measures a networks robustness via the predictive power in its representations the test performance of a linear model trained on the learned representations using a small set of clean labels we hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise to support our hypothesis we provide both theoretical and empirical evidence across various neural network architectures and different domains we also find that when the network is wellaligned with the target function its predictive power in representations could improve upon stateoftheart sota noisylabeltraining methods in terms of test accuracy and even outperform sophisticated methods that use clean labels less,How Does a Neural Network's Architecture Impact Its Robustness to Noisy Labels?,"27 November, 2021"
39,Jimmy Ba,in learningassisted theorem proving one of the most critical challenges is to generalize to theorems unlike those seen at training time in this paper we introduce int an inequality theorem proving benchmark specifically designed to test agents generalization ability int is based on a procedure for generating theorems and proofs this procedures knobs allow us to measure different types of generalization each reflecting a distinct challenge characteristic to automated theorem proving in addition unlike prior benchmarks for learningassisted theorem proving int provides a lightweight and userfriendly theorem proving environment with fast simulations conducive to performing learningbased and searchbased research we introduce learningbased baselines and evaluate them across dimensions of generalization with the benchmark we then evaluate the same agents augmented with monte carlo tree search mcts at test time and show that mcts can help to prove new theorems less,INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving,"3 April, 2021"
40,Jimmy Ba,reinforcement learning has enabled agents to solve challenging tasks in unknown environments however manually crafting reward functions can be time consuming expensive and error prone to human error competing objectives have been proposed for agents to learn without external supervision but it has been unclear how well they reflect task rewards or human behavior to accelerate the development of intrinsic objectives we retrospectively compute potential objectives on precollected datasets of agent behavior rather than optimizing them online and compare them by analyzing their correlations we study input entropy information gain and empowerment across seven agents three atari games and the d game minecraft we find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward moreover input entropy and information gain correlate more strongly with human similarity than task reward does suggesting the use of intrinsic objectives for designing agents that behave similarly to human players less,Evaluating Agents without Rewards,"9 February, 2021"
41,Jimmy Ba,learning taskagnostic dynamics models in highdimensional observation spaces can be challenging for modelbased rl agents we propose a novel way to learn latent world models by learning to predict sequences of future actions conditioned on task completion these taskconditioned models adaptively focus modeling capacity on taskrelevant dynamics while simultaneously serving as an effective heuristic for planning with sparse rewards we evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior modelfree approaches less,Planning from Pixels using Inverse Dynamics Models,"4 December, 2020"
42,Jimmy Ba,in this work we focus on an analogical reasoning task that contains rich compositional structures ravens progressive matrices rpm to discover compositional structures of the data we propose the scattering compositional learner scl an architecture that composes neural networks in a sequence our scl achieves stateoftheart performance on two rpm datasets with a relative improvement on balancedraven and on pgm over the previous stateoftheart we additionally show that our model discovers compositional representations of objects attributes eg shape color size and their relationships eg progression union we also find that the compositional representation makes the scl significantly more robust to testtime domain shifts and greatly improves zeroshot generalization to previously unseen analogies less,"The Scattering Compositional Learner: Discovering Objects, Attributes, Relationships in Analogical Reasoning","8 July, 2020"
43,Jimmy Ba,what goals should a multigoal reinforcement learning agent pursue during training in longhorizon tasks when the desired test time goal distribution is too distant to offer a useful learning signal we argue that the agent should not pursue unobtainable goals instead it should set its own intrinsic goals that maximize the entropy of the historical achieved goal distribution we propose to optimize this objective by having the agent pursue past achieved goals in sparsely explored areas of the goal space which focuses exploration on the frontier of the achievable goal set we show that our strategy achieves an order of magnitude better sample efficiency than the prior state of the art on longhorizon multigoal tasks including maze navigation and block stacking less,Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning,"6 July, 2020"
44,Jimmy Ba,the choice of batchsize in a stochastic optimization algorithm plays a substantial role for both optimization and generalization increasing the batchsize used typically improves optimization but degrades generalization to address the problem of improving generalization while maintaining optimal convergence in largebatch training we propose to add covariance noise to the gradients we demonstrate that the learning performance of our method is more accurately captured by the structure of the covariance matrix of the noise rather than by the variance of gradients moreover over the convexquadratic we prove in theory that it can be characterized by the frobenius norm of the noise matrix our empirical studies with standard deep learning modelarchitectures and datasets shows that our method not only improves generalization performance in largebatch training but furthermore does so in a way where the optimization performance remains desirable and the training duration is not elongated less,An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise,"28 February, 2020"
45,Jimmy Ba,the vast majority of successful deep neural networks are trained using variants of stochastic gradient descent sgd algorithms recent attempts to improve sgd can be broadly categorized into two approaches adaptive learning rate schemes such as adagrad and adam and accelerated schemes such as heavyball and nesterov momentum in this paper we propose a new optimization algorithm lookahead that is orthogonal to these previous approaches and iteratively updates two sets of weights intuitively the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer we show that lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost we empirically demonstrate lookahead can significantly improve the performance of sgd and adam even with their default hyperparameter settings on imagenet cifar neural machine translation and penn treebank less,"Lookahead Optimizer: k steps forward, 1 step back","3 December, 2019"
46,Jimmy Ba,despite the recent successes in robotic locomotion control the design of robot relies heavily on human engineering automatic robot design has been a long studied subject but the recent progress has been slowed due to the large combinatorial search space and the difficulty in evaluating the found candidates to address the two challenges we formulate automatic robot design as a graph search problem and perform evolution search in graph space we propose neural graph evolution nge which performs selection on current candidates and evolves new ones iteratively different from previous approaches nge uses graph neural networks to parameterize the control policies which reduces evaluation cost on new candidates with the help of skill transfer from previously evaluated designs in addition nge applies graph mutation with uncertainty gmuc by incorporating model uncertainty which reduces the search space by balancing exploration and exploitation we show that nge significantly outperforms previous methods by an order of magnitude as shown in experiments nge is the first algorithm that can automatically discover kinematically preferred robotic graph structures such as a fish with two symmetrical flat sidefins and a tail or a cheetah with athletic front and back legs instead of using thousands of cores for weeks nge efficiently solves searching problem within a day on a single cpucore amazon ec machine less,Neural Graph Evolution: Towards Efficient Automatic Robot Design,"12 June, 2019"
47,Jimmy Ba,building agents to interact with the web would allow for significant improvements in knowledge understanding and representation learning however web navigation tasks are difficult for current deep reinforcement learning rl models due to the large discrete action space and the varying number of actions between the states in this work we introduce domqnet a novel architecture for rlbased web navigation to address both of these problems it parametrizes q functions with separate networks for different action categories clicking a dom element and typing a string input our model utilizes a graph neural network to represent the treestructured html of a standard web page we demonstrate the capabilities of our model on the miniwob environment where we can match or outperform existing work without the use of expert demonstrations furthermore we show x improvements in sample efficiency when training in the multitask setting allowing our model to transfer learned behaviours across tasks less,DOM-Q-NET: Grounded RL on Structured Language,"19 February, 2019"
48,Jimmy Ba,recurrent neural networks rnns provide stateoftheart performance in processing sequential data but are memory intensive to train limiting the flexibility of rnn models which can be trained reversible rnnsrnns for which the hiddentohidden transition can be reversedoffer a path to reduce the memory requirements of training as hidden states need not be stored and instead can be recomputed during backpropagation we first show that perfectly reversible rnns which require no storage of the hidden activations are fundamentally limited because they cannot forget information from their hidden state we then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of we extend our technique to attentionbased sequencetosequence models where it maintains performance while reducing activation memory cost by a factor of in the encoder and a factor of in the decoder less,Reversible Recurrent Neural Networks,"25 October, 2018"
49,Jimmy Ba,stochastic neural net weights are used in a variety of contexts including regularization bayesian neural nets exploration in reinforcement learning and evolution strategies unfortunately due to the large number of weights all the examples in a minibatch typically share the same weight perturbation thereby limiting the variance reduction effect of large minibatches we introduce flipout an efficient method for decorrelating the gradients within a minibatch by implicitly sampling pseudoindependent weight perturbations for each example empirically flipout achieves the ideal linear variance reduction for fully connected networks convolutional networks and rnns we find significant speedups in training neural networks with multiplicative gaussian perturbations we show that flipout is effective at regularizing lstms and outperforms previous methods flipout also enables us to vectorize evolution strategies in our experiments a single gpu with flipout can handle the same throughput as at least cpu cores using existing methods equivalent to a factorof cost reduction on amazon web services less,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,"2 April, 2018"
50,Jimmy Ba,we introduce adam an algorithm for firstorder gradientbased optimization of stochastic objective functions based on adaptive estimates of lowerorder moments the method is straightforward to implement is computationally efficient has little memory requirements is invariant to diagonal rescaling of the gradients and is well suited for problems that are large in terms of data andor parameters the method is also appropriate for nonstationary objectives and problems with very noisy andor sparse gradients the hyperparameters have intuitive interpretations and typically require little tuning some connections to related algorithms on which adam was inspired are discussed we also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework empirical results demonstrate that adam works well in practice and compares favorably to other stochastic optimization methods finally we discuss adamax a variant of adam based on the infinity norm less,Adam: A Method for Stochastic Optimization,"29 January, 2017"
51,Jimmy Ba,training stateoftheart deep neural networks is computationally expensive one way to reduce the training time is to normalize the activities of the neurons a recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a minibatch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case this significantly reduces the training time in feedforward neural networks however the effect of batch normalization is dependent on the minibatch size and it is not obvious how to apply it to recurrent neural networks in this paper we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case like batch normalization we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the nonlinearity unlike batch normalization layer normalization performs exactly the same computation at training and test times it is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks empirically we show that layer normalization can substantially reduce the training time compared with previously published techniques less,Layer Normalization,"21 July, 2016"
52,Jimmy Ba,motivated by the recent progress in generative models we introduce a model that generates images from natural language descriptions the proposed model iteratively draws patches on a canvas while attending to the relevant words in the description after training on microsoft coco we compare our model with several baseline generative models on image generation and retrieval tasks we demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset less,Generating Images from Captions with Attention,"29 February, 2016"
53,Jimmy Ba,convolutional neural networks cnn have achieved state of the art performance on both classification and segmentation tasks applying cnns to microscopy images is challenging due to the lack of datasets labeled at the single cell level we extend the application of cnns to microscopy image classification and segmentation using multiple instance learning mil we present the adaptive noisyand mil pooling function a new mil operator that is robust to outliers combining cnns with mil enables training cnns using full resolution microscopy images with global labels we base our approach on the similarity between the aggregation function used in mil and pooling layers used in cnns we show that training mil cnns endtoend outperforms several previous methods on both mammalian and yeast microscopy images without requiring any segmentation steps less,Classifying and Segmenting Microscopy Images Using Convolutional Multiple Instance Learning,"17 November, 2015"
54,Jimmy Ba,despite their success convolutional neural networks are computationally expensive because they must examine all image locations stochastic attentionbased models have been shown to improve computational efficiency at test time but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates borrowing techniques from the literature on training deep generative models we present the wakesleep recurrent attention model a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation less,Learning Wake-Sleep Recurrent Attention Models,"22 September, 2015"
55,Jimmy Ba,currently deep neural networks are the state of the art on problems such as speech recognition and computer vision in this extended abstract we show that shallow feedforward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models moreover in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model we evaluate our method on the timit phoneme recognition task and are able to train shallow fullyconnected nets that perform similarly to complex wellengineered deep convolutional architectures our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feedforward nets than those currently available less,Do Deep Nets Really Need to be Deep?,"10 October, 2014"
56,Fahiem Bacchus,propositional model counting or sat is the problem of computing the number of satisfying assignments of a boolean formula and many discrete probabilistic inference problems can be translated into a model counting problem to be solved by sat solvers generic exact sat solvers however are often not scalable to industriallevel instances in this paper we present neuro an approach for learning branching heuristics for exact sat solvers via evolution strategies es to reduce the number of branching steps the solver takes to solve an instance we experimentally show that our approach not only reduces the step count on similarly distributed heldout instances but it also generalizes to much larger instances from the same problem family the gap between the learned and the vanilla solver on larger instances is sometimes so wide that the learned solver can even overcome the run time overhead of querying the model and beat the vanilla in wallclock time by orders of magnitude less,Learning Branching Heuristics for Propositional Model Counting,"7 July, 2020"
57,Fahiem Bacchus,this is the proceedings of the twentyfirst conference on uncertainty in artificial intelligence which was held in edinburgh scotland july less,Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence (2005),"28 August, 2014"
58,Fahiem Bacchus,in probabilistic logic nilsson uses the device of a probability distribution over a set of possible worlds to assign probabilities to the sentences of a logical language in his paper nilsson concentrated on inference and associated computational issues this paper on the other hand examines the probabilistic semantics in more detail particularly for the case of firstorder languages and attempts to explain some of the features and limitations of this form of probability logic it is pointed out that the device of assigning probabilities to logical sentences has certain expressive limitations in particular statistical assertions are not easily expressed by such a device this leads to certain difficulties with attempts to give probabilistic semantics to default reasoning using probabilities assigned to logical sentences less,Probability Distributions Over Possible Worlds,"27 March, 2013"
59,Fahiem Bacchus,a number of writersjoseph halpern and fahiem bacchus among them have offered semantics for formal languages in which inferences concerning probabilities can be made our concern is different this paper provides a formalization of nonmonotonic inferences in which the conclusion is supported only to a certain degree such inferences are clearly invalid since they must allow the falsity of a conclusion even when the premises are true nevertheless such inferences can be characterized both syntactically and semantically the premises of probabilistic arguments are sets of statements as in a database or knowledge base the conclusions categorical statements in the language we provide standards for both this form of inference for which high probability is required and for an inference in which the conclusion is qualified by an intermediate interval of support less,Semantics for Probabilistic Inference,"13 March, 2013"
60,Fahiem Bacchus,we present a mechanism for constructing graphical models specifically bayesian networks from a knowledge base of general probabilistic information the unique feature of our approach is that it uses a powerful firstorder probabilistic logic for expressing the general knowledge base this logic allows for the representation of a wide range of logical and probabilistic information the model construction procedure we propose uses notions from direct inference to identify pieces of local statistical information from the knowledge base that are most appropriate to the particular event we want to reason about these pieces are composed to generate a joint probability distribution specified as a bayesian network although there are fundamental difficulties in dealing with fully general knowledge our procedure is practical for quite rich knowledge bases and it supports the construction of a far wider range of networks than allowed for by current template technology less,Using First-Order Probability Logic for the Construction of Bayesian Networks,"6 March, 2013"
61,Fahiem Bacchus,in previous work bghk bghk we have studied the randomworlds approach a particular and quite powerful method for generating degrees of belief ie subjective probabilities from a knowledge base consisting of objective firstorder statistical and default information but allowing a knowledge base to contain only objective information is sometimes limiting we occasionally wish to include information about degrees of belief in the knowledge base as well because there are contexts in which old beliefs represent important information that should influence new beliefs in this paper we describe three quite general techniques for extending a method that generates degrees of belief from objective information to one that can make use of degrees of belief as well all of our techniques are bloused on wellknown approaches such as crossentropy we discuss general connections between the techniques and in particular show that although conceptually and technically quite different all of the techniques give the same answer when applied to the randomworlds method less,Generating New Beliefs From Old,"27 February, 2013"
62,Fahiem Bacchus,we propose a new directed graphical representation of utility functions called ucpnetworks that combines aspects of two existing graphical models generalized additive models and cpnetworks the network decomposes a utility function into a number of additive factors with the directionality of the arcs reflecting conditional dependence of preference statements in the underlying qualitative preference ordering under a em ceteris paribus all else being equal interpretation this representation is arguably natural in many settings furthermore the strong cpsemantics ensures that computation of optimization and dominance queries is very efficient we also demonstrate the value of this representation in decision making finally we describe an interactive elicitation procedure that takes advantage of the linear nature of the constraints on tradeoff weights imposed by a ucpnetwork this procedure allows the network to be refined until the regret of the decision with minimax regret with respect to the incompletely specified utility function falls below a specified threshold eg the cost of further questioning less,UCP-Networks: A Directed Graphical Representation of Conditional Utilities,"10 January, 2013"
63,Fahiem Bacchus,the promise of lifted probabilistic inference is to carry out probabilistic inference in a relational probabilistic model without needing to reason about each individual separately grounding out the representation by treating the undistinguished individuals as a block current exact methods still need to ground out in some cases typically because the representation of the intermediate results is not closed under the lifted operations we set out to answer the question as to whether there is some fundamental reason why lifted algorithms would need to ground out undifferentiated individuals we have two main results we completely characterize the cases where grounding is polynomial in a population size and show how we can do lifted inference in time polynomial in the logarithm of the population size for these cases for the case of noargument and singleargument parametrized random variables where the grounding is not polynomial in a population size we present lifted inference which is polynomial in the population size whereas grounding is exponential neither of these cases requires reasoning separately about the individuals that are not explicitly mentioned less,Towards Completely Lifted Search-based Probabilistic Inference,"21 July, 2011"
64,Fahiem Bacchus,agents interacting with an incompletely known world need to be able to reason about the effects of their actions and to gain further information about that world they need to use sensors of some sort unfortunately both the effects of actions and the information returned from sensors are subject to error to cope with such uncertainties the agent can maintain probabilistic beliefs about the state of the world with probabilistic beliefs the agent will be able to quantify the likelihood of the various outcomes of its actions and is better able to utilize the information gathered from its errorprone actions and sensors in this paper we present a model in which we can reason about an agents probabilistic degrees of belief and the manner in which these beliefs change as various actions are executed we build on a general logical theory of action developed by reiter and others formalized in the situation calculus we propose a simple axiomatization that captures an agents state of belief and the manner in which these beliefs change when actions are executed our model displays a number of intuitively reasonable properties less,Reasoning about Noisy Sensors and Effectors in the Situation Calculus,"9 September, 1998"
65,Ravin Balakrishnan,"Online synchronous tutoring allows for immediate engagement between instructors and audiences over distance. However, tutoring physical skills remains challenging because current telepresence approaches may not allow for adequate spatial awareness, viewpoint control of the demonstration activities scattered across an entire work area, and the instructor’s sufficient awareness of the audience. We present Asteroids, a novel approach for tangible robotic telepresence, to enable workbench-scale physical embodiments of remote people and tangible interactions by the instructor. With Asteroids, the audience can actively control a swarm of mini-telepresence robots, change camera positions, and switch to other robots’ viewpoints. Demonstrators can perceive the audiences’ physical presence while using tangible manipulations to control the audience’s viewpoints and presentation flow.",ASTEROIDS: Exploring Swarms of Mini-Telepresence Robots for Physical Skill Demonstration,2022-04-27 00:00:00
66,Ravin Balakrishnan,"Remotely controlled camera drones can support live, dynamic, and interactive virtual tours for travelers to overcome distance, expense, and health barriers. Yet, assigning one drone to one traveler may incur unnecessary waste of resources, and an abundance of concurrent drones raises safety concerns. While sharing the input and output of a single drone among multiple concurrent users can alleviate these limitations, standard control sharing protocols, such as turn-taking, are often inefficient. We present Constellation, a multi-user drone control system that synthesizes diverse user goals and generates efficient flight paths for the group. It supports point-of-interest specification on both static 3D environmental maps and live camera views. The generated paths minimize all users’ total extra waiting time.",Constellation: a Multi-User Interface for Remote Drone Tours,2021-11-09 00:00:00
67,Ravin Balakrishnan,"An increasingly popular way of experiencing remote places is by viewing 360 virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360 video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a ",Route tapestries: Navigating 360 virtual tour videos using slit-scan visualizations,2021-10-10 00:00:00
68,Ravin Balakrishnan,"Manipulating virtual objects using bare hands has been an attractive interaction paradigm in virtual and augmented reality due to its intuitive nature. However, one limitation of freehand input lies in the ambiguous resulting effect of the interaction. The same gesture performed on a virtual object could invoke different operations on the object depending on the context, object properties, and user intention. We present an experimental analysis of a set of disambiguation techniques in a virtual reality environment, comparing three input modalities (head gaze, speech, and foot tap) paired with three different timings in which options appear to resolve ambiguity (before, during, and after an interaction). The results indicate that using head gaze for disambiguation during an interaction with the object achieved the best performance.",Disambiguation techniques for freehand object manipulations in virtual reality,2020-03-22 00:00:00
69,Ravin Balakrishnan,"Camera drones, a rapidly emerging technology, offer people the ability to remotely inspect an environment with a high degree of mobility and agility. However, manual remote piloting of a drone is prone to errors. In contrast, autopilot systems can require a significant degree of environmental knowledge and are not necessarily designed to support flexible visual inspections. Inspired by camera manipulation techniques in interactive graphics, we designed StarHopper, a novel touch screen interface for efficient object-centric camera drone navigation, in which a user directly specifies the navigation of a drone camera relative to a specified object of interest. The system relies on minimal environmental information and combines both manual and automated control mechanisms to give users the freedom to remotely explore an environment with efficiency and accuracy. A lab study shows that StarHopper offers an efficiency gain of 35.4% over manual piloting, complimented by an overall user preference towards our object-centric navigation system.",StarHopper: A Touch Interface for Remote Object-Centric Drone Navigation,2019-12-21 00:00:00
70,Ravin Balakrishnan,"Assisting input from a keyboard is described. In an embodiment, a processor receives a plurality of key-presses from the keyboard comprising alphanumeric data for input to application software executed at the processor. The processor analyzes the plurality of key-presses to detect at least one predefined typing pattern, and, in response, controls a display device to display a representation of at least a portion of the keyboard in association with a user interface of the application software. In another embodiment, a computer device has a keyboard and at least one sensor arranged to monitor at least a subset of keys on the keyboard, and detect an object within a predefined distance of a selected key prior to activation of the selected key. The processor then controls the display device to display a representation of a portion of the keyboard comprising the selected key.",Assisting input from a keyboard,2019-09-10 00:00:00
71,Ravin Balakrishnan,"Online language lessons have adopted live broadcasted videos to provide more real-time interactive experiences between language teachers and learners. However, learner interactions are primarily limited to the built-in text chat in the live stream. Using text alone, learners cannot get feedback on important aspects of a language, such as speaking skills, that are afforded only by offering richer types of interactions. We present results from a 2-week in-the-wild study, in which we investigate the use of text, audio, video, image, and stickers as interaction tools for language teachers and learners in live streaming. Our language teacher explored three different teaching strategies over four live streamed English lessons, while nine students watched and interacted using multimodal tools. The findings reveal that multimodal communication yields instant feedback and increased engagement,",Integrating multimedia tools to enrich interactions in live streaming for language learning,2019-05-02 00:00:00
72,Ravin Balakrishnan,"Microarrays can be utilized to determine the comparative amount of particular mRNAs in two or more tissue samples for thousands of genes concurrently. As the supremacy of this technique has been identified, various open queries arise about suitable examination of microarray data. The multicategory cancer classification is playing a vital role in the field of medical sciences. As the numbers of cancer victims are increasing steadily, the necessity of the cancer classification techniques has become indispensible. In this research, initially preprocessing and normalization process is carried out to select the best gene datasets. Then, a combination of Advanced Integer-Coded Genetic Algorithm (AICGA) and Extreme Learning Machine (ELM), with refined group search optimizer (RGSO) technique is used for gene selection and cancer classification. AICGA is used with RGSO Based ELM classifier to choose an optimal set of genes which results in an efficient hybrid algorithm that can handle sparse data and sample imbalance. The refined group search optimizer based extreme learning machine is used to carry out the classification process. In the proposed RGSO based ELM, the weights and bias to ELM are optimized using RGSO for better simplification and classification of large value of gene datasets. The performance of the proposed approach is evaluated and the results are compared with existing methods. The proposed approaches are applied for real time",Microarray gene expression and multiclass cancer classification using extreme learning machine (ELM) with refined group search optimizer (RGSO),2019
73,Ravin Balakrishnan,the movement of a users face easily detected by a smartphones front camera is an underexploited input modality for mobile interactions we introduce three sets of faceengaged interaction techniques for augmenting the traditional mobile inputs which leverages the combination of the head movements with touch gestures and device motions all sensed via the phones builtin sensors we systematically present the space of design considerations for mobile interactions using one or more of the three input modalities ie touch motion and head the additional affordances of the proposed techniques expand the mobile interaction vocabulary and can facilitate unique usage scenarios such as onehand or touchfree interaction an initial evaluation was conducted and users had positive reactions to the new techniques indicating the promise of an intuitive and convenient user experience less,Augmenting Mobile Phone Interaction with Face-Engaged Gestures,"1 October, 2016"
74,Ashton Anderson,"In recent years, the “creator economy” has emerged as a disruptive force in creative industries. Independent creators can now reach large and diverse audiences through online platforms, and membership platforms have emerged to connect these creators with fans who are willing to financially support them. However, the structure and dynamics of how membership platforms function on a large scale remain poorly understood. In this work, we develop an analysis framework for the study of membership platforms and apply it to the complete set of Patreon pledges exceeding $2 billion since its inception in 2013 until the end of 2020. We analyze Patreon activity through three perspectives: patrons (demand), creators (supply), and the platform as a whole. We find several important phenomena that help explain how membership platforms operate. Patrons who pledge to a narrow set of creators are more loyal, but churn off the platform more often. High-earning creators attract large audiences, but these audiences are less likely to pledge to other creators. Over its history, Patreon diversified into many topics and launched higher-earning creators over time. Our analysis framework and results shed light on the functioning of membership platforms and have implications for the creator economy.",Quantifying the Creator Economy: A Large-Scale Analysis of Patreon,2022-05-31 00:00:00
75,Ashton Anderson,"Digital media platforms give users access to enormous amounts of content that they must explore to avoid boredom and satisfy their needs for heterogeneity. Existing strands of work across psychology, marketing, computer science, and music underscore the importance of the lifecycle to understanding exploratory behavior, but they are also often inconsistent with each other. In this study, we examine how users explore online content on Spotify over time, whether by discovering entirely novel music or by refreshing their listening habits from one time frame to the next. We find clear differences between users at different points of their off-platform lifecycles, with younger listeners consistently exploring unknown content less and exploiting known content more. Across their on-platform histories, users also explore in bursts by following seasonal cycles and exploratory phases. We also find that these patterns of exploration do not translate to other notions of heterogeneity like diversity; notably, younger listeners are more diverse in their consumption despite exploring less. Exploration and diversity thus capture different ways in which people find variety, potentially accounting for the inconsistencies in existing work. Together, these nuanced dynamics of exploration suggest that online platforms may be better poised to support users by incorporating different measures of heterogeneous consumption.",The Dynamics of Exploration on Spotify,2022-05-31 00:00:00
76,Ashton Anderson,"Social media reduces barriers for the formation of large, self-organizing grassroots communities. For political campaigns this poses significant opportunities to address declining party membership, but also reputational risks and potential loss of campaign coherence. While balancing these factors is often done informally, we adopt a behavioural approach by using neural community embeddings to evaluate online communities along cultural, political, and demographic dimensions. We apply this technique to the 2020 US Democratic presidential primaries and the website Reddit, providing novel insights into the important tension between campaigns and third-party actors. Using two benchmark comparison classes, we demonstrate that our embedding dimensions mirror their offline analogues, but more so the views of a candidate's supporters than the candidate's themselves. Finally, we introduce temporal aspects to our community embedding to evaluate the stability of political communities and their interrelations. These analyses serve as an exploration and application of our novel embedding methodology, and give insight into the relationship between online communities and the movements they support.",Measuring Alignment of Online Grassroots Political Communities with Political Campaigns,2022-05-31 00:00:00
77,Ashton Anderson,"Online recommendation systems are prone to create filter bubbles, whereby users are only recommended content narrowly aligned with their historical interests. In the case of media recommendation, this can reinforce political polarization by recommending topical content (eg, on the economy) at one extreme end of the political spectrum even though this topic has broad coverage from multiple political viewpoints that would provide a more balanced and informed perspective for the user. Historically, Maximal Marginal Relevance (MMR) has been used to diversify result lists and even mitigate filter bubbles, but suffers from three key drawbacks:(1) MMR directly sacrifices relevance for diversity,(2) MMR typically diversifies across all content and not just targeted dimensions (eg, political polarization), and (3) MMR is inefficient in practice due to the need to compute pairwise similarities between recommended items. To simultaneously address these limitations, we propose a novel methodology that trains Concept Activation Vectors (CAVs) for targeted topical dimensions (eg, political polarization). We then modulate the latent embeddings of user preferences in a state-of-the-art VAE-based recommender system to diversify along the targeted dimension while preserving topical relevance across orthogonal dimensions. Our experiments show that our Targeted Diversification VAE-based Collaborative Filtering (TDVAE-CF) methodology better preserves relevance of content to user preferences across a range of diversification levels in comparison",Mitigating the Filter Bubble while Maintaining Relevance: Targeted Diversification with VAE-based Recommender Systems,2022
78,Ashton Anderson,"The advent of machine learning models that surpass human decision-making ability in complex domains has initiated a movement towards building AI systems that interact with humans. Many building blocks are essential for this activity, with a central one being the algorithmic characterization of human behavior. While much of the existing work focuses on aggregate human behavior, an important long-range goal is to develop behavioral models that specialize to individual people and can differentiate among them. To formalize this process, we study the problem of behavioral stylometry, in which the task is to identify a decision-maker from their decisions alone. We present a transformer-based approach to behavioral stylometry in the context of chess, where one attempts to identify the player who played a set of games. Our method operates in a few-shot classification framework, and can correctly identify a player from among thousands of candidate players with 98% accuracy given only 100 labeled games. Even when trained on amateur play, our method generalises to out-of-distribution samples of Grandmaster players, despite the dramatic differences between amateur and world-class players. Finally, we consider more broadly what our resulting embeddings reveal about human style in chess, as well as the potential ethical implications of powerful methods for identifying individuals from behavioral data.",Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess,2021-12-06 00:00:00
79,Ashton Anderson,"Mass selection into groups of like-minded individuals may be fragmenting and polarizing online society, particularly with respect to partisan differences 1, 2, 3, 4. However, our ability to measure the social makeup of online communities and in turn, to understand the social organization of online platforms, is limited by the pseudonymous, unstructured and large-scale nature of digital discussion. Here we develop a neural-embedding methodology to quantify the positioning of online communities along social dimensions by leveraging large-scale patterns of aggregate behaviour. Applying our methodology to 5.1 billion comments made in 10,000 communities over 14 years on Reddit, we measure how the macroscale community structure is organized with respect to age, gender and US political partisanship. Examining political content, we find that Reddit underwent a significant polarization event around the 2016 US ",Quantifying social organization and political polarization in online platforms,2021/12
80,Ashton Anderson,"What makes written text appealing? In this registered report protocol, we propose to study the linguistic characteristics of news headline success using a large-scale dataset of field experiments (A/B tests) conducted on the popular website Upworthy comparing multiple headline variants for the same news articles. This unique setup allows us to control for factors that can have crucial confounding effects on headline success. Based on prior literature and a pilot partition of the data, we formulate hypotheses about the linguistic features that are associated with statistically superior headlines. We will test our hypotheses on a much larger partition of the data that will become available after the publication of this registered report protocol. Our results will contribute to resolving competing hypotheses about the linguistic features that affect the success of text and will provide avenues for research into the psychological mechanisms that are activated by those features.",Linguistic effects on news headline success: Evidence from thousands of online field experiments (Registered Report Protocol),2021-09-15 00:00:00
81,Ashton Anderson,"As online platforms become ubiquitous, there is growing concern that their use can potentially lead to negative outcomes in users' personal lives, such as disrupted sleep and impacted social relationships. A central question in the literature studying these problematic effects is whether they are associated with the amount of time users spend on online platforms. This is often addressed by either analyzing self-reported measures of time spent online, which are generally inaccurate, or using objective metrics derived from server logs or tracking software. Nonetheless, how the two types of time measures comparatively relate to problematic effects -- whether they complement or are redundant with each other in predicting problematicity -- remains unknown. Additionally, transparent research into this question is hindered by the literature's focus on closed platforms with inaccessible data, as well as selective analytical ",The Complementary Nature of Perceived and Actual Time Spent Online in Measuring Digital Well-being,2021-04-22 00:00:00
82,Ashton Anderson,"We consider the problem of predicting users’ preferences on online platforms. We build on recent findings suggesting that users’ preferences change over time, and that helping users expand their horizons is important in ensuring that they stay engaged. Most existing models of user preferences attempt to capture simultaneous preferences:“Users who like A tend to like B as well”. In this paper, we argue that these models fail to anticipate changing preferences. To overcome this issue, we seek to understand the structure that underlies the evolution of user preferences. To this end, we propose the Preference Transition Model (PTM), a dynamic model for user preferences towards classes of items. The model enables the estimation of transition probabilities between classes of items over time, which can be used to estimate how users’ tastes are expected to evolve based on their past history. ",Where To Next? A Dynamic Model of User Preferences,2021-04-19 00:00:00
83,Ashton Anderson,"Music shapes our individual and collective identities, and in turn is shaped by the social and cultural contexts it occurs in. Qualitative approaches to the study of music have uncovered rich connections between music and social context but are limited in scale, while computational approaches process large amounts of musical data but lack information on the social contexts music is embedded in. In this work, we develop a set of neural embedding methods to understand the social contexts of online music sharing, and apply them to a novel dataset containing 1.3 M instances of music sharing in Reddit communities. We find that the patterns of how people share music in public are related to, but often differ from, how they listen to music in private. We cluster artists into social genres that are based entirely on aggregate sharing patterns and reflect where artists are invoked. We also characterize the social and cultural contexts music is shared in by measuring associations with social dimensions such as age and political affiliation. Finally, we observe that a significant amount of sharing is attributable to extra-musical factors—additional meanings that people have associated with songs. We develop two methods to quantify the extra-musicality of music sharing. Our methodology is widely applicable to the study of online social contexts, and our results reveal novel cultural associations that contribute to a better understanding of the online music ecosystem",Imagine All the People: Characterizing Social Music Sharing on Reddit,2021
84,Ashton Anderson,"We consider the problem of predicting users’ preferences on online platforms. We build on recent findings suggesting that users’ preferences change over time, and that helping users expand their horizons is important in ensuring that they stay engaged. Most existing models of user preferences attempt to capture simultaneous preferences:“Users who like 𝐴 tend to like 𝐵 as well”. In this paper, we argue that these models fail to anticipate changing preferences. To overcome this issue, we seek to understand the structure that underlies the evolution of user preferences. To this end, we propose the Preference Transition Model (PTM), a dynamic model for user preferences towards classes of items. The model enables the estimation of transition probabilities between classes of items over time, which can be used to estimate how users’ tastes are expected to evolve based on their past history. We test our model’s predictive performance on a number of different prediction tasks on data from three different domains: music streaming, restaurant recommendations and movie recommendations, and find that it outperforms competing approaches. We then focus on a music application, and inspect the structure learned by our model. We find that the PTM uncovers remarkable regularities in users’ preference trajectories over time. We believe that these findings could inform a new generation of dynamic, diversity-enhancing recommender systems.",Where To Next? A Dynamic Model of User Preferences,2021
85,Angela Demke Brown,"This qualitative study examines the determinants of effective inter-organization information sharing in the Health Capital Planning process (the process), primarily in the final stage of the process which focuses on the review of final expenses and release of a holdback. Using thematic analysis and building off a scoping review that was conducted in preparation for this study, we provide a framework for effective information sharing during the process. We interviewed 17 leaders from the Government of Ontario and hospitals across the province. The results of the interviews indicate that the most essential determinants of effective inter-organization information sharing in the process: organizational characteristics; reducing complex bureaucracies; preserving human resources and expertise; clear and standardized information; reducing policy changes; networks; negotiation abilities; information technology; training; record retention; and early planning. This study confirmed the need for effective intra-organization and interpersonal information sharing to achieve successful inter-organization information sharing.",The determinants of effective inter-organization information sharing in the health capital planning process.,2022-05-04 00:00:00
86,Angela Demke Brown,"This special section of the ACM Transactions on Storage presents some of the highlights of the storage-related papers published in the 15th USENIX Symposium on Operating Systems Design and Implementation (OSDI’21). The OSDI Symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation. Despite OSDI’s broad view of the systems area, the design and implementation of storage systems have always been important topics for OSDI. In particular, out of the 165 OSDI’21 submissions, 26 of them (= 16%) addressed various storage-related aspects; out of its 31 accepted papers, 6 (= 19%) addressed storage-related themes, constituting a significant part of the OSDI’21 program. Out of the above, for this special section of ACM Transactions on Storage, we selected two high-quality papers. Each includes some additional material, which has been …",Introduction to the Special Section on USENIX OSDI 2021,2022-01-29 00:00:00
87,Angela Demke Brown,"Deterministic databases offer several benefits: they ensure serializable execution while avoiding concurrency-control related aborts, and they scale well in distributed environments. Today, most deterministic database designs use partitioning to scale up and avoid contention. However, partitioning requires significant programmer effort, leads to poor performance under skewed workloads, and incurs unnecessary overheads in certain uncontended workloads.",Caracal: Contention management with deterministic concurrency control,2021-10-26 00:00:00
88,Angela Demke Brown,"We propose Spiffy, an annotation language for specifying the on-disk format of a file system. File-system developers annotate the data structures of a file system, and we use these annotations to generate a library that allows identifying, parsing, and traversing file-system metadata, providing support for both offline and online storage applications. This approach simplifies the development of storage applications ",Spiffy: Enabling File-System Aware Storage Applications,2020-08-04 00:00:00
89,Angela Demke Brown,"CSV is a popular Open Data format widely used in a variety of domains for its simplicity and effectiveness in storing and disseminating data. Unfortunately, data published in this format often does not conform to strict specifications, making automated data extraction from CSV files a painful task. While table discovery from HTML pages or spreadsheets has been studied extensively, extracting tables from CSV files still poses a considerable challenge due to their loosely defined format and limited embedded metadata.",Pytheas pattern-based table discovery in CSV files,2020-07-01 00:00:00
90,Angela Demke Brown,"In client-server architectures, there are cases where there is a need to transfer large amounts of data from the server to the client (or less frequently, in the opposite direction). Mobile application markets are a notable example where the clients need to download large chunks of data in the order of megabytes at a time, compared to a typical RPC request which is in the order of kilobytes.",Deduplicating future data transfer using data exchanged in the past to decrease mobile bandwidth usage,2020-06-15 00:00:00
91,Angela Demke Brown,"Many file system applications such as defragmentation tools, file system checkers or data recovery tools, operate at the storage layer. Today, developers of these storage applications require detailed knowledge of the file system format, which takes a significant amount of time to learn, often by trial and error, due to insufficient documentation or specification of the format. Furthermore, these applications perform ad-hoc processing of the file-system metadata, leading to bugs and vulnerabilities.",Spiffy: Enabling {File-System} Aware Storage Applications,2018
92,Angela Demke Brown,"File system management applications, such as data scrubbers, defragmentation tools, resizing tools, and partition editors, are essential for maintaining, optimizing, and administering storage systems. These applications require fine-grained control over file-system metadata and data, such as the ability to migrate a data block to another physical location. Such control is not available with the VFS API, and so these applications bypass the VFS and access and modify file-system metadata directly. As a result, these applications do not work across file systems, and must be developed from scratch for each file system, which involves significant engineering effort and impedes adoption of new file systems.",Breaking apart the {VFS} for managing file systems,2018
93,Angela Demke Brown,"Primary-backup replication is commonly used for providing fault tolerance in databases. It is performed by replaying the database recovery log on a backup server. Such a scheme raises several challenges for modern, high-throughput multi-core databases. It is hard to replay the recovery log concurrently, and so the backup can become the bottleneck. Moreover, with the high transaction rates on the primary, the log transfer can cause network bottlenecks. Both these bottlenecks can significantly slow the primary database.",Scalable replay-based replication for fast databases,2017-09-01 00:00:00
94,Angela Demke Brown,"This special issue of the ACM Transactions on Storage presents some of the highlights of the 14th USENIX Conference on File and Storage Technologies (FAST’16). In the 14 years since its inception, FAST has grown into a very active community, attracting 115 submissions and a record number of over 550 attendees in 2016. FAST’16 continues the conference’s tradition of bringing together storage-system researchers and practitioners to explore new directions in the design, implementation, evaluation, and deployment of storage systems. FAST takes a broad view of storage systems, encompassing everything from low-level storage devices to information management systems. From this broad scope, we selected four high-quality articles for publication in this special issue of ACM Transactions on Storage. The first article, which was also selected as one of the best papers at the conference, is “Writes Wrought Right ",Introduction to the Special Issue on USENIX FAST 2016,2017-03-24 00:00:00
95,Angela Demke Brown,"Disaggregation of resources in the data center, especially at the rack-scale, offers the opportunity to use valuable resources more efficiently. It is common that mass storage racks in large-scale clouds are filled with servers with Hard Disk Drives (HDDs) attached directly to each of them, either using SATA or SAS depending on the number of HDDs.",Understanding {Rack-Scale} Disaggregated Storage,2017
96,Angela Demke Brown,"Cluster computing frameworks such as Apache Hadoop and Apache Spark are commonly used to analyze large data sets. The analysis often involves running multiple, similar queries on the same data sets. This data reuse should improve query performance, but we find that these frameworks schedule query tasks independently of each other and are thus unable to exploit the data sharing across these tasks. We present Quartet, a system that leverages information on cached data to schedule together tasks that share data. Our preliminary results are promising, showing that Quartet can increase the cache hit rate of Hadoop and Spark jobs by up to 54%. Our results suggest a shift in the way we think about job and task scheduling today, as Quartet is expected to perform better as more jobs are dispatched on the same data.",Quartet: Harmonizing task scheduling and caching for cluster computing,2016
97,Angela Demke Brown,"The rapid growth of spatiotemporal Big Data is fueling the emergence and growth of many applications. Many of these applications are characterized by complex spatiotemporal queries. An important category of such queries is the trajectory-based spatiotemporal topological join queries, which combine a trajectory dataset and a spatial objects dataset based on spatiotemporal predicates. Although these queries have many important use-cases, they have not received much attention from the research community. We systematically evaluate several feasible in-memory spatiotemporal topological join algorithms, using existing trajectory index (TB-tree) and spatial index (STR). We show that even the best among these algorithms is long running and not scalable. To address the performance problems of these algorithms we introduce PISTON, a parallel in-memory indexing system targeted for spatiotemporal topological ",Parallel in-memory trajectory-based spatiotemporal topological join,2015-10-29 00:00:00
98,Angela Demke Brown,"Storage systems rely on maintenance tasks, such as backup and layout optimization, to ensure data availability and good performance. These tasks access large amounts of data and can significantly impact foreground applications. We argue that storage maintenance can be performed more efficiently by prioritizing processing of data that is currently cached in memory. Data can be cached either due to other maintenance tasks requesting it previously, or due to overlapping foreground I/O activity.",Opportunistic storage maintenance,2015-10-04 00:00:00
99,Anthony Bonner,measuring graph clustering quality remains an open problem to address it we introduce quality measures based on comparisons of intra and intercluster densities an accompanying statistical test of the significance of their differences and a stepbystep routine for clustering quality assessment our null hypothesis does not rely on any generative model for the graph unlike modularity which uses the configuration model as a null model our measures are shown to meet the axioms of a good clustering quality function unlike the very commonly used modularity measure they also have an intuitive graphtheoretic interpretation a formal statistical interpretation and can be easily tested for significance our work is centered on the idea that well clustered graphs will display a significantly larger intracluster density than intercluster density we develop tests to validate the existence of such a cluster structure we empirically explore the behavior of our measures under a number of stress test scenarios and compare their behavior to the commonly used modularity and conductance measures empirical stress test results confirm that our measures compare very favorably to the established ones in particular they are shown to be more responsive to graph structure and less sensitive to sample size and breakdowns during numerical implementation and less sensitive to uncertainty in connectivity these features are especially important in the context of larger data sets or when the data may contain errors in the connectivity patterns less,A Statistical Density-Based Analysis of Graph Clustering Algorithm Performance,"18 March, 2020"
100,Anthony Bonner,knn is one of the most popular classification methods but it often fails to work well with inappropriate choice of distance metric or due to the presence of numerous classirrelevant features linear feature transformation methods have been widely applied to extract classrelevant information to improve knn classification which is very limited in many applications kernels have been used to learn powerful nonlinear feature transformations but these methods fail to scale to large datasets in this paper we present a scalable nonlinear feature mapping method based on a deep neural network pretrained with restricted boltzmann machines for improving knn classification in a largemargin framework which we call dnetknn dnetknn can be used for both classification and for supervised dimensionality reduction the experimental results on two benchmark handwritten digit datasets show that dnetknn has much better performance than largemargin knn using a linear mapping and knn based on a deep autoencoder pretrained with retricted boltzmann machines less,Large-Margin kNN Classification Using a Deep Encoder Network,"9 June, 2009"
101,Allan Borodin,we consider the online stochastic matching problem for bipartite graphs where edges adjacent to an online node must be probed to determine if they exist based on known edge probabilities our algorithms respect commitment in that if a probed edge exists it must be used in the matching we study this matching problem subject to a downwardclosed constraint on each online nodes allowable edge probes our setting generalizes the commonly studied patience or timeout constraint which limits the number of probes that can be made to an online nodes adjacent edges we introduce a new lp that we prove is a relaxation of an optimal offline probing algorithm the adaptive benchmark and which overcomes the limitations of previous lp relaxations a tight frac ratio when the stochastic graph is generated from a known stochastic type graph where the tth online node is drawn independently from a known distribution scrdt and is chosen adversarially we refer to this setting as the known id stochastic matching problem with adversarial arrivals a e ratio when the stochastic graph is generated from a known stochastic type graph where the tth online node is drawn independently from a known distribution scrdt and is a random permutation we refer to this setting as the known id stochastic matching problem with random order arrivals our results improve upon the previous best competitive ratio of in the known iid setting against the standard adaptive benchmark moreover we are the first to study the prophet secretary matching problem in the context of probing where we match the best known classical result less,Prophet Matching Meets Probing with Commitment,"29 July, 2021"
102,Allan Borodin,within the context of stochastic probing with commitment we consider the online stochastic matching problem that is the one sided online bipartite matching problem where edges adjacent to an online node must be probed to determine if they exist based on known edge probabilities if a probed edge exists it must be used in the matching if possible we study this problem in the generality of a patience or budget constraint which limits the number of probes that can be made to edges adjacent to an online node the patience constraint results in modelling and computational efficiency issues that are not encountered in the special cases of unit patience and full ie unlimited patience the stochastic matching problem leads to a variety of settings our main contribution is to provide a new lp relaxation and a unified approach for establishing new and improved competitive bounds in three different input model settings namely adversarial random order and known iid in all these settings the algorithm does not have any control on the ordering of the online nodes we establish competitive bounds in these settings all of which generalize the standard nonstochastic setting when edges do not need to be probed ie exist with certainty all of our competitive ratios hold for arbitrary edge probabilities and patience constraints a e ratio when the stochastic graph is known offline vertices are weighted and online arrivals are adversarial a e ratio when the stochastic graph is known edges are weighted and online arrivals are given in random order ie in rom the random order model a e ratio when online arrivals are drawn iid from a known stochastic type graph and edges are weighted a tight e ratio when the stochastic graph is unknown edges are weighted and online arrivals are given in random order less,"Bipartite Stochastic Matching: Online, Random Order, and I.I.D. Models","6 January, 2021"
103,Allan Borodin,we perform an experimental study of algorithms for online bipartite matching under the known iid input model with integral types in the last decade there has been substantial effort in designing complex algorithms with the goal of improving worstcase approximation ratios our goal is to determine how these algorithms perform on more practical instances rather than worstcase instances in particular we are interested in whether the ranking of the algorithms by their worstcase performance is consistent with the ranking of the algorithms by their averagecasepractical performance we are also interested in whether preprocessing times and implementation difficulties that are introduced by these algorithms are justified in practice to that end we evaluate these algorithms on different random inputs as well as reallife instances obtained from publicly available repositories we compare these algorithms against several simple greedystyle algorithms most of the complex algorithms in the literature are presented as being nongreedy ie an algorithm can intentionally skip matching a node that has available neighbors to simplify the analysis every such algorithm can be turned into a greedy one without hurting its worstcase performance on our benchmarks nongreedy versions of these algorithms perform much worse than their greedy versions greedy versions perform about as well as the simplest greedy algorithm by itself this together with our other findings suggests that simplest greedy algorithms are competitive with the stateoftheart worstcase algorithms for online bipartite matching on many averagecase and practical input families greediness is by far the most important property of online algorithms for bipartite matching less,An Experimental Study of Algorithms for Online Bipartite Matching,"14 August, 2018"
104,Allan Borodin,we examine the case of items with a limited shelflife where storing an item before consumption may carry a cost to a buyer or distributor for example eggs milk or groupon coupons have a fixed expiry date and seasonal goods can suffer a decrease in value we show how this setting contrasts with recent results by berbeglia et al arxivv for items with infinite shelflife we prove tight bounds on the sellers profits showing how they relate to the items shelflife we show counterintuitively that in our limited shelflife setting increasing storage costs can sometimes lead to less profit for the seller which cannot happen when items have unlimited shelflife we also provide an algorithm that calculates optimal prices finally we examine empirically the relationship between profits and buyer utility as the storage cost and shelflife duration change and observe properties some of which are unique to the limited shelflife setting less,Seasonal Goods and Spoiled Milk: Pricing for a Limited Shelf-Life,"6 May, 2018"
105,Allan Borodin,recently renault studied the dual bin packing problem in the perrequest advice model of online algorithms he showed that given o advice bits for each input item allows approximating the dual bin packing problem online to within a factor of renault asked about the advice complexity of dual bin packing in the tapeadvice model of online algorithms we make progress on this question let s be the maximum bit size of an input item weight we present a conceptually simple online algorithm that with total advice oleftfracs log nright approximates the dual bin packing to within a factor to this end we describe and analyze a simple offline ptas for the dual bin packing problem although a ptas for a more general problem was known prior to our work kellerer chekuri and khanna our ptas is arguably simpler to state and analyze as a result we could easily adapt our ptas to obtain the advicecomplexity result we also consider whether the dependence on s is necessary in our algorithm we show that if s is unrestricted then for small enough obtaining a approximation to the dual bin packing requires n bits of advice to establish this lower bound we analyze an online reduction that preserves the advice complexity and approximation ratio from the binary separation problem due to boyar et al we define two natural advice complexity classes that capture the distinction similar to the turing machine world distinction between pseudo polynomial time algorithms and polynomial time algorithms our results on the dual bin packing problem imply the separation of the two classes in the advice complexity world less,A Simple PTAS for the Dual Bin Packing Problem and Advice Complexity of Its Online Version,"4 August, 2017"
106,Allan Borodin,result diversification is an important aspect in webbased search document summarization facility location portfolio management and other applications given a set of ranked results for a set of objects eg web documents facilities etc with a distance between any pair the goal is to select a subset s satisfying the following three criteria a the subset s satisfies some constraint eg bounded cardinality b the subset contains results of high quality and c the subset contains results that are diverse relative to the distance measure the goal of result diversification is to produce a diversified subset while maintaining high quality as much as possible we study a broad class of problems where the distances are a metric where the constraint is given by independence in a matroid where quality is determined by a monotone submodular function and diversity is defined as the sum of distances between objects in s our problem is a generalization of the em max sum diversification problem studied in citegosh which in turn is a generaliztion of the em max sum pdispersion problem studied extensively in location theory it is nphard even with the triangle inequality we propose two simple and natural algorithms a greedy algorithm for a cardinality constraint and a local search algorithm for an arbitary matroid constraint we prove that both algorithms achieve constant approximation ratios less,"Max-Sum Diversification, Monotone Submodular Functions and Dynamic Updates","24 November, 2016"
107,Allan Borodin,following the work of babaioff et al we consider the pricing game with strategic vendors and a single buyer modeling a scenario in which multiple competing vendors have very good knowledge of a buyer as is common in online markets we add to this model the realistic assumption that the buyer has a fixed budget and does not have unlimited funds when the buyers valuation function is additive we are able to completely characterize the different possible pure nash equilibria pne and in particular obtain a necessary and sufficient condition for uniqueness furthermore we characterize the market clearing or walresian equilibria for all submodular valuations surprisingly for certain monotone submodular function valuations we show that the pure ne can exhibit some counterintuitive phenomena namely there is a valuation such that the pricing will be market clearing and within budget if the buyer does not reveal the budget but will result in a smaller set of allocated items and higher prices for items if the buyer does reveal the budget it is also the case that the conditions that guarantee market clearing in babaioff et al for submodular functions are not necessarily market clearing when there is a budget furthermore with respect to social welfare while without budgets all equilibria are optimal ie poa pos we show that with budgets the worst equilibrium may only achieve n of the best equilibrium less,Budgetary Effects on Pricing Equilibrium in Online Markets,"8 February, 2016"
108,Allan Borodin,we consider auctions in which greedy algorithms paired with firstprice or criticalprice payment rules are used to resolve multiparameter combinatorial allocation problems we study the price of anarchy for social welfare in such auctions we show for a variety of equilibrium concepts including bayesnash equilibrium and correlated equilibrium the resulting price of anarchy bound is close to the approximation factor of the underlying greedy algorithm less,Price of Anarchy for Greedy Auctions,"30 November, 2014"
109,Allan Borodin,unconstrained submodular maximization captures many nphard combinatorial optimization problems including maxcut maxdicut and variants of facility location problems recently buchbinder et al presented a surprisingly simple linear time randomized greedylike online algorithm that achieves a constant approximation ratio of matching optimally the hardness result of feige et al motivated by the algorithm of buchbinder et al we introduce a precise algorithmic model called doublesided myopic algorithms we show that while the algorithm of buchbinder et al can be realized as a randomized online doublesided myopic algorithm no such deterministic algorithm even with adaptive ordering can achieve the same approximation ratio with respect to the maxdicut problem we relate the buchbinder et al algorithm and our myopic framework to the online algorithm and inapproximation of barnoy and lampis less,Bounds on Double-Sided Myopic Algorithms for Unconstrained Non-monotone Submodular Maximization,"24 April, 2014"
110,Michael Brudno,"Despite recent progress in the understanding of the genetic etiologies of rare diseases (RDs), a significant number remain intractable to diagnostic and discovery efforts. Broad data collection and sharing of information among RD researchers is therefore critical. In 2018, the Care4Rare Canada Consortium launched the project C4R‐SOLVE, a subaim of which was to collect, harmonize, and share both retrospective and prospective Canadian clinical and multiomic data. Here, we introduce Genomics4RD, an integrated web‐accessible platform to share Canadian phenotypic and multiomic data between researchers, both within Canada and internationally, for the purpose of discovering the mechanisms that cause RDs. Genomics4RD has been designed to standardize data collection and processing, and to help users systematically collect, prioritize, and visualize participant information. Data storage, authorization",Genomics4RD: An integrated platform to share Canadian deep‐phenotype and multiomic data for international rare disease gene discovery,2022/6
111,Michael Brudno,"A major challenge in validating genetic causes for patients with rare diseases (RDs) is the difficulty in identifying other RD patients with overlapping phenotypes and variants in the same candidate gene. This process, known as matchmaking, requires robust data sharing solutions to be effective. In 2014 we launched PhenomeCentral, a RD data repository capable of collecting computer‐readable genotypic and phenotypic data for the purposes of RD matchmaking. Over the past 7 years PhenomeCentral's features have been expanded and its data set has consistently grown. There are currently 1615 users registered on PhenomeCentral, which have contributed over 12,000 patient cases. Most of these cases contain detailed phenotypic terms, with a significant portion also providing genomic sequence data or other forms of clinical information. Matchmaking within PhenomeCentral, and with connections to othe",PhenomeCentral: 7 years of rare disease matchmaking,2022/6
112,Michael Brudno,"Machine learning models trained on retrospective electronic health record data were evaluated in a decision analytical model study conducted at the ED of the Hospital for Sick Children Toronto, Canada. Data were collected on all patients aged 0 to 18 years presenting to the ED from July 1, 2018, to June 30, 2019 (77 219 total patient visits).",Assessment of Machine Learning–Based Medical Directives to Expedite Care in Pediatric Emergency Medicine,2022-03-01 00:00:00
113,Michael Brudno,"High-grade diffuse glioma (HGG) is the leading cause of brain tumour death. While the genetic drivers of HGG have been well described, targeting these has thus far had little impact on survival suggesting other mechanisms are at play. Here we interrogate the alternative splicing landscape of pediatric and adult HGG through multi-omic analyses, uncovering an increased splicing burden compared with normal brain. The rate of recurrent alternative splicing in cancer drivers exceeds their mutation rate, a pattern that is recapitulated in pan-cancer analyses, and is associated with worse prognosis in HGG.",Splicing is an alternate oncogenic pathway activation mechanism in glioma,2022-01-31 00:00:00
114,Michael Brudno,"Current clinical note-taking approaches cannot capture the entirety of information available from patient encounters and detract from patient-clinician interactions. By surveying healthcare providers’ current note-taking practices and attitudes toward new clinical technologies, we developed a patient-centered paradigm for clinical note-taking that makes use of hybrid tablet/keyboard devices and artificial intelligence (AI) technologies. PhenoPad is an intelligent clinical note-taking interface that captures free-form notes and standard phenotypic information via a variety of modalities, including speech and natural language processing techniques, handwriting recognition, and more. The output is unobtrusively presented on mobile devices to clinicians for real-time validation and can be automatically transformed into digital formats that would be compatible with integration into electronic health record systems. ",PhenoPad: Building AI enabled note-taking interfaces for patient encounters,2022-01-27 00:00:00
115,Michael Brudno,"Chronic pain is a devastating problem affecting 1 in 5 individuals around the globe, with neuropathic pain the most debilitating and poorly treated type of chronic pain. Advances in transcriptomics and data mining have contributed to cataloging diverse cellular pathways and transcriptomic alterations in response to peripheral nerve injury but have focused on phenomenology and classifying transcriptomic responses. Here, with the goal of identifying new types of pain-relieving agents, we compared transcriptional reprogramming changes in the dorsal spinal cord after peripheral nerve injury cross-sex and cross-species and imputed commonalities, as well as differences in cellular pathways and gene regulation. We identified 93 transcripts in the dorsal horn that were increased by peripheral nerve injury in male and female mice and rats. Following gene ontology and transcription factor analyses, we constructed a pain interactome for the proteins encoded by the differentially expressed genes, discovering new, conserved signaling nodes. We interrogated the interactome with the Drug-Gene database to predict FDA-approved medications that may modulate key nodes within the network. ",Conserved transcriptional programming across sex and species after peripheral nerve injury predicts treatments for neuropathic pain,2022-01-01 00:00:00
116,Michael Brudno,"Local and systemic reactivity rates were high but short-lived, particularly in the younger cohort and with mRNA-1273 vaccine. After a single COVID-19 vaccine, 84% younger but only 46% older participants had positive IgG antibodies to both spike protein and receptor binding domain (RBD) antigens, increasing to 100/98% with the second dose respectively. In multivariable linear regression model, lower normalized IgG RBD antibody ratios two weeks after the second dose were statistically associated with older age, male gender, cancer diagnosis, lower body weight, BNT162b2 relative to mRNA-1273 and longer dose intervals. Antibody ratios in both cohorts declined 12 weeks post second vaccine dose.",Safety and efficacy of preventative COVID vaccines: the StopCoV study,2022-01-01 00:00:00
117,Michael Brudno,We present a U-Net based automatic segmentation pipeline to robustly segment brain structures on both neonatal and child MR images of children born preterm with enhanced robustness of data input.,Automatic U-Net based Segmentation Pipeline for Neonatal and Child Brain MRI,2022
118,Michael Brudno,"Patients enrolled as part of the Finding of Rare Disease Genes in Canada (FORGE) and Care4Rare Canada research programs had their exome sequencing data reanalyzed by a multidisciplinary research team over a 2-year period. Compelling variants in genes not previously associated with a human phenotype were submitted through the MME node PhenomeCentral, and outcomes were collected.",Outcome of over 1500 matches through the Matchmaker Exchange for rare disease gene discovery: The 2-year experience of Care4Rare Canada,2022-01-01 00:00:00
119,Michael Brudno,"Large pretrained language models (LMs) like BERT have improved performance in many disparate natural language processing (NLP) tasks. However, fine tuning such models requires a large number of training examples for each target task. Simultaneously, many realistic NLP problems are"" few shot"", without a sufficiently large training set. In this work, we propose a novel conditional neural process-based approach for few-shot text classification that learns to transfer from other diverse tasks with rich annotation. Our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classifier conditioned on the task representation. While previous task-aware few-shot learners represent tasks by input encoding, our novel task representation is more powerful, as the gradient captures input-output relationships of a task. Experimental results show that our approach outperforms traditional fine-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a collection of diverse few-shot tasks. We further conducted analysis and ablations to justify our design choices.",Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation,2021-12-06 00:00:00
120,Michael Brudno,"We present the Canadian Distributed Infrastructure for Genomics (CanDIG) platform, which enables federated querying and analysis of human genomics and linked biomedical data. CanDIG leverages the standards and frameworks of the Global Alliance for Genomics and Health (GA4GH) and currently hosts data for five pan-Canadian projects. We describe CanDIG’s key design decisions and features as a guide for other federated data systems.",CanDIG: Federated network across Canada for multi-omic and health data discovery and analysis,2021-11-10 00:00:00
121,Michael Brudno,"The Global Alliance for Genomics and Health (GA4GH) aims to accelerate biomedical advances by enabling the responsible sharing of clinical and genomic data through both harmonized data aggregation and federated approaches. The decreasing cost of genomic sequencing (along with other genome-wide molecular assays) and increasing evidence of its clinical utility will soon drive the generation of sequence data from tens of millions of humans, with increasing levels of diversity. In this perspective, we present the GA4GH strategies for addressing the major challenges of this data revolution. We describe the GA4GH organization, which is fueled by the development efforts of eight Work Streams and informed by the needs of 24 Driver Projects and other key stakeholders. We present the GA4GH suite of secure, interoperable technical standards and policy frameworks and review the current status of standards ",GA4GH: International policies and standards for data sharing across genomic research and healthcare,2021-11-10 00:00:00
122,Michael Brudno,"We promote a shared vision and guide for how and when to federate genomic and health-related data sharing, enabling connections and insights across independent, secure databases. The GA4GH encourages a federated approach wherein data providers have the mandate and resources to share, but where data cannot move for legal or technical reasons. We recommend a federated approach to connect national genomics initiatives into a global network and precision medicine resource.",International federation of genomic medicine databases using GA4GH standards,2021-11-10 00:00:00
123,Michael Brudno,"The creation of Voxe will occur over two phases that build on previous research. The user interface design phase employs a ‘user-centric’ approach to identify end-users’ needs and iteratively refine the look and layout of Voxe to meet these needs. Transplant recipients, aged 10–17, and healthcare providers will participate in three rounds of testing (24 participants total). Participants will: (1) complete",Creation of an electronic patient-reported outcome measure platform Voxe: a mixed methods study protocol in paediatric solid organ transplantation,2021-10-01 00:00:00
124,Michael Brudno,"MetaFusion is a flexible metacalling tool that amalgamates outputs from any number of fusion callers. Individual caller results are standardized by conversion into the new file type Common Fusion Format. Calls are annotated, merged using graph clustering, filtered and ranked to provide a final output of high-confidence candidates. MetaFusion consistently achieves higher precision and recall than individual callers on real and simulated datasets, and reaches up to 100% precision, indicating that ensemble calling is imperative for high-confidence results. MetaFusion uses FusionAnnotator to",MetaFusion: a high-confidence metacaller for filtering and prioritizing RNA-seq gene fusion candidates,2021-10-01 00:00:00
125,Michael Brudno,"Malformations of cortical development represent an important cause of developmental disability and neurologic morbidity and mortality.1 Advances in genetic methodology, particularly the widespread implementation of next-generation DNA sequencing technology (e.g., multigene panels and whole exome sequencing [WES]), have significantly improved diagnostic yield in neurogenetic disease.2 The current yield for a range of conditions, including brain malformations, epilepsy,3 global developmental delay, and movement disorders, is approximately 50%",Child neurology: RNA sequencing for the diagnosis of lissencephaly,2021-09-21 00:00:00
126,Michael Brudno,large pretrained language models lms like bert have improved performance in many disparate natural language processing nlp tasks however fine tuning such models requires a large number of training examples for each target task simultaneously many realistic nlp problems are few shot without a sufficiently large training set in this work we propose a novel conditional neural processbased approach for fewshot text classification that learns to transfer from other diverse tasks with rich annotation our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classifier conditioned on the task representation while previous taskaware fewshot learners represent tasks by input encoding our novel task representation is more powerful as the gradient captures inputoutput relationships of a task experimental results show that our approach outperforms traditional finetuning sequential transfer learning and stateoftheart meta learning approaches on a collection of diverse fewshot tasks we further conducted analysis and ablations to justify our design choices less,Grad2Task: Improved Few-shot Text Classification Using Gradients for Task Representation,"27 January, 2022"
127,Marsha Chechik,machine vision components mvc are becoming safetycritical assuring their quality including safety is essential for their successful deployment assurance relies on the availability of precisely specified and ideally machineverifiable requirements mvcs with stateoftheart performance rely on machine learning ml and training data but largely lack such requirements in this paper we address the need for defining machineverifiable reliability requirements for mvcs against transformations that simulate the full range of realistic and safetycritical changes in the environment using human performance as a baseline we define reliability requirements as if the changes in an image do not affect a humans decision neither should they affect the mvcs to this end we provide a class of safetyrelated image transformations reliability requirement classes to specify correctnesspreservation and predictionpreservation for mvcs a method to instantiate machineverifiable requirements from these requirements classes using human performance experiment data human performance experiment data for image recognition involving eight commonly used transformations from about human participants and a method for automatically checking whether an mvc satisfies our requirements further we show that our reliability requirements are feasible and reusable by evaluating our methods on stateoftheart pretrained image classification models finally we demonstrate that our approach detects reliability gaps in mvcs that other existing methods are unable to detect less,"If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components","8 February, 2022"
128,Marsha Chechik,safetycritical software systems are in many cases designed and implemented as families of products usually referred to as software product lines spls products within an spl vary from each other in terms of which features they include applying existing analysis techniques to spls and their safety cases is usually challenging because of the potentially exponential number of products with respect to the number of supported features in this paper we present a methodology and infrastructure for certified emphlifting of existing singleproduct safety analyses to product lines to ensure certified safety of our infrastructure we implement it in an interactive theorem prover including formal definitions lemmas correctness criteria theorems and proofs we apply this infrastructure to formalize and lift a change impact assessment cia algorithm we present a formal definition of the lifted algorithm outline its correctness proof with the full machinechecked proof available online and discuss its implementation within a model management framework less,Towards Certified Analysis of Software Product Line Safety Cases,"30 April, 2021"
129,Marsha Chechik,variabilityaware computing is the efficient application of programs to different sets of inputs that exhibit some variability one example is program analyses applied to software product lines spls in this paper we present the design and development of a variabilityaware version of the souffl datalog engine the engine can take facts annotated with presence conditions pcs as input and compute the pcs of its inferred facts eliminating facts that do not exist in any valid configuration we evaluate our variabilityaware souffl implementation on several fact sets annotated with pcs to measure the associated overhead in terms of processing time and database size less,Variability-aware Datalog,"9 December, 2019"
130,Marsha Chechik,there is a growing interest in techniques for detecting whether a logic specification is satisfied too easily or vacuously for example the specification every request is eventually followed by an acknowledgment is satisfied vacuously by a system that never generates any requests vacuous satisfaction misleads users of modelchecking into thinking that a system is correct there are several existing definitions of vacuity originally beer et al formalized vacuity as insensitivity to syntactic perturbation however this definition is only reasonable for vacuity in a single occurrence armoni et al argued that vacuity must be robust not affected by semantically invariant changes such as extending a model with additional atomic propositions they show that syntactic vacuity is not robust for ltl and propose an alternative definition trace vacuity in this article we continue this line of research we show that trace vacuity is not robust for branching time logic we refine it to apply uniformly to linear and branching time logic and to not suffer from the common pitfalls of prior definitions our new definition bisimulation vacuity is a proper nontrivial extension of both syntactic and trace vacuity we discuss the complexity of detecting bisimulation vacuity and give efficient algorithms to detect vacuity for several practicallyrelevant subsets of ctl less,Robust Vacuity for Branching Temporal Logic,"13 October, 2010"
131,Marsha Chechik,a patternbased approach to the presentation codification and reuse of property specifications for finitestate verification was proposed by dwyer and his collegues the patterns enable nonexperts to read and write formal specifications for realistic systems and facilitate easy conversion of specifications between formalisms such as ltl ctl qre in this paper we extend the pattern system with events changes of values of variables in the context of ltl less,Events in Property Patterns,"29 June, 1999"
132,Marsha Chechik,for over a decade researchers in formal methods tried to create formalisms that permit natural specification of systems and allow mathematical reasoning about their correctness the availability of fullyautomated reasoning tools enables more nonspecialists to use formal methods effectively their responsibility reduces to just specifying the model and expressing the desired properties thus it is essential that these properties be represented in a language that is easy to use and sufficiently expressive lineartime temporal logic is a formalism that has been extensively used by researchers for specifying properties of systems when such properties are closed under stuttering ie their interpretation is not modified by transitions that leave the system in the same state verification tools can utilize a partialorder reduction technique to reduce the size of the model and thus analyze larger systems if ltl formulas do not contain the next operator the formulas are closed under stuttering but the resulting language is not expressive enough to capture many important properties eg properties involving events determining if an arbitrary ltl formula is closed under stuttering is hard it has been proven to be pspacecomplete in this paper we relax the restriction on ltl that guarantees closure under stuttering introduce the notion of edges in the context of ltl and provide theorems that enable syntactic reasoning about closure under stuttering of ltl formulas less,Events in Linear-Time Properties,"28 June, 1999"
133,Marsha Chechik,"Applying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to ""lift"" particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffle Datalog engine. We evaluate our implementation on a set of Java and C-language benchmark annotative software product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually.",Annotative Software Product Line Analysis Using Variability-aware Datalog,2022-05-19 00:00:00
134,Marsha Chechik,"Machine Vision Components (MVC) are becoming safety-critical. Assuring their quality, including safety, is essential for their successful deployment. Assurance relies on the availability of precisely specified and, ideally, machine-verifiable requirements. MVCs with state-of-the-art performance rely on machine learning (ML) and training data but largely lack such requirements. In this paper, we address the need for defining machine-verifiable reliability requirements for MVCs against transformations that simulate the full range of realistic and safety-critical changes in the environment. Using human performance as a baseline, we define reliability requirements as: 'if the changes in an image do not affect a human's decision, neither should they affect the MVC's.' To this end, we provide: (1) a class of safety-related image transformations; (2) reliability requirement classes to specify correctness-preservation and prediction-preservation for MVCs; (3) a method to instantiate machine-verifiable requirements from these requirements classes using human performance experiment data; (4) human performance experiment data for image recognition involving eight commonly used transformations, from about 2000 human participants; and (5) a method for automatically checking whether an MVC satisfies our requirements. Further, we show that our reliability requirements are feasible and reusable by evaluating our methods on 13 state-of-the-art pre-trained image classification models. Finally, we demonstrate that our approach detects reliability gaps in MVCs that other existing methods are unable to detect.","If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components",2022-02-08 00:00:00
135,Marsha Chechik,"Abstract Software Product Lines (SPLs) are families of related software products developed from a common set of artifacts. Most existing analysis tools cannot be applied to an entire SPL, but rather must be applied an SPL’s products one at a time. Some tools have been redesigned or re-implemented to support the kind of variability exhibited in SPLs, but this usually takes a lot of effort and is error-prone. Declarative analyses written in languages like Datalog have been collectively lifted to SPLs in prior work, which enables the application of existing declarative analyses to SPLs. In this paper, we apply five declarative analyses (behaviour alteration, recusion analysis, simplifiable global variable analysis, and two of their variants)",Applying Declarative Analysis to Industrial Automotive Software Product Line Models,2022-01-27 00:00:00
136,Marsha Chechik,"Assurance cases are a means to argue about the safety, security, etc. of software systems in critical domains. In previous work, we presented a tool called MMINT-A to automate change impact assessment of assurance cases given system design changes. In this paper, we argue that applying model-driven techniques to assurance case development allows safety engineers and assessors to ask questions about these artifacts and answer them using automated tool support–something not achievable with traditional document-based approaches. To support this argument, we present a library of well-formedness constraints on assurance cases structured in the Goal Structuring Notation (GSN). The constraints are formalized using OCL and implemented in MMINT-A. We also discuss other types of constraint checks that are useful in the automotive domain given the ISO 26262 standard and internal company processes.",Assurance Case Property Checking with MMINT-A and OCL,2022
137,Niv Dayan,many modern applications produce massive streams of data series that need to be analyzed requiring efficient similarity search operations however the stateoftheart data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance or storage costs we pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order to address this problem we present coconut the first data series index based on sortable summarizations and the first efficient solution for indexing and querying streaming series the first innovation in coconut is an inverted sortable data series summarization that organizes data series based on a zorder curve keeping similar series close to each other in the sorted order as a result coconut is able to use bulk loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk ios we then explore prefixbased and medianbased splitting policies for bottomup bulk loading showing that medianbased splitting outperforms the state of the art ensuring that all nodes are densely populated finally we explore the impact of sortable summarizations on variablesized window queries showing that they can be supported in the presence of updates through efficient merging of temporal partitions overall we show analytically and empirically that coconut dominates the stateoftheart data series indexes in terms of construction speed query speed and storage costs less,Coconut: sortable summarizations for scalable indexes over static and streaming data series,"16 April, 2021"
138,Niv Dayan,many modern applications produce massive amounts of data series that need to be analyzed requiring efficient similarity search operations however the stateoftheart data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance or storage costs we pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order this leads to two design problems first traditional bulkloading algorithms based on sorting cannot be used instead index construction takes place through slow topdown insertions which create a noncontiguous index that results in many random ios second data series cannot be sorted and split across nodes evenly based on their median value thus most leaf nodes are in practice nearly empty this further slows down query speed and amplifies storage costs to address these problems we present coconut the first innovation in coconut is an inverted sortable data series summarization that organizes data series based on a zorder curve keeping similar series close to each other in the sorted order as a result coconut is able to use bulkloading techniques that rely on sorting to quickly build a contiguous index using large sequential disk ios we then explore prefixbased and medianbased splitting policies for bottomup bulkloading showing that medianbased splitting outperforms the state of the art ensuring that all nodes are densely populated overall we show analytically and empirically that coconut dominates the stateoftheart data series indexes in terms of construction speed query speed and storage costs less,Coconut: a scalable bottom-up approach for building data series indexes,"19 June, 2020"
139,Niv Dayan,deterministic quantum interactions between single photons and single quantum emitters are a vital building block towards the distribution of quantum information between remote systems deterministic photonatom state transfer has been demonstrated by using protocols that include active feedback or synchronized control pulses here we demonstrate a completely passive swap gate between the states of a single photon and a single atom the underlying mechanism is singlephoton raman interaction sprint an interferencebased effect in which a photonic qubit deterministically controls the state of a material qubit encoded in the two ground states of a system and vice versa using a nanofibercoupled microsphere resonator coupled to single rb atoms we swap a photonic qubit into the atom and back demonstrating nonclassical fidelities in both directions requiring no control fields or feedback protocol the gate takes place automatically at the timescale of the atoms cavity enhanced spontaneous emission time applicable to any waveguidecoupled system this scheme provides a versatile building block for the modular scaling up of quantum information processing systems less,Demonstration of a passive photon-atom swap gate,"29 November, 2017"
140,Niv Dayan,how stable is the performance of your flashbased solid state drives ssds this question is central for database designers and administrators cloud service providers and ssd constructors the answer depends on writeamplification ie garbage collection overhead more specifically the answer depends on how writeamplification evolves in time how then can one model and manage writeamplification especially when application workloads change this is the focus of this paper managing writeamplification boils down to managing the surplus physical space called overprovisioned space modern ssds essentially separate the physical space into several partitions based on the update frequency of the pages they contain and divide the overprovisioned space among the groups so as to minimize writeamplification we introduce wolf a block manager that allocates overprovisioned space to ssd partitions using a nearoptimal closedform expression based on the sizes and update frequencies of groups of pages our evaluation shows that wolf is robust to workloads change with an improvement factor of with respect to the stateoftheart we also show that wolf performs comparably and even slightly better than the state of the art with stable workloads over improvement with a tpcc workload less,Modelling and Managing SSD Write-amplification,"1 April, 2015"
141,Niv Dayan,"A method for managing a log structured merged (LSM) tree of key value (KV) pairs, the LSM tree is stored in a non-volatile memory, the method may include merging runs of the LSM tree to provide merged runs; writing merged runs to the non-volatile memory; adding new runs to the LSM tree, wherein the adding comprises writing runs to the non-volatile memory; and updating at least one management data structure (MDS) to reflect the merging and the adding; wherein an MDS of the at least one MDS stores a mapping between keys of the KV pairs of the LSM tree, fingerprints associated with the KV pairs of the LSM tree, and compressed run identifiers that identify runs of the LSM tree",Managing a lsm tree of key value pairs that is stored in a non-volatile memory,2022-03-10 00:00:00
142,Niv Dayan,"Embodiments of the invention utilize an improved LSM-tree-based key-value approach to strike the optimal balance between the costs of updates and lookups and storage space. The improved approach involves use of a new merge policy that removes merge operations from all but the largest levels of LSM-tree. In addition, the improved approach may include an improved LSM-tree that allows separate control over the frequency of merge operations for the largest level and for all other levels. By adjusting various parameters, such as the storage capacity of the largest level, the storage capacity of the other smaller levels, and/or the size ratio between adjacent levels in the improved LSM-tree, the improved LSM-tree-based key-value approach may maximize throughput for a particular workload.",Key-value stores with optimized merge policies and optimized lsm-tree structures,2021-11-04 00:00:00
143,Niv Dayan,"Embodiments of the invention utilize a “data canopy” that breaks statistical measures down to basic primitives for various data portions and stores the basic aggregates in a library within an in-memory data structure. When a queried statistical measure involves a basic aggregate stored in the library over a data portion that at least partially overlaps the data portion associated with the basic aggregate, the basic aggregate may be reused in the statistical computation of the queried measure.",Systems and methods for accelerating exploratory statistical analysis,2021-09-09 00:00:00
144,Niv Dayan,"With the end of Moore's Law, database architects are turning to hardware accelerators to offload computationally intensive tasks from the CPU. In this paper, we show that accelerators can facilitate far more than just computation: they enable algorithms and data structures that lavishly expand computation in order to optimize for disparate cost metrics. We introduce the Pliops Extreme Data Processor (XDP), a novel storage engine implemented from the ground up using customized hardware. At its core, XDP consists of an accelerated hash table to index the data in storage using less memory and fewer storage accesses for queries than the best alternative. XDP also employs an accelerated compressor, a capacitor, and a lock-free RAID sub-system to minimize storage space and recovery time while minimizing performance penalties. As a result, XDP overcomes cost contentions that have so far been inescapable.",The end of Moore's law and the rise of the data processor,2021-07-01 00:00:00
145,Niv Dayan,"Modern key-value stores typically rely on an LSM-tree in storage (SSD) to handle writes and Bloom filters in memory (DRAM) to optimize reads. With ongoing advances in SSD technology shrinking the performance gap between storage and memory devices, the Bloom filters are now emerging as a performance bottleneck.",Chucky: A Succinct Cuckoo Filter for LSM-Tree,2021-06-09 00:00:00
146,Niv Dayan,"Embodiments of the present invention provide a new multi-level data structure, log-structured merge bush (LSM-bush), to alleviate the performance compromise between LSH-table and LSM-tree data structures. Similar to LSM-tree, LSM-bush may buffer writes in memory, merge the writes as sorted runs across multiple levels in storage, and use in-memory fence pointers and Bloom filters to facilitate lookups. LSM-bush differs from LSM-tree in that it allows newer data to be merged more “lazily” than LSM-tree. This can be achieved by allowing larger numbers of runs to be collected at the smaller levels before merging them.",File management with log-structured merge bush,2020-08-06 00:00:00
147,Niv Dayan,"Many modern applications produce massive amounts of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. This leads to two design problems. First, traditional bulk-loading algorithms based on sorting cannot be used. Instead, index construction takes place through slow top-down insertions, which create a non-contiguous index that results in many random I/Os. Second, data series cannot be sorted and split across nodes evenly based on their median value; thus, most leaf nodes are in practice nearly empty. This further slows down query speed and amplifies storage costs. To address these problems, we present Coconut. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading techniques that rely on sorting to quickly build a contiguous index using large sequential disk I/Os. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series ",Coconut: A scalable bottom-up approach for building data series indexes,2020-06-20 00:00:00
148,Eyal de Lara,fog computing envisions that deploying services of an application across resources in the cloud and those located at the edge of the network may improve the overall performance of the application when compared to running the application on the cloud however there are currently no benchmarks that can directly compare the performance of the application across the cloudonly edgeonly and cloudedge deployment platform to obtain any insight on performance improvement this paper proposes defog a first fog benchmarking suite to i alleviate the burden of fog benchmarking by using a standard methodology and ii facilitate the understanding of the target platform by collecting a catalogue of relevant metrics for a set of benchmarks the current portfolio of defog benchmarks comprises six relevant applications conducive to using the edge experimental studies are carried out on multiple target platforms to demonstrate the use of defog for collecting metrics related to application latencies communication and computation for understanding the impact of stress and concurrent users on application latencies and for understanding the performance of deploying different combination of services of an application across the cloud and edge defog is available for public download httpsgithubcomqubblessondefog less,DeFog: Fog Computing Benchmarks,"25 July, 2019"
149,Eyal de Lara,"Methods and systems for data management are described, particularly for processing global queries. Each global query includes a user-defined query constraint value, such as laxity or query response time limit. The query receiving node maintains a copy of the previously updated data from all of its children node. The query receiving node first searches for the requested query data in its local data storage to minimize children node query. If any portion of the requested data in the local data storage fails to meet the query constraint value, then the child node from which the data came from is tasked with recursively executing the global query.",Method to improve global query performance in an edge network,2022-06-09 00:00:00
150,Eyal de Lara,"The present disclosure provides systems, methods, and computer-readable media for managing graphics processing unit (GPU) allocation for a virtual machine (VM). A first GPU driver, associated with a first GPU, is offloaded from an operating system (OS) of the VM. Then, the first GPU is deallocated from the VM. A second GPU is allocated to the VM, and a second GPU driver, associated with the second GPU, is loaded in the OS of the VM. To restore a GPU context from the first GPU within the second GPU, a GPU command log from the first GPU is replayed to the second GPU.",Allocation of graphics processing units for virtual machines,2022-05-03 00:00:00
151,Eyal de Lara,"The COVID-19 (SARS-CoV-2) pandemic has placed the world in a state of emergency for the better part of two years. COVID-19 can range from being asymptomatic to causing potentially fatal complications, such as acute respiratory distress syndrome. Acute COVID-19 infections typically last for approximately 2 weeks and common symptoms include cough, fatigue, and shortness of breath. A warning sign of potential deterioration from COVID-19 used by healthcare practitioners is an objective decrease in oxygen saturation. In this work, we explore the prediction of low oxygen saturation in ambulatory patients with COVID-19.",Predicting Low Oxygen Saturation of COVID-19 Patients Using a Random Forest Classifier,2022/5
152,Eyal de Lara,"The coronavirus disease (COVID-19) pandemic has impacted hundreds of millions of people worldwide. Most patients with COVID-19 will have mild-moderate illness and can be managed at home, but a minority are hospitalized due to declining oxygen levels. Since objective hypoxia does not always result in subjective dyspnea in patients with COVID-19, some health care providers have used oxygen saturation monitors to monitor for worsening symptoms. Knowledge of oxygenation levels over the course of acute COVID-19 infection may help health care providers and patients know what to expect of COVID-19 infection and when they should seek further medical attention.",Trends in Oxygen Level During Acute COVID-19 Infection in Patients Quarantining at Home,2022/5
153,Eyal de Lara,"DNN inference is time-consuming and resource hungry. Partitioning and early exit are ways to run DNNs efficiently on the edge. Partitioning balances the computation load on multiple servers, and early exit offers to quit the inference process sooner and save time. Usually, these two are considered separate steps with limited flexibility. This work combines partitioning and early exit and proposes a performance model to estimate both inference latency and accuracy. We use this performance model to offer the best partitioned/early exit DNN based on deployment information and user preferences. Our experiments show that the flexibility in number and position of partitioning points and placement on available devices plays an important role in deciding the best output. In the future, we plan to turn this work into a"" one-click"" system to train and optimize models for edge computing.",Combining DNN partitioning and early exit,2022-04-05 00:00:00
154,Eyal de Lara,"Pervasive sensing using wearables for health monitoring presents a promising and unique opportunity to widely manage illnesses and conditions. To better understand the capabilities and limitations of using wearable devices for health monitoring, systems need to be developed and studies conducted. We conducted one such study for monitoring patients with Chronic Obstructive Pulmonary Disease (COPD), in which we aim to understand the disease and predict patient outcomes. However, despite a carefully well-planned and well-conducted study that resulted in a very large dataset, some non-obvious design oversights meant the data was much less useful. We analyze the shortcomings of our study to construct lessons and concrete actions to avoid these pitfalls. We ratify these lessons by briefly discussing a second iteration of our study, in which we apply these lessons and obtain much better outcomes",Hindsight is 20/20: Retrospective lessons for conducting longitudinal wearable sensing studies,2022-03-21 00:00:00
155,Eyal de Lara,"Acoustic speech characteristics have previously been identified as possible indicators of respiratory disease when recorded in controlled lab settings. However, the ability to measure and leverage these indicators during people’s everyday lives has been largely under-explored. In this study, we use continuous audio data from smartwatches worn by individuals suffering from COPD, as well as symptom information through daily self-reports. By applying pre-trained models for voice activity detection and speaker verification models, we are able to isolate moments of the user’s own speech and extract important speech features. We then use those features in an isolation forest outlier detector to discriminate between days with normal and worsening symptoms, achieving an AUC of nearly 0.60 on this challenging problem.",Unobtrusive monitoring of COPD patients using speech collected from smartwatches in the wild,2022-03-21 00:00:00
156,Eyal de Lara,"This article argues that low latency, high bandwidth, device proliferation, sustainable digital infrastructure, and data privacy and sovereignty continue to motivate the need for edge computing research even though its initial concepts were formulated more than a decade ago.",Revisiting the arguments for edge computing research,2021-07-01 00:00:00
157,Eyal de Lara,"Chronic pain is often an ongoing challenge for patients to track and collect data. Pain-O-Vision is a smartwatch enabled pain management system that uses computer vision to capture the details of painful events from the user. A natural reaction to pain is to clench ones fist. The embedded camera is used to capture different types of fist clenching, to represent different levels of pain. An initial prototype was built on an Android smartwatch that uses a cloud-based classification service to detect the fist clench gestures. Our results show that it is possible to map a fist clench to different levels of pain which allows the patient to record the intensity of a painful event without carrying a specialized pain management device.","Pain-o-vision, effortless pain management",2021-06-24 00:00:00
158,Eyal de Lara,"Continuous monitoring of cough may provide insights into the health of individuals as well as the effectiveness of treatments. Smart-watches, in particular, are highly promising for such monitoring: they are inexpensive, unobtrusive, programmable, and have a variety of sensors. However, current mobile cough detection systems are not designed for smartwatches, and perform poorly when applied to real-world smartwatch data since they are often evaluated on data collected in the lab.In this work we propose CoughWatch, a lightweight cough detector for smartwatches that uses audio and movement data for in-the-wild cough detection. On our in-the-wild data, CoughWatch achieves a precision of 82% and recall of 55%, compared to 6% precision and 19% recall achieved by the current state-of-the-art approach. Furthermore, by incorporating gyroscope and accelerometer data,",Coughwatch: Real-world cough detection using smartwatches,2021-06-06 00:00:00
159,Eyal de Lara,"Edge computing is the next Internet frontier that will leverage computing resources located near users, sensors, and data stores to provide more responsive services. Therefore, it is envisioned that a large-scale, geographically dispersed, and resource-rich distributed system will emerge and play a key role in the future Internet. However, given the loosely coupled nature of such complex systems, their operational conditions are expected to change significantly over time. In this context, the performance characteristics of such systems will need to be captured rapidly, which is referred to as performance benchmarking, for application deployment, resource orchestration, and adaptive decision-making. Edge performance benchmarking is a nascent research avenue that has started gaining momentum over the past five years. This article first reviews articles published over the past three decades to trace the history of ",A survey on edge performance benchmarking,2021-04-22 00:00:00
160,Eyal de Lara,"Smartwatches can collect heart rate data unobtrusively and continuously, making them a promising tool for conducting long term studies, monitoring chronic conditions, and providing timely intervention. Healthcare applications, however, require us to understand the reliability of collected readings, both in terms of quality and quantity. The accuracy of optical heart rate (HR) measurements has been studied extensively in recent years, identifying several common causes of errors. For example, previous research has demonstrated that inaccurate HR readings occur more frequently in dark skin as compared to light skin due to melanin absorption. Smartwatches therefore implement a confidence mechanism to estimate reliability of HR readings. We study the effect of skin tone on the reliability of confidence estimation of seven consumer-grade WearOS smartwatches. We find that some watches systematically ","Skin tone, confidence, and data quality of heart rate sensing in WearOS smartwatches",2021-03-22 00:00:00
161,Eyal de Lara,"Chronic obstructive pulmonary disease (COPD) is one of the leading causes of human mortality worldwide. Traditionally, estimating COPD severity has been done in controlled clinical conditions using cough sounds, respiration, and heart rate variability, with the latter reporting insights on the autonomic dysfunction caused by the disease. Advancements in remote monitoring and wearable device technologies, in turn, have allowed for remote COPD monitoring in daily life conditions. In this study, we explore the potential for predicting COPD severity and exacerbation using a low-cost wearable device that measures heart rate and activity data. We collected smartwatch sensor data from 35 COPD patients over a period of three months. Our evaluation shows that future trajectory of the disease can be predicted using only the first few days of continuous unobtrusive wearable data collected from COPD patients. ",Remote copd severity and exacerbation detection using heart rate and activity data measured from a wearable device,2021-03-22 00:00:00
162,Sven Dickinson,differentiable rendering is an essential operation in modern vision allowing inverse graphics approaches to d understanding to be utilized in modern machine learning frameworks explicit shape representations voxels point clouds or meshes while relatively easily rendered often suffer from limited geometric fidelity or topological constraints on the other hand implicit representations occupancy distance or radiance fields preserve greater fidelity but suffer from complex or inefficient rendering processes limiting scalability in this work we endeavour to address both shortcomings with a novel shape representation that allows fast differentiable rendering within an implicit architecture building on implicit distance representations we define directed distance fields ddfs which map an oriented point position and direction to surface visibility and depth such a field can render a depth map with a single forward pass per pixel enable differential surface geometry extraction eg surface normals and curvatures via network derivatives be easily composed and permit extraction of classical unsigned distance fields using probabilistic ddfs pddfs we show how to model inherent discontinuities in the underlying field finally we apply our method to fitting single shapes unpaired daware generative image modelling and singleimage d reconstruction tasks showcasing strong performance with simple architectural components via the versatility of our representation less,Representing 3D Shapes with Probabilistic Directed Distance Fields,"9 December, 2021"
163,Sven Dickinson,a complete representation of d objects requires characterizing the space of deformations in an interpretable manner from articulations of a single instance to changes in shape across categories in this work we improve on a prior generative model of geometric disentanglement for d shapes wherein the space of object geometry is factorized into rigid orientation nonrigid pose and intrinsic shape the resulting model can be trained from raw d shapes without correspondences labels or even rigid alignment using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space the geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object furthermore it enables tasks like pose transfer and poseaware retrieval without requiring supervision we evaluate our model on its generative modelling representation learning and disentanglement performance showing improved rotation invariance and intrinsicextrinsic factorization quality over the prior model less,Disentangling Geometric Deformation Spaces in Generative Latent Shape Models,"27 February, 2021"
164,Sven Dickinson,representing d shape is a fundamental problem in artificial intelligence which has numerous applications within computer vision and graphics one avenue that has recently begun to be explored is the use of latent representations of generative models however it remains an open problem to learn a generative model of shape that is interpretable and easily manipulated particularly in the absence of supervised labels in this paper we propose an unsupervised approach to partitioning the latent space of a variational autoencoder for d point clouds in a natural way using only geometric information our method makes use of tools from spectral differential geometry to separate intrinsic and extrinsic shape information and then considers several hierarchical disentanglement penalties for dividing the latent space in this manner including a novel one that penalizes the jacobian of the latent representation of the decoded output with respect to the latent encoding we show that the resulting representation exhibits intuitive and interpretable behavior enabling tasks such as pose transfer and poseaware shape retrieval that cannot easily be performed by models with an entangled representation less,Geometric Disentanglement for Generative Latent Shape Models,"18 August, 2019"
165,Sven Dickinson,the computer vision community has witnessed recent advances in scene categorization from images with the stateofthe art systems now achieving impressive recognition rates on challenging benchmarks such as the places dataset such systems have been trained on photographs which include color texture and shading cues the geometry of shapes and surfaces as conveyed by scene contours is not explicitly considered for this task remarkably humans can accurately recognize natural scenes from line drawings which consist solely of contourbased shape cues here we report the first computer vision study on scene categorization of line drawings derived from popular databases including an artist scene database mit and places specifically we use offtheshelf pretrained cnns to perform scene classification given only contour information as input and find performance levels well above chance we also show that medialaxis based contour salience methods can be used to select more informative subsets of contour pixels and that the variation in cnn classification performance on various choices for these subsets is qualitatively similar to that observed in human performance moreover when the salience measures are used to weight the contours as opposed to pruning them we find that these weights boost our cnn performance above that for unweighted contour input that is the medial axis based salience weights appear to add useful information that is not available when cnns are trained to use contours alone less,Scene Categorization from Contours: Medial Axis Based Salience Measures,"26 November, 2018"
166,Sven Dickinson,the role of symmetry in computer vision has waxed and waned in importance during the evolution of the field from its earliest days at first figuring prominently in support of bottomup indexing it fell out of favor as shape gave way to appearance and recognition gave way to detection with a strong prior in the form of a target object the role of the weaker priors offered by perceptual grouping was greatly diminished however as the field returns to the problem of recognition from a large database the bottomup recovery of the parts that make up the objects in a cluttered scene is critical for their recognition the medial axis community has long exploited the ubiquitous regularity of symmetry as a basis for the decomposition of a closed contour into medial parts however todays recognition systems are faced with cluttered scenes and the assumption that a closed contour exists ie that figureground segmentation has been solved renders much of the medial axis communitys work inapplicable in this article we review a computational framework previously reported in lee et al levinshtein et al that bridges the representation power of the medial axis and the need to recover and group an objects parts in a cluttered scene our framework is rooted in the idea that a maximally inscribed disc the building block of a medial axis can be modeled as a compact superpixel in the image we evaluate the method on images of cluttered scenes less,A Framework for Symmetric Part Detection in Cluttered Scenes,"5 February, 2015"
167,Sven Dickinson,we present an approach to labeling short video clips with english verbs as event descriptions a key distinguishing aspect of this work is that it labels videos with verbs that describe the spatiotemporal interaction between event participants humans and objects interacting with each other abstracting away all objectclass information and finegrained image characteristics and relying solely on the coarsegrained motion of the event participants we apply our approach to a large set of distinct verb classes and a corpus of videos yielding two surprising outcomes first a classification accuracy of greater than on a outof labeling task and greater than on a variety of outof subsets of this labeling task is independent of the choice of which of two different timeseries classifiers we employ second we achieve this level of accuracy using a highly impoverished intermediate representation consisting solely of the bounding boxes of one or two event participants as a function of time this indicates that successful event recognition depends more on the choice of appropriate features that characterize the linguistic invariants of the event classes than on the particular classifier algorithms less,Large-Scale Automatic Labeling of Video Events with Verbs Based on Event-Participant Interaction,"16 April, 2012"
168,David Duvenaud,energybased models ebms present a flexible and appealing way to represent uncertainty despite recent advances training ebms on highdimensional data remains a challenging problem as the stateoftheart approaches are costly unstable and require considerable tuning and domain expertise to apply successfully in this work we present a simple method for training ebms at scale which uses an entropyregularized generator to amortize the mcmc sampling typically used in ebm training we improve upon prior mcmcbased entropy regularization methods with a fast variational approximation we demonstrate the effectiveness of our approach by using it to train tractable likelihood models next we apply our estimator to the recently proposed joint energy model jem where we match the original performance with faster and stable training this allows us to extend jem models to semisupervised classification on tabular data from a variety of continuous domains less,No MCMC for me: Amortized sampling for fast and stable training of energy-based models,"6 June, 2021"
169,David Duvenaud,we generalize gradient descent with momentum for optimization in differentiable games to have complexvalued momentum we give theoretical motivation for our method by proving convergence on bilinear zerosum games for simultaneous and alternating updates our method gives realvalued parameter updates making it a dropin replacement for standard optimizers we empirically demonstrate that complexvalued momentum can improve convergence in realistic adversarial games like generative adversarial networks by showing we can find better solutions with an almost identical computational cost we also show a practical generalization to a complexvalued adam variant which we use to train biggan to better inception scores on cifar less,Complex Momentum for Optimization in Games,"1 June, 2021"
170,David Duvenaud,effective training of deep neural networks can be challenging and there remain many open questions on how to best learn these models recently developed methods to improve neural network training examine teaching providing learned information during the training process to improve downstream model performance in this paper we take steps towards extending the scope of teaching we propose a flexible teaching framework using commentaries learned metainformation helpful for training on a particular task we present gradientbased methods to learn commentaries leveraging recent work on implicit differentiation for scalability we explore diverse applications of commentaries from weighting training examples to parameterising labeldependent data augmentation policies to representing attention masks that highlight salient image regions we find that commentaries can improve training speed andor performance and provide insights about the dataset and training process we also observe that commentaries generalise they can be reused when training new models to obtain performance benefits suggesting a usecase where commentaries are stored with a dataset and leveraged in future for improved model training less,Teaching with Commentaries,"11 March, 2021"
171,David Duvenaud,explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature we propose fit a framework that evaluates the importance of observations for a multivariate timeseries blackbox model by quantifying the shift in the predictive distribution over time fit defines the importance of an observation based on its contribution to the distributional shift under a kldivergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved we also demonstrate the need to control for timedependent distribution shifts we compare with stateoftheart baselines on simulated and realworld clinical data and demonstrate that our approach is superior in identifying important time points and observations throughout the time series less,What went wrong and when? Instance-wise Feature Importance for Time-series Models,"28 October, 2020"
172,David Duvenaud,the adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations we generalize this method to stochastic differential equations allowing timeefficient and constantmemory computation of gradients with highorder adaptive solvers specifically we derive a stochastic differential equation whose solution is the gradient a memoryefficient algorithm for caching noise and conditions under which numerical solutions converge in addition we combine our method with gradientbased stochastic variational inference for latent stochastic differential equations we use our method to fit stochastic dynamics defined by neural networks achieving competitive performance on a dimensional motion capture dataset less,Scalable Gradients for Stochastic Differential Equations,"18 October, 2020"
173,David Duvenaud,we present a new method for evaluating and training unnormalized density models our approach only requires access to the gradient of the unnormalized models logdensity we estimate the stein discrepancy between the data density px and the model density qx defined by a vector function of the data we parameterize this function with a neural network and fit its parameters to maximize the discrepancy this yields a novel goodnessoffit test which outperforms existing methods on high dimensional data furthermore optimizing qx to minimize this discrepancy produces a novel method for training unnormalized models which scales more gracefully than existing methods the ability to both learn and compare models is a unique feature of the proposed method less,Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling,"14 August, 2020"
174,David Duvenaud,we propose a new family of efficient and expressive deep generative models of graphs called graph recurrent attention networks grans our model generates graphs one block of nodes and associated edges at a time the block size and sampling stride allow us to trade off sample quality for efficiency compared to previous rnnbased graph generative models our framework better captures the autoregressive conditioning between the alreadygenerated and tobegenerated parts of the graph using graph neural networks gnns with attention this not only reduces the dependency on node ordering but also bypasses the longterm bottleneck caused by the sequential nature of rnns moreover we parameterize the output distribution per block using a mixture of bernoulli which captures the correlations among generated edges within the block finally we propose to handle node orderings in generation by marginalizing over a family of canonical orderings on standard benchmarks we achieve stateoftheart time efficiency and sample quality compared to previous models additionally we show our model is capable of generating large graphs of up to k nodes with good quality to the best of our knowledge gran is the first deep graph generative model that can scale to this size our code is released at httpsgithubcomlrjconangran less,Efficient Graph Generation with Graph Recurrent Attention Networks,"17 July, 2020"
175,David Duvenaud,the impact of gradient noise on training deep models is widely acknowledged but not well understood in this context we study the distribution of gradients during training we introduce a method gradient clustering to minimize the variance of average minibatch gradient with stratified sampling we prove that the variance of average minibatch gradient is minimized if the elements are sampled from a weighted clustering in the gradient space we measure the gradient variance on common deep learning benchmarks and observe that contrary to common assumptions gradient variance increases during training and smaller learning rates coincide with higher variance in addition we introduce normalized gradient variance as a statistic that better correlates with the speed of convergence compared to gradient variance less,A Study of Gradient Variance in Deep Learning,"8 July, 2020"
176,David Duvenaud,gradients of neural networks can be computed efficiently for any architecture but some applications require differential operators with higher time complexity we describe a family of restricted neural network architectures that allow efficient computation of a family of differential operators involving dimensionwise derivatives used in cases such as computing the divergence our proposed architecture has a jacobian matrix composed of diagonal and hollow nondiagonal components we can then modify the backward computation graph to extract dimensionwise derivatives efficiently with automatic differentiation we demonstrate these cheap differential operators for solving rootfinding subproblems in implicit ode solvers exact density evaluation for continuous normalizing flows and evaluating the fokkerplanck equation for training stochastic differential equation models less,Neural Networks with Cheap Differential Operators,"7 December, 2019"
177,David Duvenaud,word embeddings are often criticized for capturing undesirable word associations such as gender stereotypes however methods for measuring and removing such biases remain poorly understood we show that for any embedding model that implicitly does matrix factorization debiasing vectors post hoc using subspace projection bolukbasi et al is under certain conditions equivalent to training on an unbiased corpus we also prove that weat the most common association test for word embeddings systematically overestimates bias given that the subspace projection method is provably effective we use it to derive a new measure of association called the textitrelational inner product association ripa experiments with ripa reveal that on average skipgram with negative sampling sgns does not make most words any more gendered than they are in the training corpus however for genderstereotyped words sgns actually amplifies the gender association in the corpus less,Understanding Undesirable Word Embedding Associations,"17 August, 2019"
178,David Duvenaud,time series with nonuniform intervals occur in many applications and are difficult to model using standard recurrent neural networks rnns we generalize rnns to have continuoustime hidden dynamics defined by ordinary differential equations odes a model we call odernns furthermore we use odernns to replace the recognition network of the recentlyproposed latent ode model both odernns and latent odes can naturally handle arbitrary time gaps between observations and can explicitly model the probability of observation times using poisson processes we show experimentally that these odebased models outperform their rnnbased counterparts on irregularlysampled data less,Latent ODEs for Irregularly-Sampled Time Series,"8 July, 2019"
179,David Duvenaud,many deep learning algorithms can be easily fooled with simple adversarial examples to address the limitations of existing defenses we devised a probabilistic framework that can generate an exponentially large ensemble of models from a single model with just a linear cost this framework takes advantage of neural network depth and stochastically decides whether or not to insert noise removal operators such as vaes between layers we show empirically the important role that model gradients have when it comes to determining transferability of adversarial examples and take advantage of this result to demonstrate that it is possible to train models with limited adversarial attack transferability additionally we propose a detection method based on metric learning in order to detect adversarial examples that have no hope of being cleaned of maliciously engineered noise less,Stochastic Combinatorial Ensembles for Defending Against Adversarial Examples,"8 September, 2018"
180,David Duvenaud,amortized inference allows latentvariable models trained via variational learning to scale to large datasets the quality of approximate inference is determined by two factors a the capacity of the variational distribution to match the true posterior and b the ability of the recognition network to produce good variational parameters for each datapoint we examine approximate inference in variational autoencoders in terms of these factors we find that divergence from the true posterior is often due to imperfect recognition networks rather than the limited complexity of the approximating distribution we show that this is due partly to the generator learning to accommodate the choice of approximation furthermore we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation less,Inference Suboptimality in Variational Autoencoders,"27 May, 2018"
181,David Duvenaud,variational bayesian neural nets combine the flexibility of deep learning with bayesian uncertainty estimation unfortunately there is a tradeoff between cheap but simple variational families egfully factorized or expensive and complicated inference procedures we show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound elbo this insight allows us to train fullcovariance fully factorized or matrixvariate gaussian variational posteriors using noisy versions of natural gradient adam and kfac respectively making it possible to scale up to modernsize convnets on standard regression benchmarks our noisy kfac algorithm makes better predictions and matches hamiltonian monte carlos predictive variances better than existing methods its improved uncertainty estimates lead to more efficient exploration in active learning and intrinsic motivation for reinforcement learning less,Noisy Natural Gradient as Variational Inference,"26 February, 2018"
182,David Duvenaud,we propose generative neural network methods to generate dna sequences and tune them to have desired properties we present three approaches creating synthetic dna sequences using a generative adversarial network a dnabased variant of the activation maximization deep dream design method and a joint procedure which combines these two approaches together we show that these tools capture important structures of the data and when applied to designing probes for protein binding microarrays allow us to generate new sequences whose properties are estimated to be superior to those found in the training data we believe that these results open the door for applying deep generative models to advance genomics research less,Generating and designing DNA with deep generative models,"17 December, 2017"
183,David Duvenaud,the standard interpretation of importanceweighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound we give an alternate interpretation of this procedure that it optimizes the standard variational lower bound but using a more complex distribution we formally derive this result present a tighter lower bound and visualize the implicit importanceweighted distribution less,Reinterpreting Importance-Weighted Autoencoders,"14 August, 2017"
184,David Duvenaud,we propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound specifically we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior we analyze the behavior of this gradient estimator theoretically and empirically and generalize it to more complex variational distributions such as mixtures and importanceweighted posteriors less,"Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference","28 May, 2017"
185,David Duvenaud,herding and kernel herding are deterministic methods of choosing samples which summarise a probability distribution a related task is choosing samples for estimating integrals using bayesian quadrature we show that the criterion minimised when selecting samples in kernel herding is equivalent to the posterior variance in bayesian quadrature we then show that sequential bayesian quadrature can be viewed as a weighted version of kernel herding which achieves performance superior to any other weighted herding method we demonstrate empirically a rate of convergence faster than on our results also imply an upper bound on the empirical error of the bayesian quadrature estimate less,Optimally-Weighted Herding is Bayesian Quadrature,"15 July, 2016"
186,David Duvenaud,choosing appropriate architectures and regularization strategies for deep networks is crucial to good predictive performance to shed light on this problem we analyze the analogous problem of constructing useful priors on compositions of functions specifically we study the deep gaussian process a type of infinitelywide deep neural network we show that in standard architectures the representational capacity of the network tends to capture fewer degrees of freedom as the number of layers increases retaining only a single degree of freedom in the limit we propose an alternate network architecture which does not suffer from this pathology we also examine deep covariance functions obtained by composing infinitely many feature transforms lastly we characterize the class of models obtained by performing dropout on gaussian processes less,Avoiding pathologies in very deep networks,"8 July, 2016"
187,David Duvenaud,we show that unconverged stochastic gradient descent can be interpreted as a procedure that samples from a nonparametric variational approximate posterior distribution this distribution is implicitly defined as the transformation of an initial distribution by a sequence of optimization updates by tracking the change in entropy over this sequence of transformations during optimization we form a scalable unbiased estimate of the variational lower bound on the log marginal likelihood we can use this bound to optimize hyperparameters instead of using crossvalidation this bayesian interpretation of sgd suggests improved overfittingresistant optimization procedures and gives a theoretical foundation for popular tricks such as early stopping and ensembling we investigate the properties of this marginal likelihood estimator on neural network models less,Early Stopping is Nonparametric Variational Inference,"6 April, 2015"
188,David Duvenaud,markov chain monte carlo mcmc algorithms are a workhorse of probabilistic modeling and inference but are difficult to debug and are prone to silent failure if implemented naively we outline several strategies for testing the correctness of mcmc algorithms specifically we advocate writing code in a modular way where conditional probability calculations are kept separate from the logic of the sampler we discuss strategies for both unit testing and integration testing as a running example we show how a python implementation of gibbs sampling for a mixture of gaussians model can be tested less,Testing MCMC code,"16 December, 2014"
189,David Duvenaud,in practical bayesian optimization we must often search over structures with differing numbers of parameters for instance we may wish to search over neural network architectures with an unknown number of layers to relate performance data gathered for different architectures we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure we show that this kernel improves model quality and bayesian optimization results over several simpler baseline kernels less,Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces,"14 September, 2014"
190,Steve Easterbrook,recent works in video prediction have mainly focused on passive forecasting and lowlevel actionconditional prediction which sidesteps the learning of interaction between agents and objects we introduce the task of semantic actionconditional video prediction which uses semantic action labels to describe those interactions and can be regarded as an inverse problem of action recognition the challenge of this new task primarily lies in how to effectively inform the model of semantic action information inspired by the idea of mixture of experts we embody each abstract label by a structured combination of various visual concept learners and propose a novel video prediction model modular action concept network mac our method is evaluated on two newly designed synthetic datasets clevrbuildingblocks and sapienkitchen and one realworld dataset called towercreation extensive experiments demonstrate that mac can correctly condition on given instructions and generate corresponding future frames without need of bounding boxes we further show that the trained model can make outofdistribution generalization be quickly adapted to new object categories and exploit its learnt features for object detection showing the progression towards higherlevel cognitive abilities more visualizations can be found at httpwwwpairtorontoedumac less,Modular Action Concept Grounding in Semantic Video Prediction,"26 April, 2022"
191,Steve Easterbrook,climate projections suffer from uncertain equilibrium climate sensitivity the reason behind this uncertainty is the resolution of global climate models which is too coarse to resolve key processes such as clouds and convection these processes are approximated using heuristics in a process called parameterization the selection of these parameters can be subjective leading to significant uncertainties in the way clouds are represented in global climate models here we explore three deep network algorithms to infer these parameters in an objective and datadriven way we compare the performance of a fullyconnected network a onedimensional and a twodimensional convolutional networks to recover the underlying parameters of the lorenz model a nonlinear dynamical system that has similar behavior to the climate system less,Recovering the parameters underlying the Lorenz-96 chaotic dynamics,"16 June, 2019"
192,Steve Easterbrook,"This paper describes a simple decision support system (DSS) 2 intended to aid urban planners in visualizing the impacts of planning interventions on urban infrastructure systems. It demonstrates a means by which urban planners can visualize the impact of zoning decisions or analyze the effects of growth. The software provides users with a district-level model in which each infrastructure system is represented as a network that delivers resources to buildings. Interdependencies between individual infrastructure systems allow demands from one system to propagate to another, providing the user with the ability to visualize changes across the entire set of infrastructures.",A constraint-based decision support system for capacity planning in interdependent evolving urban systems,2021
193,Steve Easterbrook,"Gravity waves play an essential role in driving and maintaining global circulation. To understand their contribution in the atmosphere, the accurate reproduction of their distribution is important. Thus, a deep learning approach for the estimation of gravity wave momentum fluxes was proposed, and its performance at 100 hPa was tested using data from low‐resolution zonal and meridional winds, temperature, and specific humidity at 300, 700, and 850 hPa in the Hokkaido region (Japan). To this end, a deep convolutional neural network was trained on 29‐year reanalysis data sets (JRA‐55 and DSJRA‐55), and the final 5‐year data were reserved for evaluation. The results showed that the fine‐scale momentum flux distribution of the gravity waves could be estimated at a reasonable computational cost. Particularly, in winter, when gravity waves are stronger, the median root means square errors (RMSEs) of the ",Application of deep learning to estimate atmospheric gravity wave parameters in reanalysis data sets,2020-10-16 00:00:00
194,Steve Easterbrook,"Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational efficiency. The lightweight nature of our model enables us to incorporate 3D convolutions without concern of memory bottleneck, enhancing the model’s ability to capture both short-term and long-term temporal dependencies. Our proposed approach achieves state-of-the-art results on Moving MNIST, Traffic4cast and KITTI datasets. We further demonstrate the transferability of our self-supervised learning method by exploiting its learnt features for object detection on KITTI. Our competitive results indicate the potential of using CrevNet as a generative pre-training strategy to guide downstream tasks.",Efficient and information-preserving future frame prediction and beyond,2020-09-23 00:00:00
195,Steve Easterbrook,"Although it has never been rigorously demonstrated, there is a common belief that grades in computer science courses are bimodal. We statistically analyzed 778 distributions of final course grades from a large research university and found that only 5.8% of the distributions passed tests of multimodality. We then devised a psychology experiment to understand why CS educators believe their grades to be bimodal. We showed 53 CS professors a series of histograms displaying ambiguous distributions that we asked them to categorize. A random half of participants were primed to think about the fact that CS grades are commonly thought to be bimodal; these participants were more likely to label ambiguous distributions as ""bimodal."" Participants were also more likely to label distributions as bimodal if they believed that some students are innately predisposed to do better at CS. These results suggest that bimodal ",Evidence that computer science grades are not bimodal,2019-12-20 00:00:00
196,Steve Easterbrook,"Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational efficiency.",Crevnet: Conditionally reversible video prediction,2019-10-25 00:00:00
197,Steve Easterbrook,"Climate projections suffer from uncertain equilibrium climate sensitivity. The reason behind this uncertainty is the resolution of global climate models, which is too coarse to resolve key processes such as clouds and convection. These processes are approximated using heuristics in a process called parameterization. The selection of these parameters can be subjective, leading to significant uncertainties in the way clouds are represented in global climate models. Here, we explore three deep network algorithms to infer these parameters in an objective and data-driven way. We compare the performance of a fully-connected network, a one-dimensional and, a two-dimensional convolutional networks to recover the underlying parameters of the Lorenz-96 model, a non-linear dynamical system that has similar behavior to the climate system.",Recovering the parameters underlying the Lorenz-96 chaotic dynamics,2019-06-16 00:00:00
198,Steve Easterbrook,"Research in ICT about forced displacement focuses mainly on refugees. Internally displaced people (IDPs), however, are rarely discussed in ICT and related disciplines. This paper aims to fill in the gap and provide an insight into the everyday lives of conflict-driven IDPs and their ICTs usage based on our original fieldwork at several IDP and refugee camps in northern Iraq. Our work includes extended field observations, surveys with 86 IDPs and 47 refugees, and examination of recent reports about IDPs from international NGOs that have been active in that region. Our findings illustrate that IDPs live under similar resource-constrained environment as refugees and, in some cases, suffer from even harsher restrictions. We highlight how these confines limit their ICTs usage and discuss opportunities for future ICT research and policy implication to improve the quality of life of the displaced residing within their own ",Exile within borders: Understanding the limits of the internally displaced people (IDPs) in Iraq,2019-06-10 00:00:00
199,Steve Easterbrook,"Climate Science has a long history, and telling the story of the discovery of climate change can be a powerful way of improving students' understanding of the key concepts underlying the greenhouse effect. The Arrhenius Project is an effort to rebuild Svante Arrhenius' 1896 energy balance model in Python, and will be used as part of an educational tool for grade 10-12 science classrooms. Students in these grades are prone to misconceptions about the mechanisms of the greenhouse effect, largely due to a lack of curriculum on these mechanisms. The goals of the Arrhenius Project are to clear up some of these misconceptions while improving student understanding of the theory behind climate modelling.",The Arrhenius Model of Climate Change as a Classroom Experiment,2018/12
200,Steve Easterbrook,"Svante Arrhenius's two-dimensional energy balance model is widely recognized as the first ever climate model, and is lauded in histories of climate science for producing values of climate sensitivity remarkably close to the current IPCC uncertainty range. However, in his 2009 thesis, Dufresne showed that the warming signal in Arrhenius's model is entirely due to a systematic error in the radiation data used by Arrhenius for his model, and that with modern radiation transmittance data, the model would show virtually no warming under doubling of CO2. Dufresne hypothesized that the errors in the radiation data are almost entirely balanced by the fact that Arrhenius used a single layer version of his model for his main results-a simplification he made due to computational limits. We have completed a detailed re-implementation of Arrhenius's model to explore these errors in more detail. By switching between Arrhenius's ",Correcting Historical Errors: An investigation of systematic errors in Arrhenius's 1896 climate model,2018/12
201,Steve Easterbrook,to fork a project is to copy the existing code base and move in a direction different than that of the erstwhile project leadership forking provides a rapid way to address new requirements by adapting an existing solution however it can also create a plethora of similar tools and fragment the developer community hence it is not always clear whether forking is the right strategy in this paper we describe a mixedmethods exploratory case study that investigated the process of forking a project the study concerned the forking of an opensource tool for managing software projects trac trac was forked to address differing requirements in an academic setting the paper makes two contributions to our understanding of code forking first our exploratory study generated several theories about code forking in open source projects for further research second we investigated one of these theories in depth via a quantitative study we conjectured that the features of the oss forking process would allow new requirements to be addressed we show that the forking process in this case was successful at fulfilling the new projects requirements less,Code forking in open-source software: a requirements perspective,"16 April, 2010"
202,Faith Ellen,approximate agreement is one of the few variants of consensus that can be solved in a waitfree manner in asynchronous systems where processes communicate by reading and writing to shared memory in this work we consider a natural generalisation of approximate agreement on arbitrary undirected connected graphs each process is given a vertex of the graph as input and if nonfaulty must output a vertex such that all the outputs are within distance of one another and each output value lies on a shortest path between two input values from prior work it is known that there is no waitfree algorithm among n ge processes for this problem on any cycle of length c ge by reduction from set agreement castaeda et al in this work we investigate the solvability and complexity of this task on general graphs we give a new direct proof of the impossibility of approximate agreement on cycles of length c ge via a generalisation of sperners lemma to convex polygons we also extend the reduction from set agreement to a larger class of graphs showing that approximate agreement on on these graphs is unsolvable furthermore we show that combinatorial arguments used by both existing proofs are necessary by showing that the impossibility of a waitfree algorithm in the nonuniform iterated snapshot model cannot be proved via an extensionbased proof on the positive side we present a waitfree algorithm for a class of graphs that properly contains the class of chordal graphs less,Wait-free approximate agreement on graphs,"16 March, 2021"
203,Faith Ellen,broadcast is one of the fundamental network communication primitives one node of a network called the mathitsource has a message that has to be learned by all other nodes we consider the feasibility of deterministic broadcast in radio networks if nodes of the network do not have any labels deterministic broadcast is impossible even in the fourcycle on the other hand if all nodes have distinct labels then broadcast can be carried out eg in a roundrobin fashion and hence olog nbit labels are sufficient for this task in nnode networks in fact olog bit labels where is the maximum degree are enough to broadcast successfully hence it is natural to ask if very short labels are sufficient for broadcast our main result is a positive answer to this question we show that every radio network can be labeled using bits in such a way that broadcast can be accomplished by some universal deterministic algorithm that does not know the network topology nor any bound on its size moreover at the expense of an extra bit in the labels we get the additional strong property that there exists a common round in which all nodes know that broadcast has been completed finally we show that bit labels are also sufficient to solve both versions of broadcast in the case where the labeling scheme does not know which node is the source less,Constant-Length Labeling Schemes for Deterministic Radio Broadcast,"2 August, 2019"
204,Faith Ellen,this work is a continuation of efforts to define and understand competitive analysis of algorithms in a distributed shared memory setting which is surprisingly different from the classical online setting in fact in a distributed shared memory setting we find a counterexample to the theorem concerning classical randomized online algorithms which shows that if there is a ccompetitive randomized algorithm against an adaptive offline adversary then there is a ccompetitive deterministic algorithm bendavid borodin karp tardos wigderson in a distributed setting there is additional lack of knowledge concerning what the other processes have done there is also additional power for the adversary having control of the scheduler which decides when each process is allowed to take steps we consider the list accessing problem which is a benchmark problem for sequential online algorithms in the distributed version of this problem each process has its own finite sequence of requests to a shared list the scheduler arises as a major issue in its competitive analysis we introduce two different adversaries which differ in how they are allowed to schedule processes and use them to perform competitive analysis of distributed list accessing we prove tight upper and lower bounds on combinatorial properties of merges of the request sequences which we use in the analysis our analysis shows that the effects of the adversarial scheduler can be quite significant dominating the usual quality loss due to lack of information about the future less,The Scheduler is Very Powerful in Competitive Analysis of Distributed List Accessing,"18 July, 2018"
205,Faith Ellen,for many years herlihys elegant computability based consensus hierarchy has been our best explanation of the relative power of various types of multiprocessor synchronization objects when used in deterministic algorithms however key to this hierarchy is treating synchronization instructions as distinct objects an approach that is far from the realworld where multiprocessor programs apply synchronization instructions to collections of arbitrary memory locations we were surprised to realize that when considering instructions applied to memory locations the computability based hierarchy collapses this leaves open the question of how to better capture the power of various synchronization instructions in this paper we provide an approach to answering this question we present a hierarchy of synchronization instructions classified by their space complexity in solving obstructionfree consensus our hierarchy provides a classification of combinations of known instructions that seems to fit with our intuition of how useful some are in practice while questioning the effectiveness of others we prove an essentially tight characterization of the power of buffered read and write instructionsinterestingly we show a similar result for multilocation atomic assignments less,A Complexity-Based Hierarchy for Multiprocessor Synchronization,"3 May, 2018"
206,Faith Ellen,we describe a general technique for obtaining provably correct nonblocking implementations of a large class of tree data structures where pointers are directed from parents to children updates are permitted to modify any contiguous portion of the tree atomically our nonblocking algorithms make use of the llx scx and vlx primitives which are multiword generalizations of the standard ll sc and vl primitives and have been implemented from singleword cas to illustrate our technique we describe how it can be used in a fairly straightforward way to obtain a nonblocking implementation of a chromatic tree which is a relaxed variant of a redblack tree the height of the tree at any time is oc log n where n is the number of keys and c is the number of updates in progress we provide an experimental performance analysis which demonstrates that our java implementation of a chromatic tree rivals and often significantly outperforms other leading concurrent dictionaries less,A General Technique for Non-blocking Trees,"18 December, 2017"
207,Murat Erdogdu,we study stochastic convex optimization under infinite noise variance specifically when the stochastic gradient is unbiased and has uniformly bounded th moment for some in we quantify the convergence rate of the stochastic mirror descent algorithm with a particular class of uniformly convex mirror maps in terms of the number of iterations dimensionality and related geometric parameters of the optimization problem interestingly this algorithm does not require any explicit gradient clipping or normalization which have been extensively used in several recent empirical and theoretical works we complement our convergence results with informationtheoretic lower bounds showing that no other algorithm using only stochastic firstorder oracles can achieve improved rates our results have several interesting consequences for devising onlinestreaming stochastic approximation algorithms for problems arising in robust statistics and machine learning less,Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization under Infinite Noise Variance,"23 February, 2022"
208,Murat Erdogdu,in stochastic optimization the population risk is generally approximated by the empirical risk however in the largescale setting minimization of the empirical risk may be computationally restrictive in this paper we design an efficient algorithm to approximate the population risk minimizer in generalized linear problems such as binary classification with surrogate losses and generalized linear regression models we focus on largescale problems where the iterative minimization of the empirical risk is computationally intractable ie the number of observations n is much larger than the dimension of the parameter p ie n gg p gg we show that under random subgaussian design the true minimizer of the population risk is approximately proportional to the corresponding ordinary least squares ols estimator using this relation we design an algorithm that achieves the same accuracy as the empirical risk minimizer through iterations that attain up to a cubic convergence rate and that are cheaper than any batch optimization algorithm by at least a factor of mathcalop we provide theoretical guarantees for our algorithm and analyze the convergence behavior in terms of data dimensions finally we demonstrate the performance of our algorithm on wellknown classification and regression problems through extensive numerical studies on largescale datasets and show that it achieves the highest performance compared to several other widely used and specialized optimization algorithms less,Scalable Approximations for Generalized Linear Problems,"21 November, 2016"
209,Murat Erdogdu,"We study stochastic convex optimization under infinite noise variance. Specifically, when the stochastic gradient is unbiased and has uniformly bounded -th moment, for some , we quantify the convergence rate of the Stochastic Mirror Descent algorithm with a particular class of uniformly convex mirror maps, in terms of the number of iterations, dimensionality and related geometric parameters of the optimization problem. Interestingly this algorithm does not require any explicit gradient clipping or normalization, which have been extensively used in several recent empirical and theoretical works. We complement our convergence results with information-theoretic lower bounds showing that no other algorithm using only stochastic first-order oracles can achieve improved rates. Our results have several interesting consequences for devising online/streaming stochastic approximation algorithms for problems arising in robust statistics and machine learning.",Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization under Infinite Noise Variance,2022-06-28 00:00:00
210,Murat Erdogdu,"For the task of sampling from a density  on $\R^ d $, where  is possibly non-convex but -gradient Lipschitz, we prove that averaged Langevin Monte Carlo outputs a sample with -relative Fisher information after  iterations. This is the sampling analogue of complexity bounds for finding an -approximate first-order stationary points in non-convex optimization and therefore constitutes a first step towards the general theory of non-log-concave sampling. We discuss numerous extensions and applications of our result; in particular, it yields a new state-of-the-art guarantee for sampling from distributions which satisfy a Poincaré inequality.",Towards a theory of non-log-concave sampling: first-order stationarity guarantees for Langevin Monte Carlo,2022-06-28 00:00:00
211,Murat Erdogdu,"Policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success, yet existing convergence analysis still relies on non-intuitive, impractical and often opaque conditions. In particular, existing rates are achieved in limited settings, under strict regularity conditions. In this work, we establish explicit convergence rates of policy gradient methods, extending the convergence regime to weakly smooth policy classes with L2 integrable gradient. We provide intuitive examples to illustrate the insight behind these new conditions. Notably, our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions. Lastly we provide performance guarantees for the converged policies.",Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings,2022-06-28 00:00:00
212,Murat Erdogdu,"n the proportional asymptotic limit where  at the same rate, and an idealized student-teacher setting, we show that the first gradient update contains a rank-1 ""spike"", which results in an alignment between the first-layer weights and the linear component of the teacher model . To characterize the impact of this alignment, we compute the prediction risk of ridge regression on the conjugate kernel after one gradient step on  with learning rate , when  is a single-index model. We consider two scalings of the first step learning rate . For small , we establish a Gaussian equivalence property for the trained feature map, and prove that the learned kernel improves upon the initial random features model, but cannot defeat the best linear model on the input. Whereas for sufficiently large , we prove that for certain , the same ridge estimator on trained features can go beyond this ""linear regime"" and outperform a wide range of random features and rotationally invariant kernels. Our results demonstrate that even one gradient step can lead to a considerable advantage over random features, and highlight the role of learning rate scaling in the initial phase of training.",High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation,2022-05-03 00:00:00
213,Murat Erdogdu,"We study stochastic convex optimization under infinite noise variance. Specifically, when the stochastic gradient is unbiased and has uniformly bounded -th moment, for some , we quantify the convergence rate of the Stochastic Mirror Descent algorithm with a particular class of uniformly convex mirror maps, in terms of the number of iterations, dimensionality and related geometric parameters of the optimization problem. Interestingly this algorithm does not require any explicit gradient clipping or normalization, which have been extensively used in several recent empirical and theoretical works. We complement our convergence results with information-theoretic lower bounds showing that no other algorithm using only stochastic first-order oracles can achieve improved rates. Our results have several interesting consequences for devising online/streaming stochastic approximation algorithms for",Mirror Descent Strikes Again: Optimal Stochastic Convex Optimization under Infinite Noise Variance,2022/2
214,Murat Erdogdu,"We analyze the oracle complexity of sampling from polynomially decaying heavy-tailed target densities based on running the Unadjusted Langevin Algorithm on certain transformed versions of the target density. The specific class of closed-form transformation maps that we construct are shown to be diffeomorphisms, and are particularly suited for developing efficient diffusion-based samplers. We characterize the precise class of heavy-tailed densities for which polynomial-order oracle complexities (in dimension and inverse target accuracy) could be obtained, and provide illustrative examples. We highlight the relationship between our assumptions and functional inequalities (super and weak Poincar\'e inequalities) based on non-local Dirichlet forms defined via fractional Laplacian operators, used to characterize the heavy-tailed equilibrium densities of certain stable-driven stochastic differential equations.",Heavy-tailed sampling via transformed unadjusted Langevin algorithm,2022-01-20 00:00:00
215,Murat Erdogdu,"Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution  under the sole assumption that  satisfies a Poincar\'e inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\'enyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that  satisfies either a Lata{\l}a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\'e and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions.",Analysis of Langevin Monte Carlo from Poincaré to Log-Sobolev,2021-12-23 00:00:00
216,Murat Erdogdu,"In this work, we establish risk bounds for Empirical Risk Minimization (ERM) with both dependent and heavy-tailed data-generating processes. We do so by extending the seminal works~\cite {pmlr-v35-mendelson14, mendelson2018learning} on the analysis of ERM with heavy-tailed but independent and identically distributed observations, to the strictly stationary exponentially -mixing case. We allow for the interaction between the noise and inputs to be even polynomially heavy-tailed, which covers a significantly large class of heavy-tailed models beyond what is analyzed in the learning theory literature. We illustrate our theoretical results by obtaining rates of convergence for high-dimensional linear regression with dependent and heavy-tailed data.",On Empirical Risk Minimization with Dependent and Heavy-Tailed Data,2021-12-06 00:00:00
217,Murat Erdogdu,"Understanding generalization in deep learning has been one of the major challenges in statistical learning theory over the last decade. While recent work has illustrated that the dataset and the training algorithm must be taken into account in order to obtain meaningful generalization bounds, it is still theoretically not clear which properties of the data and the algorithm determine the generalization performance. In this study, we approach this problem from a dynamical systems theory perspective and represent stochastic optimization algorithms as\emph {random iterated function systems}(IFS). Well studied in the dynamical systems literature, under mild assumptions, such IFSs can be shown to be ergodic with an invariant measure that is often supported on sets with a\emph {fractal structure}. As our main contribution, we prove that the generalization error of a stochastic optimization algorithm can be bounded based on thecomplexity'of the fractal structure that underlies its invariant measure. Then, by leveraging results from dynamical systems theory, we show that the generalization error can be explicitly linked to the choice of the algorithm (eg, stochastic gradient descent--SGD), algorithm hyperparameters (eg, step-size, batch-size), and the geometry of the problem (eg, Hessian of the loss). We further specialize our results to specific problems (eg, linear/logistic regression, one hidden-layered neural networks) and algorithms (eg, SGD and preconditioned variants), and obtain analytical estimates for our bound. For modern neural networks, we develop an efficient algorithm to compute the developed bound and support our theory with various",Fractal structure and generalization properties of stochastic optimization algorithms,2021-12-06 00:00:00
218,Murat Erdogdu,"Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in specific senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying causes that make the networks amenable to such simple compression schemes is still missing. In this study, focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD:(i) as the network size goes to infinity, the system can converge to a mean-field limit, where the network weights behave independently [DBDFŞ20],(ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution [HM20, GŞZ21]. Assuming that both of these phenomena occur simultaneously, we prove that the networks are guaranteed to be'-compressible', and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which are consistent with the observation that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy tails, which, in combination with overparametrization, result ",Heavy tails in SGD and compressibility of overparametrized neural networks,2021-12-06 00:00:00
219,Murat Erdogdu,"Machine learning is vulnerable to a wide variety of attacks. It is now well understood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we find that the attacker can either prevent the model from learning, or poison it to learn behaviours specified by the attacker. Furthermore, we find that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not specific to the model or dataset, but rather target the stochastic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to find that the adversary can disrupt model training and even introduce backdoors.",Manipulating sgd with data ordering attacks,2021-12-06 00:00:00
220,Murat Erdogdu,"Recent studies have provided both empirical and theoretical evidence illustrating that heavy tails can emerge in stochastic gradient descent (SGD) in various scenarios. Such heavy tails potentially result in iterates with diverging variance, which hinders the use of conventional convergence analysis techniques that rely on the existence of the second-order moments. In this paper, we provide convergence guarantees for SGD under a state-dependent and heavy-tailed noise with a potentially infinite variance, for a class of strongly convex objectives. In the case where the -th moment of the noise exists for some , we first identify a condition on the Hessian, coined-positive (semi-) definiteness', that leads to an interesting interpolation between the positive semi-definite cone () and the cone of diagonally dominant matrices with non-negative diagonal entries (). Under this condition, we provide a convergence rate for the distance to the global optimum in . Furthermore, we provide a generalized central limit theorem, which shows that the properly scaled Polyak-Ruppert averaging converges weakly to a multivariate -stable random vector. Our results indicate that even under heavy-tailed noise with infinite variance, SGD can converge to the global optimum without necessitating any modification neither to the loss function nor to the algorithm itself, as typically required in robust statistics. We demonstrate the implications of our resultsover misspecified models, in the presence of heavy-tailed data.",Convergence rates of stochastic gradient descent under infinite noise variance,2021-12-06 00:00:00
221,Murat Erdogdu,"Stein variational gradient descent (SVGD) is a deterministic inference algorithm that evolves a set of particles to fit a target distribution. Despite its computational efficiency, SVGD often underestimates the variance of the target distribution in high dimensions. In this work we attempt to explain the variance collapse in SVGD. On the qualitative side, we compare the SVGD update with gradient descent on the maximum mean discrepancy (MMD) objective; we observe that the variance collapse phenomenon relates to the bias from deterministic updates present in the"" driving force"" of SVGD, and empirically verify that removal of such bias leads to more accurate variance estimation. On the quantitative side, we demonstrate that the variance collapse of SVGD can be accurately predicted in the proportional asymptotic limit, ie, when the number of particles  and dimensions  diverge at the same rate. In particular, for learning high-dimensional isotropic Gaussians, we derive the exact equilibrium variance for both SVGD and MMD-descent under certain near-orthogonality assumption on the converged particles, and confirm that SVGD suffers from the"" curse of dimensionality"".",Understanding the Variance Collapse of SVGD in High Dimensions,2021-09-29 00:00:00
222,Amir-massoud Farahmand,prioritized experience replay er has been empirically shown to improve sample efficiency across many domains and attracted great attention however there is little theoretical understanding of why such prioritized sampling helps and its limitations in this work we take a deep look at the prioritized er in a supervised learning setting we show the equivalence between the errorbased prioritized sampling method for mean squared error and uniform sampling for cubic power loss we then provide theoretical insight into why it improves convergence rate upon uniform sampling during early learning based on the insight we further point out two limitations of the prioritized er method outdated priorities and insufficient coverage of the sample space to mitigate the limitations we propose our modelbased stochastic gradient langevin dynamics sampling method we show that our method does provide states distributed close to an ideal prioritized sampling distribution estimated by the bruteforce method which does not suffer from the two limitations we conduct experiments on both discrete and continuous control problems to show our approachs efficacy and examine the practical implication of our method in an autonomous driving application less,Understanding and Mitigating the Limitations of Prioritized Experience Replay,"11 June, 2022"
223,Amir-massoud Farahmand,in many areas such as the physical sciences life sciences and finance control approaches are used to achieve a desired goal in complex dynamical systems governed by differential equations in this work we formulate the problem of controlling stochastic partial differential equations spde as a reinforcement learning problem we present a learningbased distributed control approach for online control of a system of spdes with high dimensional stateaction space using deep deterministic policy gradient method we tested the performance of our method on the problem of controlling the stochastic burgers equation describing a turbulent fluid flow in an infinitely large domain less,Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations,"8 December, 2021"
224,Amir-massoud Farahmand,this paper considers the problem of learning a model in modelbased reinforcement learning mbrl we examine how the planning module of an mbrl algorithm uses the model and propose that the model learning module should incorporate the way the planner is going to use the model this is in contrast to conventional model learning approaches such as those based on maximum likelihood estimate that learn a predictive model of the environment without explicitly considering the interaction of the model and the planner we focus on policy gradient type of planning algorithms and derive new loss functions for model learning that incorporate how the planner uses the model we call this approach policyaware model learning paml we theoretically analyze a generic modelbased policy gradient algorithm and provide a convergence guarantee for the optimized policy we also empirically evaluate paml on some benchmark problems showing promising results less,Policy-Aware Model Learning for Policy Gradient Methods,"3 January, 2021"
225,Amir-massoud Farahmand,reinforcement learning rl agents typically learn memoryless policiespolicies that only consider the last observation when selecting actions learning memoryless policies is efficient and optimal in fully observable environments however some form of memory is necessary when rl agents are faced with partial observability in this paper we study a lightweight approach to tackle partial observability in rl we provide the agent with an external memory and additional actions to control what if anything is written to the memory at every step the current memory state is part of the agents observation and the agent selects a tuple of actions one action that modifies the environment and another that modifies the memory when the external memory is sufficiently expressive optimal memoryless policies yield globally optimal solutions unfortunately previous attempts to use external memory in the form of binary memory have produced poor results in practice here we investigate alternative forms of memory in support of learning effective memoryless policies our novel forms of memory outperform binary and lstmbased memory in wellestablished partially observable domains less,The act of remembering: a study in partially observable reinforcement learning,"4 October, 2020"
226,Amir-massoud Farahmand,we present a visual symptom checker that combines a pretrained convolutional neural network cnn with a reinforcement learning rl agent as a question answering qa model this method increases the classification confidence and accuracy of the visual symptom checker and decreases the average number of questions asked to narrow down the differential diagnosis a deep qnetwork dqnbased rl agent learns how to ask the patient about the presence of symptoms in order to maximize the probability of correctly identifying the underlying condition the rl agent uses the visual information provided by cnn in addition to the answers to the asked questions to guide the qa system we demonstrate that the rlbased approach increases the accuracy more than compared to the cnnonly approach which only uses the visual information to predict the condition moreover the increased accuracy is up to compared to the approach that uses the visual information provided by cnn along with a conventional decision treebased qa system we finally show that the rlbased approach not only outperforms the decision treebased approach but also narrows down the diagnosis faster in terms of the average number of asked questions less,Improving Skin Condition Classification with a Visual Symptom Checker Trained using Reinforcement Learning,"7 August, 2019"
227,Amir-massoud Farahmand,we propose augmenting deep neural networks with an attention mechanism for the visual object detection task as perceiving a scene humans have the capability of multiple fixation points each attended to scene content at different locations and scales however such a mechanism is missing in the current stateoftheart visual object detection methods inspired by the human vision system we propose a novel deep network architecture that imitates this attention mechanism as detecting objects in an image the network adaptively places a sequence of glimpses of different shapes at different locations in the image evidences of the presence of an object and its location are extracted from these glimpses which are then fused for estimating the object class and bounding box coordinates due to lacks of ground truth annotations of the visual attention mechanism we train our network using a reinforcement learning algorithm with policy gradients experiment results on standard object detection benchmarks show that the proposed network consistently outperforms the baseline networks that does not model the attention mechanism less,Attentional Network for Visual Object Detection,"5 February, 2017"
228,Amir-massoud Farahmand,tackling large approximate dynamic programming or reinforcement learning problems requires methods that can exploit regularities or intrinsic structure of the problem in hand most current methods are geared towards exploiting the regularities of either the value function or the policy we introduce a general classificationbased approximate policy iteration capi framework which encompasses a large class of algorithms that can exploit regularities of both the value function and the policy space depending on what is advantageous this framework has two main components a generic value function estimator and a classifier that learns a policy based on the estimated value function we establish theoretical guarantees for the sample complexity of capistyle algorithms which allow the policy evaluation step to be performed by a wide variety of algorithms including temporaldifferencestyle methods and can handle nonparametric representations of policies our bounds on the estimation error of the performance loss are tighter than existing results we also illustrate this approach empirically on several problems including a large hiv control task less,Classification-based Approximate Policy Iteration: Experiments and Extended Discussions,"1 July, 2014"
229,Amir-massoud Farahmand,"Model-based reinforcement learning (MBRL) is a sample efficient technique to obtain control policies, yet unavoidable modeling errors often lead performance deterioration. The model in MBRL is often solely fitted to reconstruct dynamics, state observations in particular, while the impact of model error on the policy is not captured by the training objective. This leads to a mismatch between the intended goal of MBRL, enabling good policy and value learning, and the target of the loss function employed in practice, future state prediction. Naive intuition would suggest that value-aware model learning would fix this problem and, indeed, several solutions to this objective mismatch problem have been proposed based on theoretical analysis. However, they tend to be inferior in practice to commonly used maximum likelihood (MLE) based approaches. In this paper we propose the Value-gradient weighted Model Learning (VaGraM), a novel method for value-aware model learning which improves the performance of MBRL in challenging settings, such as small model capacity and the presence of distracting state dimensions. We analyze both MLE and value-aware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning value-aware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting. We verify our analysis by showing that our loss function is able to achieve high returns on the Mujoco benchmark suite while being more robust than maximum likelihood based approaches.",Value Gradient weighted Model-Based Reinforcement Learning,2022/4
230,Amir-massoud Farahmand,"The advancement of dynamics models enables model-based planning in complex environments. Existing dynamics models commonly study image-based games with fully observable states. Generalizing these models to Text-Based Games (TBGs), which commonly describe the partially observable states with noisy text observations, is challenging. In this work, we propose an Object-Oriented Text Dynamics (OOTD) model that enables planning algorithms to solve decision-making problems in text domains. OOTD predicts a memory graph that dynamically remembers the history of object observations and filters object-irrelevant information. To facilitate the robustness of dynamics, our OOTD model identifies the objects influenced by input actions and predicts the belief of object states with independently parameterized transition layers. We develop variational objectives under the object-supervised and self-supervised settings to model the stochasticity of predicted dynamics. Empirical results show OOTD-based planner significantly outperforms model-free baselines in terms of sample efficiency and running scores.",Learning Object-Oriented Dynamics for Planning from Text,2022/4
231,Amir-massoud Farahmand,"Prioritized Experience Replay (ER) has been empirically shown to improve sample efficiency across many domains and attracted great attention; however, there is little theoretical understanding of why such prioritized sampling helps and its limitations. In this work, we take a deep look at the prioritized ER. In a supervised learning setting, we show the equivalence between the error-based prioritized sampling method for mean squared error and uniform sampling for cubic power loss. We then provide theoretical insight into why it improves convergence rate upon uniform sampling during early learning. Based on the insight, we further point out two limitations of the prioritized ER method: 1) outdated priorities and 2) insufficient coverage of the sample space. To mitigate the limitations, we propose our model-based stochastic gradient Langevin dynamics sampling method. We show that our method does provide states distributed close to an ideal prioritized sampling distribution estimated by the brute-force method, which does not suffer from the two limitations. We conduct experiments on both discrete and continuous control problems to show our approach's efficacy and examine the practical implication of our method in an autonomous driving application.",Understanding and Mitigating the Limitations of Prioritized Replay,2022-02-28 00:00:00
232,Amir-massoud Farahmand,"In many areas, such as the physical sciences, life sciences, and finance, control approaches are used to achieve a desired goal in complex dynamical systems governed by differential equations. In this work we formulate the problem of controlling stochastic partial differential equations (SPDE) as a reinforcement learning problem. We present a learning-based, distributed control approach for online control of a system of SPDEs with high dimensional state-action space using deep deterministic policy gradient method. We tested the performance of our method on the problem of controlling the stochastic Burgers' equation, describing a turbulent fluid flow in an infinitely large domain.",Deep Reinforcement Learning for Online Control of Stochastic Partial Differential Equations,2021-10-21 00:00:00
233,Amir-massoud Farahmand,"The convergence rate of Value Iteration (VI), a fundamental procedure in dynamic programming and reinforcement learning, for solving MDPs can be slow when the discount factor is close to one. We propose modifications to VI in order to potentially accelerate its convergence behaviour. The key insight is the realization that the evolution of the value function approximations  in the VI procedure can be seen as a dynamical system. This opens up the possibility of using techniques from\emph {control theory} to modify, and potentially accelerate, this dynamics. We present such modifications based on simple controllers, such as PD (Proportional-Derivative), PI (Proportional-Integral), and PID. We present the error dynamics of these variants of VI, and provably (for certain classes of MDPs) and empirically (for more general classes) show that the convergence rate can be significantly improved. We also propose a gain adaptation mechanism in order to automatically select the controller gains, and empirically show the effectiveness of this procedure.",PID accelerated value iteration algorithm,2021-07-01 00:00:00
234,Amir-massoud Farahmand,"These are lectures notes for a graduate-level Introduction to Reinforcement Learning (RL) course, taught at the Department of Computer Science, University of Toronto, in Spring 2021. The course is introductory in the sense that it does not assume prior exposure to reinforcement learning. It is not, however, focused on being a collection of algorithms or only providing high-level intuition. Instead, it tries to build the mathematical intuition behind many important ideas and concepts often encountered in RL. I prove many basic, or sometimes not so basic, results in RL. If the proof of some result is too complicated, I prove a simplified version of it. This course requires some level of mathematical maturity. These lectures notes are a work in progress. New chapters will be added each week during the course. The content may change dramatically in future revisions of the work. Sometimes I have not had a chance to do a close proofread of each chapter before posting them. I add a footnote at the beginning of each chapter showing what stage of maturity the chapter is. The version 0.05 is for the first full draft, version 0.1 is after its first proofread and possible revisions, and the versions below 0.05 are for incomplete chapters (which means that I have the content ready, but I haven’t typed it yet).",Lecture Notes on Reinforcement Learning,2021-04-08 00:00:00
235,Azadeh Farzan,we present tada live a concurrent separation logic for reasoning compositionally about the termination of blocking finegrained concurrent programs the crucial challenge is how to deal with abstract atomic blocking that is abstract atomic operations that have blocking behaviour arising from busywaiting patterns as found in for example finegrained spin locks our fundamental innovation is with the design of abstract specifications that capture this blocking behaviour as liveness assumptions on the environment we design a logic that can reason about the termination of clients which use such operations without breaking their abstraction boundaries and the correctness of the implementations of the operations with respect to their abstract specifications we introduce a novel semantic model using layered subjective obligations to express liveness invariants and a proof system that is sound with respect to the model the subtlety of our specifications and reasoning is illustrated using several case studies less,TaDA Live: Compositional Reasoning for Termination of Fine-grained Concurrent Programs,"29 November, 2021"
236,Azadeh Farzan,we propose an automated verification technique for hypersafety properties which express sets of valid interrelations between multiple finite runs of a program the key observation is that constructing a proof for a small representative set of the runs of the product program ie the product of the several copies of the program by itself called a reduction is sufficient to formally prove the hypersafety property about the program we propose an algorithm based on a counterexampleguided refinement loop that simultaneously searches for a reduction and a proof of the correctness for the reduction we demonstrate that our tool weaver is very effective in verifying a diverse array of hypersafety properties for a diverse class of input programs less,Reductions for Automated Hypersafety Verification,"22 May, 2019"
237,Azadeh Farzan,this paper focuses on automated synthesis of divideandconquer parallelism which is a common parallel programming skeleton supported by many crossplatform multithreaded libraries the challenges of producing manually or automatically a correct divideandconquer parallel program from a given sequential code are twofold assuming that individual worker threads execute a code identical to the sequential code the programmer has to provide the extra code for dividing the tasks and combining the computation results and sometimes the sequential code may not be usable as is and may need to be modified by the programmer we address both challenges in this paper we present an automated synthesis technique for the case where no modifications to the sequential code are required and we propose an algorithm for modifying the sequential code to make it suitable for parallelization when some modification is necessary the paper presents theoretical results for when this em modification is efficiently possible and experimental evaluation of the technique and the quality of the produced parallel programs less,Automated Synthesis of Divide and Conquer Parallelism,"28 January, 2017"
238,Azadeh Farzan,"We propose SE2GIS, a novel inductive recursion synthesis approach with the ability to both synthesize code and declare a problem unsolvable. SE2GIS combines a symbolic variant of counterexample-guided inductive synthesis (CEGIS) with a new dual inductive procedure, which focuses on proving a synthesis problem unsolvable rather than finding a solution for it. A vital component of this procedure is a new algorithm that produces a witness, a set of concrete assignments to relevant variables, as a proof that the synthesis instance is not solvable. Witnesses in the dual inductive procedure play the same role that solutions do in classic CEGIS; that is, they ensure progress. Given a reference function, invariants on the input recursive data types, and a target family of recursive functions, SE2GIS synthesizes an implementation in this family that is equivalent to the reference implementation, or declares the problem",Recursion synthesis with unrealizability witnesses,2022-06-13 00:00:00
239,Azadeh Farzan,"We present a systematic investigation and experimental evaluation of a large space of algorithms for the verification of concurrent programs. The algorithms are based on sequentialization. In the analysis of concurrent programs, the general idea of sequentialization is to select a subset of interleavings, represent this subset as a sequential program, and apply a generic analysis for sequential programs. For the purpose of verification, the sequentialization has to be sound (meaning that the proof for the sequential program entails the correctness of the concurrent program). We use the concept of a preference order to define which interleavings the sequentialization is to select ("" the most preferred ones""). A verification algorithm based on sound sequentialization that is parametrized in a preference order allows us to directly evaluate the impact of the selection of the subset of interleavings on the performance of th",Sound sequentialization for concurrent program verification,2022-06-13 00:00:00
240,Azadeh Farzan,"Ultimate GemCutter verifies concurrent programs using the CEGAR paradigm, by generalizing from spurious counterexample traces to larger sets of correct traces. We integrate classical CEGAR generalization with orthogonal generalization across interleavings. Thereby, we are able to prove correctness of programs otherwise out-of-reach for interpolation-based verification. The competition results show significant advantages over other concurrency approaches in the Ultimate family.",and the Axes of Generalization,2022
241,Azadeh Farzan,"We present TaDA Live, a concurrent separation logic for reasoning compositionally about the termination of blocking fine-grained concurrent programs. The crucial challenge is how to deal with abstract atomic blocking: that is, abstract atomic operations that have blocking behaviour arising from busy-waiting patterns as found in, for example, fine-grained spin locks. Our fundamental innovation is with the design of abstract specifications that capture this blocking behaviour as liveness assumptions on the environment. We design a logic that can reason about the termination of clients that use such operations without breaking their abstraction boundaries, and the correctness of the implementations of the operations with respect to their abstract specifications. We introduce a novel semantic model using layered subjective obligations to express liveness invariants and a proof system that is sound with respect to the ",TaDA Live: Compositional reasoning for termination of fine-grained concurrent programs,2021-11-10 00:00:00
242,Azadeh Farzan,"Quantifier bounding is a standard approach in inductive program synthesis in dealing with unbounded domains. In this paper, we propose one such bounding method for the synthesis of recursive functions over recursive input data types. The synthesis problem is specified by an input reference (recursive) function and a recursion skeleton. The goal is to synthesize a recursive function equivalent to the input function whose recursion strategy is specified by the recursion skeleton. In this context, we illustrate that it is possible to selectively bound a subset of the (recursively typed) parameters, each by a suitable bound. The choices are guided by counterexamples. The evaluation of our strategy on a broad set of benchmarks shows that it succeeds in efficiently synthesizing non-trivial recursive functions where standard across-the-board bounding would fail.",Counterexample-Guided Partial Bounding for Recursive Function Synthesis,2021-07-20 00:00:00
243,Azadeh Farzan,"We propose a fully automated method that takes as input an iterative or recursive reference implementation and produces divide-and-conquer implementations that are functionally equivalent to the input. Three interdependent components have to be synthesized: a function that divides the original problem instance, a function that solves each sub-instance, and a function that combines the results of sub-computations. We propose a methodology that splits the synthesis problem into three successive phases, each with a substantially reduced state space compared to the original monolithic task, and therefore substantially more tractable. Our methodology is implemented as an addition to the existing synthesis tool Parsynt, and we demonstrate the efficacy of it by synthesizing highly nontrivial divide-and-conquer implementations of a set of benchmarks fully automatically.",Phased synthesis of divide and conquer programs,2021-06-19 00:00:00
244,Azadeh Farzan,"Let Sc be a type that stands for any scalar type used in typical programming languages, such as int and bool, whenever the specific type is not important in the context. Scalars are assumed to be of constant size, and conversely, any constant-size representable data type is assumed to be scalar. Consequently, all operations on scalars are assumed to have constant time complexity. Type S defines the set of all sequences of elements of type Sc. The concatenation operator•: S× S→ S defined over sequences is associative. The sequence type stands in for arrays, lists, or any collection data type that admits a linear iterator and an associative composition operator.",From Iterative Implementations to Single-pass Functions,2021
245,Azadeh Farzan,"Linearizability is the de facto correctness criterion for concurrent data type implementations. Violation of linearizability is witnessed by an error trace in which the outputs of individual operations do not match those of a sequential execution of the same operations. Extensive work has been done in discovering linearizability violations, but little work has been done in trying to provide useful hints to the programmer when a violation is discovered by a tester tool. In this paper, we propose an approach that identifies the root causes of linearizability errors in the form of code blocks whose atomicity is required to restore linearizability. The key insight of this paper is that the problem can be reduced to a simpler algorithmic problem of identifying minimal root causes of conflict serializability violation in an error trace combined with a heuristic for identifying which of these are more likely to be the true root cause of non",Root Causing Linearizability Violations,2020-07-21 00:00:00
246,Azadeh Farzan,"Program reductions are used widely to simplify reasoning about the correctness of concurrent and distributed programs. In this paper, we propose a general approach to proof simplification of concurrent programs based on exploring generic classes of reductions. We introduce two classes of sound program reductions, study their theoretical properties, show how they can be effectively used in algorithmic verification, and demonstrate that they are very effective in producing proofs of a diverse class of programs without targeting specific syntactic properties of these programs. The most novel contribution of this paper is the introduction of the concept of context in the definition of program reductions. We demonstrate how commutativity of program steps in some program contexts can be used to define a generic class of sound reductions which can be used to automatically produce proofs for programs whose complete",Reductions for safety proofs,2019-12-20 00:00:00
247,Azadeh Farzan,"Program reductions are used widely to simplify reasoning about the correctness of concurrent and distributed programs. In this paper, we propose a general approach to proof simplification of concurrent programs based on exploring generic classes of reductions. We introduce two classes of sound program reductions, study their theoretical properties, show how they can be effectively used in algorithmic verification, and demonstrate that they are very effective in producing proofs of a diverse class of programs without targeting specific syntactic properties of these programs. The most novel contribution of this paper is the introduction of the concept of context in the definition of program reductions. We demonstrate how commutativity of program steps in some program contexts can be used to define a generic class of sound reductions which can be used to automatically produce proofs for programs whose complete Floyd-Hoare style proofs are theoretically beyond the reach of automated verification technology of today.",Reductions for Safety Proofs (Extended Version),2019-10-31 00:00:00
248,Azadeh Farzan,this paper presents a new method for automatically generating numerical invariants for imperative programs given a program our procedure computes a binary inputoutput relation on program states which overapproximates the behaviour of the program it is compositional in the sense that it operates by decomposing the program into parts computing an abstract meaning of each part and then composing the meanings our method for approximating loop behaviour is based on first approximating the meaning of the loop body extracting recurrence relations from that approximation and then using the closed forms to approximate the loop our experiments demonstrate that on verification tasks our method is competitive with leading invariant generation and verification tools less,Compositional Invariant Generation via Linear Recurrence Analysis,"31 January, 2015"
249,Sanja Fidler,existing transformerbased image backbones typically propagate feature information in one direction from lower to higherlevels this may not be ideal since the localization ability to delineate accurate object boundaries is most prominent in the lower highresolution feature maps while the semantics that can disambiguate image signals belonging to one object vs another typically emerges in a higher level of processing we present hierarchical interlevel attention hila an attentionbased method that captures bottomup and topdown updates between features of different levels hila extends hierarchical vision transformer architectures by adding local connections between features of higher and lower levels to the backbone encoder in each iteration we construct a hierarchy by having higherlevel features compete for assignments to update lowerlevel features belonging to them iteratively resolving objectpart relationships these improved lowerlevel features are then used to reupdate the higherlevel features hila can be integrated into the majority of hierarchical architectures without requiring any changes to the base model we add hila into segformer and the swin transformer and show notable improvements in accuracy in semantic segmentation with fewer parameters and flops project website and code httpswwwcstorontoedugaryleunghila less,Improving Semantic Segmentation in Transformers using Hierarchical Inter-Level Attention,"5 July, 2022"
250,Sanja Fidler,absence of largescale labeled data in the practitioners target domain can be a bottleneck to applying machine learning algorithms in practice transfer learning is a popular strategy for leveraging additional data to improve the downstream performance but finding the most relevant data to transfer from can be challenging neural data server nds a search engine that recommends relevant data for a given downstream task has been previously proposed to address this problem nds uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task thus the computational cost to each user grows with the number of sources to address these issues we propose scalable neural data server snds a largescale search engine that can theoretically index thousands of datasets to serve relevant ml data to end users snds trains the mixture of experts on intermediary datasets during initialization and represents both data sources and downstream tasks by their proximity to the intermediary datasets as such computational cost incurred by snds users remains fixed as new datasets are added to the server we validate snds on a plethora of real world tasks and find that data recommended by snds improves downstream task performance over baselines we also demonstrate the scalability of snds by showing its ability to select relevant data for transfer outside of the natural image setting less,Scalable Neural Data Server: A Data Recommender for Transfer Learning,"19 June, 2022"
251,Sanja Fidler,neural approximations of scalar and vector fields such as signed distance functions and radiance fields have emerged as accurate highquality representations stateoftheart results are obtained by conditioning a neural approximation with a lookup from trainable feature grids that take on part of the learning task and allow for smaller more efficient neural networks unfortunately these feature grids usually come at the cost of significantly increased memory consumption compared to standalone neural network models we present a dictionary method for compressing such feature grids reducing their memory consumption by up to x and permitting a multiresolution representation which can be useful for outofcore streaming we formulate the dictionary optimization as a vectorquantized autodecoder problem which lets us learn endtoend discrete neural representations in a space where no direct supervision is available and with dynamic topology and structure our source code will be available at httpsgithubcomnvtlabsvqad less,Variable Bitrate Neural Fields,"15 June, 2022"
252,Sanja Fidler,neural implicit fields have recently emerged as a useful representation for d shapes these fields are commonly represented as neural networks which map latent descriptors and d coordinates to implicit function values the latent descriptor of a neural field acts as a deformation handle for the d shape it represents thus smoothness with respect to this descriptor is paramount for performing shapeediting operations in this work we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the fields lipschitz constant compared with prior lipschitz regularized networks ours is computationally fast can be implemented in four lines of code and requires minimal hyperparameter tuning for geometric applications we demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from d point clouds showing both qualitative and quantitative improvements over existing stateoftheart and nonregularized baselines less,Learning Smooth Neural Functions via Lipschitz Regularization,"10 May, 2022"
253,Sanja Fidler,modern computer vision applications rely on learningbased perception modules parameterized with neural networks for tasks like object detection these modules frequently have low expected error overall but high error on atypical groups of data due to biases inherent in the training process in building autonomous vehicles av this problem is an especially important challenge because their perception modules are crucial to the overall system performance after identifying failures in av a human team will comb through the associated data to group perception failures that share common causes more data from these groups is then collected and annotated before retraining the model to fix the issue in other words error groups are found and addressed in hindsight our main contribution is a pseudoautomatic method to discover such groups in foresight by performing causal interventions on simulated scenes to keep our interventions on the data manifold we utilize masked language models we verify that the prioritized groups found via intervention are challenging for the object detector and show that retraining with data collected from these groups helps inordinately compared to adding more iid data we also plan to release software to run interventions in simulated scenes which we hope will benefit the causality community less,Causal Scene BERT: Improving object detection by searching for challenging groups of data,"21 April, 2022"
254,Sanja Fidler,in this paper we address the problem of texture representation for d shapes for the challenging and underexplored tasks of texture transfer and synthesis previous works either apply spherical texture maps which may lead to large distortions or use continuous texture fields that yield smooth outputs lacking details we argue that the traditional way of representing textures with images and linking them to a d mesh via uv mapping is more desirable since synthesizing d images is a wellstudied problem we propose auvnet which learns to embed d surfaces into a d aligned uv space by mapping the corresponding semantic parts of different d shapes to the same location in the uv space as a result textures are aligned across objects and can thus be easily synthesized by generative models of images texture alignment is learned in an unsupervised manner by a simple yet effective texture alignment module taking inspiration from traditional works on linear subspace learning the learned uv mapping and aligned texture representations enable a variety of applications including texture transfer texture synthesis and textured single view d reconstruction we conduct experiments on multiple datasets to demonstrate the effectiveness of our method project page httpsnvtlabsgithubioauvnet less,AUV-Net: Learning Aligned UV Maps for Texture Transfer and Synthesis,"6 April, 2022"
255,Sanja Fidler,active learning is the process of training a model with limited labeled data by selecting a core subset of an unlabeled data pool to label the large scale of data sets used in deep learning forces most sample selection strategies to employ efficient heuristics this paper introduces an integer optimization problem for selecting a core set that minimizes the discrete wasserstein distance from the unlabeled pool we demonstrate that this problem can be tractably solved with a generalized benders decomposition algorithm our strategy uses highquality latent features that can be obtained by unsupervised learning on the unlabeled pool numerical results on several data sets show that our optimization approach is competitive with baselines and particularly outperforms them in the low budget regime where less than one percent of the data set is labeled less,Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach,"5 March, 2022"
256,Sanja Fidler,standard federated learning fl techniques are limited to clients with identical network architectures this restricts potential usecases like crossplatform training or interorganizational collaboration when both data privacy and architectural proprietary are required we propose a new fl framework that accommodates heterogeneous client architecture by adopting a graph hypernetwork for parameter sharing a property of the graph hyper network is that it can adapt to various computational graphs thereby allowing meaningful parameter sharing across models unlike existing solutions our framework does not limit the clients to share the same architecture type makes no use of external data and does not require clients to disclose their model architecture compared with distillationbased and nongraph hypernetwork baselines our method performs notably better on standard benchmarks we additionally show encouraging generalization performance to unseen architectures less,Federated Learning with Heterogeneous Architectures using Graph HyperNetworks,"20 January, 2022"
257,Sanja Fidler,the task of shape space learning involves mapping a train set of shapes to and from a latent representation space with good generalization properties often realworld collections of shapes have symmetries which can be defined as transformations that do not change the essence of the shape a natural way to incorporate symmetries in shape space learning is to ask that the mapping to the shape space encoder and mapping from the shape space decoder are equivariant to the relevant symmetries in this paper we present a framework for incorporating equivariance in encoders and decoders by introducing two contributions i adapting the recent frame averaging fa framework for building generic efficient and maximally expressive equivariant autoencoders and ii constructing autoencoders equivariant to piecewise euclidean motions applied to different parts of the shape to the best of our knowledge this is the first fully piecewise euclidean equivariant autoencoder construction training our framework is simple it uses standard reconstruction losses and does not require the introduction of new losses our architectures are built of standard backbone architectures with the appropriate frame averaging to make them equivariant testing our framework on both rigid shapes dataset using implicit neural representations and articulated shape datasets using meshbased neural networks show stateoftheart generalization to unseen test shapes improving relevant baselines by a large margin in particular our method demonstrates significant improvement in generalizing to unseen articulated poses less,Frame Averaging for Equivariant Shape Space Learning,"3 December, 2021"
258,Sanja Fidler,although machine learning models trained on massive data have led to breakthroughs in several areas their deployment in privacysensitive domains remains limited due to restricted access to data generative models trained with privacy constraints on private data can sidestep this challenge providing indirect access to private data instead we propose dpsinkhorn a novel optimal transportbased generative method for learning data distributions from private data with differential privacy dpsinkhorn minimizes the sinkhorn divergence a computationally efficient approximation to the exact optimal transport distance between the model and data in a differentially private manner and uses a novel technique for controlling the biasvariance tradeoff of gradient estimates unlike existing approaches for training differentially private generative models which are mostly based on generative adversarial networks we do not rely on adversarial objectives which are notoriously difficult to optimize especially in the presence of noise imposed by privacy constraints hence dpsinkhorn is easy to train and deploy experimentally we improve upon the stateoftheart on multiple image modeling benchmarks and show differentially private synthesis of informative rgb images project pagehttpsnvtlabsgithubiodpsinkhorn less,Don't Generate Me: Training Differentially Private Generative Models with Sinkhorn Divergence,"29 November, 2021"
259,Sanja Fidler,autonomous driving relies on a huge volume of realworld data to be labeled to high precision alternative solutions seek to exploit driving simulators that can generate large amounts of labeled data with a plethora of content variations however the domain gap between the synthetic and real data remains raising the following important question what are the best ways to utilize a selfdriving simulator for perception tasks in this work we build on top of recent advances in domainadaptation theory and from this perspective propose ways to minimize the reality gap we primarily focus on the use of labels in the synthetic domain alone our approach introduces both a principled way to learn neuralinvariant representations and a theoretically inspired view on how to sample the data from the simulator our method is easy to implement in practice as it is agnostic of the network architecture and the choice of the simulator we showcase our approach on the birdseyeview vehicle segmentation task with multisensor data cameras lidar using an opensource simulator carla and evaluate the entire framework on a realworld dataset nuscenes last but not least we show what types of variations eg weather conditions number of assets map design and color diversity matter to perception networks when trained with driving simulators and which ones can be compensated for with our domain adaptation technique less,Towards Optimal Strategies for Training Self-Driving Perception Models in Simulation,"15 November, 2021"
260,Sanja Fidler,generative adversarial networks gans have recently found applications in image editing however most gan based image editing methods often require large scale datasets with semantic segmentation annotations for training only provide high level control or merely interpolate between different images here we propose editgan a novel method for high quality high precision semantic image editing allowing users to edit images by modifying their highly detailed part segmentation masks eg drawing a new mask for the headlight of a car editgan builds on a gan framework that jointly models images and their semantic segmentations requiring only a handful of labeled examples making it a scalable tool for editing specifically we embed an image into the gan latent space and perform conditional latent code optimization according to the segmentation edit which effectively also modifies the image to amortize optimization we find editing vectors in latent space that realize the edits the framework allows us to learn an arbitrary number of editing vectors which can then be directly applied on other images at interactive rates we experimentally show that editgan can manipulate images with an unprecedented level of detail and freedom while preserving full image qualitywe can also easily combine multiple edits and perform plausible edits beyond editgan training data we demonstrate editgan on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks less,EditGAN: High-Precision Semantic Image Editing,"4 November, 2021"
261,Sanja Fidler,in this work we address the problem of jointly estimating albedo normals depth and d spatiallyvarying lighting from a single image most existing methods formulate the task as imagetoimage translation ignoring the d properties of the scene however indoor scenes contain complex d light transport where a d representation is insufficient in this paper we propose a unified learningbased inverse rendering framework that formulates d spatiallyvarying lighting inspired by classic volume rendering techniques we propose a novel volumetric spherical gaussian representation for lighting which parameterizes the exitant radiance of the d scene surfaces on a voxel grid we design a physics based differentiable renderer that utilizes our d lighting representation and formulates the energyconserving image formation process that enables joint training of all intrinsic properties with the rerendering constraint our model ensures physically correct predictions and avoids the need for groundtruth hdr lighting which is not easily accessible experiments show that our method outperforms prior works both quantitatively and qualitatively and is capable of producing photorealistic results for ar applications such as virtual object insertion even for highly specular objects less,Learning Indoor Inverse Rendering with 3D Spatially-Varying Lighting,"20 October, 2021"
262,Sanja Fidler,the ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input unlocks many applications from better interactive d tools to data synthesis for training and simulation in this paper we present atiss a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments given only the room type and its floor plan in contrast to prior work which poses scene synthesis as sequence generation our model generates rooms as unordered sets of objects we argue that this formulation is more natural as it makes atiss generally useful beyond fully automatic room layout synthesis for example the same trained model can be used in interactive applications for general scene completion partial room rearrangement with any objects specified by the user as well as object suggestions for any partial room to enable this our model leverages the permutation equivariance of the transformer when conditioning on the partial scene and is trained to be permutationinvariant across object orderings our model is trained endtoend as an autoregressive generative model using only labeled d bounding boxes as supervision evaluations on four room types in the dfront dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods in addition it has fewer parameters is simpler to implement and train and runs up to times faster than existing methods less,ATISS: Autoregressive Transformers for Indoor Scene Synthesis,"7 October, 2021"
263,Sanja Fidler,we propose a method to create plausible geometric and texture style variations of d objects in the quest to democratize d content creation given a pair of textured source and target objects our method predicts a partaware affine transformation field that naturally warps the source shape to imitate the overall geometric style of the target in addition the texture style of the target is transferred to the warped source object with the help of a multiview differentiable renderer our model dstylenet is composed of two subnetworks trained in two stages first the geometric style network is trained on a large set of untextured d shapes second we jointly optimize our geometric style network and a pretrained image style transfer network with losses defined over both the geometry and the rendering of the result given a small set of highquality textured objects our method can create many novel stylized shapes resulting in effortless d content creation and styleware data augmentation we showcase our approach qualitatively on d content stylization and provide user studies to validate the quality of our results in addition our method can serve as a valuable tool to create d data augmentations for computer vision tasks extensive quantitative analysis shows that dstylenet outperforms alternative data augmentation techniques for the downstream task of singleimage d reconstruction less,3DStyleNet: Creating 3D Shapes with Geometric and Texture Style Variations,"29 August, 2021"
264,Sanja Fidler,in this paper we present a nonparametric structured latent variable model for image generation called npdraw which sequentially draws on a latent canvas in a partbypart fashion and then decodes the image from the canvas our key contributions are as follows we propose a nonparametric prior distribution over the appearance of image parts so that the latent variable whattodraw per step becomes a categorical random variable this improves the expressiveness and greatly eases the learning compared to gaussians used in the literature we model the sequential dependency structure of parts via a transformer which is more powerful and easier to train compared to rnns used in the literature we propose an effective heuristic parsing algorithm to pretrain the prior experiments on mnist omniglot cifar and celeba show that our method significantly outperforms previous structured image models like draw and air and is competitive to other generic generative models moreover we show that our models inherent compositionality and interpretability bring significant benefits in the lowdata learning regime and latent space editing code is available at httpsgithubcomzengxhnpdraw less,NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image Generation,"4 July, 2021"
265,Sanja Fidler,in this paper we introduce watchandhelp wah a challenge for testing social intelligence in agents in wah an ai agent needs to help a humanlike agent perform a complex household task efficiently to succeed the ai agent needs to i understand the underlying goal of the task by watching a single demonstration of the humanlike agent performing the same task social perception and ii coordinate with the humanlike agent to solve the task in an unseen environment as fast as possible humanai collaboration for this challenge we build virtualhomesocial a multiagent household environment and provide a benchmark including both planning and learning based baselines we evaluate the performance of ai agents with the humanlike agent as well as with real humans using objective metrics and subjective user ratings experimental results demonstrate that the proposed challenge and virtual environment enable a systematic evaluation on the important aspects of machine social intelligence at scale less,Watch-And-Help: A Challenge for Social Perception and Human-AI Collaboration,"3 May, 2021"
266,Sanja Fidler,data is the engine of modern computer vision which necessitates collecting largescale datasets this is expensive and guaranteeing the quality of the labels is a major challenge in this paper we investigate efficient annotation strategies for collecting multiclass classification labels for a large collection of images while methods that exploit learnt models for labeling exist a surprisingly prevalent approach is to query humans for a fixed number of labels per datum and aggregate them which is expensive building on prior work on online joint probabilistic modeling of human annotations and machinegenerated beliefs we propose modifications and best practices aimed at minimizing human labeling effort specifically we make use of advances in selfsupervised learning view annotation as a semisupervised learning problem identify and mitigate pitfalls and ablate several key design choices to propose effective guidelines for labeling our analysis is done in a more realistic simulation that involves querying human labelers which uncovers issues with evaluation using existing worker simulation methods simulated experiments on a k image subset of the imagenet show that it can be annotated to top accuracy with annotations per image on average a x and x improvement over prior work and manual annotation respectively project page httpsfidlerlabgithubioefficientannotationcookbook less,Towards Good Practices for Efficiently Annotating Large-Scale Image Classification Datasets,"26 April, 2021"
267,Sanja Fidler,we introduce datasetgan an automatic procedure to generate massive datasets of highquality semantically segmented images requiring minimal human effort current deep networks are extremely datahungry benefiting from training on largescale datasets which are time consuming to annotate our method relies on the power of recent gans to generate realistic images we show how the gan latent code can be decoded to produce a semantic segmentation of the image training the decoder only needs a few labeled examples to generalize to the rest of the latent space resulting in an infinite annotated dataset generator these generated datasets can then be used for training any computer vision architecture just as real datasets are as only a few images need to be manually segmented it becomes possible to annotate images in extreme detail and generate datasets with rich object and part segmentations to showcase the power of our approach we generated datasets for image segmentation tasks which include pixellevel labels for human face parts and car parts our approach outperforms all semisupervised baselines significantly and is on par with fully supervised methods which in some cases require as much as x more annotated data as our method less,DatasetGAN: Efficient Labeled Data Factory with Minimal Human Effort,"19 April, 2021"
268,Sanja Fidler,we consider the problem of estimating an objects physical properties such as mass friction and elasticity directly from video sequences such a system identification problem is fundamentally illposed due to the loss of information during image formation current solutions require precise d labels which are laborintensive to gather and infeasible to create for many systems such as deformable solids or cloth we present gradsim a framework that overcomes the dependence on d supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation this novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them moreover our unified computation graph spanning from the dynamics and through the rendering process enables learning in challenging visuomotor control tasks without relying on statebased d supervision while obtaining performance competitive to or better than techniques that rely on precise d labels less,gradSim: Differentiable simulation for system identification and visuomotor control,"6 April, 2021"
269,Sanja Fidler,impressive progress in d shape extraction led to representations that can capture object geometries with high fidelity in parallel primitivebased methods seek to represent objects as semantically consistent part arrangements however due to the simplicity of existing primitive representations these methods fail to accurately reconstruct d shapes using a small number of primitivesparts we address the tradeoff between reconstruction quality and number of parts with neural parts a novel d primitive representation that defines primitives using an invertible neural network inn which implements homeomorphic mappings between a sphere and the target object the inn allows us to compute the inverse mapping of the homeomorphism which in turn enables the efficient computation of both the implicit surface function of a primitive and its mesh without any additional postprocessing our model learns to parse d objects into semantically consistent part arrangements without any partlevel supervision evaluations on shapenet dfaust and freihand demonstrate that our primitives can capture complex geometries and thus simultaneously achieve geometrically accurate as well as interpretable reconstructions using an order of magnitude fewer primitives than stateoftheart shape abstraction methods less,Neural Parts: Learning Expressive 3D Shape Abstractions with Invertible Neural Networks,"18 March, 2021"
270,Sanja Fidler,neural signed distance functions sdfs are emerging as an effective representation for d shapes stateoftheart methods typically encode the sdf with a large fixedsize neural network to approximate complex shapes with implicit surfaces rendering with these large networks is however computationally expensive since it requires many forward passes through the network for every pixel making these representations impractical for realtime graphics we introduce an efficient neural representation that for the first time enables realtime rendering of highfidelity neural sdfs while achieving stateoftheart geometry reconstruction quality we represent implicit surfaces using an octreebased feature volume which adaptively fits shapes with multiple discrete levels of detail lods and enables continuous lod with sdf interpolation we further develop an efficient algorithm to directly render our novel neural sdf representation in realtime by querying only the necessary lods with sparse octree traversal we show that our representation is orders of magnitude more efficient in terms of rendering speed compared to previous works furthermore it produces stateoftheart reconstruction quality for complex shapes under both d geometric and d imagespace metrics less,Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes,"26 January, 2021"
271,Sanja Fidler,d shape representations that accommodate learningbased d reconstruction are an open problem in machine learning and computer graphics previous work on neural d reconstruction demonstrated benefits but also limitations of point cloud voxel surface mesh and implicit function representations we introduce deformable tetrahedral meshes deftet as a particular parameterization that utilizes volumetric tetrahedral meshes for the reconstruction problem unlike existing volumetric approaches deftet optimizes for both vertex placement and occupancy and is differentiable with respect to standard d reconstruction loss functions it is thus simultaneously highprecision volumetric and amenable to learningbased neural architectures we show that it can represent arbitrary complex topology is both memory and computationally efficient and can produce highfidelity reconstructions with a significantly smaller grid size than alternative volumetric approaches the predicted surfaces are also inherently defined as tetrahedral meshes thus do not require postprocessing we demonstrate that deftet matches or exceeds both the quality of the previous best approaches and the performance of the fastest ones our approach obtains highquality tetrahedral meshes computed directly from noisy point clouds and is the first to showcase highquality d tetmesh results using only a single image as input our project webpage httpsnvtlabsgithubiodeftet less,Learning Deformable Tetrahedral Meshes for 3D Reconstruction,"23 November, 2020"
272,Sanja Fidler,labelling data is expensive and time consuming especially for domains such as medical imaging that contain volumetric imaging data and require expert knowledge exploiting a larger pool of labeled data available across multiple centers such as in federated learning has also seen limited success since current deep learning approaches do not generalize well to images acquired with scanners from different manufacturers we aim to address these problems in a common learningbased image simulation framework which we refer to as federated simulation we introduce a physicsdriven generative approach that consists of two learnable neural modules a module that synthesizes d cardiac shapes along with their materials and a ct simulator that renders these into realistic d ct volumes with annotations since the model of geometry and material is disentangled from the imaging sensor it can effectively be trained across multiple medical centers we show that our data synthesis framework improves the downstream segmentation performance on several datasets project page httpsnvtlabsgithubiofedsim less,Fed-Sim: Federated Simulation for Medical Imaging,"1 September, 2020"
273,Sanja Fidler,manually labeling video datasets for segmentation tasks is extremely time consuming in this paper we introduce scribblebox a novel interactive framework for annotating object instances with masks in videos in particular we split annotation into two steps annotating objects with tracked boxes and labeling masks inside these tracks we introduce automation and interaction in both steps box tracks are annotated efficiently by approximating the trajectory using a parametric curve with a small number of control points which the annotator can interactively correct our approach tolerates a modest amount of noise in the box placements thus typically only a few clicks are needed to annotate tracked boxes to a sufficient accuracy segmentation masks are corrected via scribbles which are efficiently propagated through time we show significant performance gains in annotation efficiency over past work we show that our scribblebox approach reaches jf on davis with clicks per box track and frames of scribble annotation less,ScribbleBox: Interactive Annotation Framework for Video Object Segmentation,"21 August, 2020"
274,David Fleet,recent progress in medical artificial intelligence ai has delivered systems that can reach clinical expert level performance however such systems tend to demonstrate suboptimal outofdistribution performance when evaluated in clinical settings different from the training environment a common mitigation strategy is to develop separate systems for each clinical setting using sitespecific data however this quickly becomes impractical as medical data is timeconsuming to acquire and expensive to annotate thus the problem of dataefficient generalization presents an ongoing difficulty for medical ai development although progress in representation learning shows promise their benefits have not been rigorously studied specifically for outofdistribution settings to meet these challenges we present remedis a unified representation learning strategy to improve robustness and dataefficiency of medical imaging ai remedis uses a generic combination of largescale supervised transfer learning with selfsupervised learning and requires little taskspecific customization we study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data remedis exhibits significantly improved indistribution performance with up to relative improvement in diagnostic accuracy over a strong supervised baseline more importantly our strategy leads to strong dataefficient generalization of medical imaging ai matching strong supervised baselines using between to of retraining data across tasks these results suggest that remedis can significantly accelerate the lifecycle of medical imaging ai development thereby presenting an important step forward for medical imaging ai to deliver broad impact less,Robust and Efficient Medical Imaging with Self-Supervision,"3 July, 2022"
275,David Fleet,generating temporally coherent high fidelity video is an important milestone in generative modeling research we make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results our model is a natural extension of the standard image diffusion architecture and it enables jointly training from image and video data which we find to reduce the variance of minibatch gradients and speed up optimization to generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods we present the first results on a large textconditioned video generation task as well as stateoftheart results on established benchmarks for video prediction and unconditional video generation supplementary material is available at httpsvideodiffusiongithubio less,Video Diffusion Models,"22 June, 2022"
276,David Fleet,coordinate networks like multiplicative filter networks mfns and bacon offer some control over the frequency spectrum used to represent continuous signals such as images or d volumes yet they are not readily applicable to problems for which coarsetofine estimation is required including various inverse problems in which coarsetofine optimization plays a key role in avoiding poor local minima we introduce a new coordinate network architecture and training scheme that enables coarsetofine optimization with finegrained control over the frequency support of learned reconstructions this is achieved with two key innovations first we incorporate skip connections so that structure at one scale is preserved when fitting finerscale structure second we propose a novel initialization scheme to provide control over the model frequency spectrum at each stage of optimization we demonstrate how these modifications enable multiscale optimization for coarsetofine fitting to natural images we then evaluate our model on synthetically generated datasets for the the problem of singleparticle cryoem reconstruction we learn high resolution multiscale structures on par with the stateofthe art less,Residual Multiplicative Filter Networks for Multiscale Reconstruction,"1 June, 2022"
277,David Fleet,this paper develops a unified framework for imagetoimage translation based on conditional diffusion models and evaluates this framework on four challenging imagetoimage translation tasks namely colorization inpainting uncropping and jpeg restoration our simple implementation of imagetoimage diffusion models outperforms strong gan and regression baselines on all tasks without taskspecific hyperparameter tuning architecture customization or any auxiliary loss or sophisticated new techniques needed we uncover the impact of an l vs l loss in the denoising diffusion objective on sample diversity and demonstrate the importance of selfattention in the neural architecture through empirical studies importantly we advocate a unified evaluation protocol based on imagenet with human evaluation and sample quality scores fid inception score classification accuracy of a pretrained resnet and perceptual distance against original images we expect this standardized evaluation protocol to play a role in advancing imagetoimage translation research finally we show that a generalist multitask diffusion model performs as well or better than taskspecific specialist counterparts check out httpsdiffusionpalettegithubio for an overview of the results less,Palette: Image-to-Image Diffusion Models,"3 May, 2022"
278,David Fleet,we present pixseq a simple and generic framework for object detection unlike existing approaches that explicitly integrate prior knowledge about the task we cast object detection as a language modeling task conditioned on the observed pixel inputs object descriptions eg bounding boxes and class labels are expressed as sequences of discrete tokens and we train a neural network to perceive the image and generate the desired sequence our approach is based mainly on the intuition that if a neural network knows about where and what the objects are we just need to teach it how to read them out beyond the use of taskspecific data augmentations our approach makes minimal assumptions about the task yet it achieves competitive results on the challenging coco dataset compared to highly specialized and well optimized detection algorithms less,Pix2seq: A Language Modeling Framework for Object Detection,"27 March, 2022"
279,David Fleet,this paper addresses the problem of optimal scheduling of an aggregated power profile during a coordinated discharging or charging operation by means of a heterogeneous fleet of storage devices subject to availability constraints devices have heterogeneous initial levels of energy power ratings and efficiency moreover the fleet operates without crosscharging of the units an explicit feedback policy is proposed to compute a feasible schedule whenever one exists and scalable design procedures to achieve maximum time to failure or minimal unserved energy in the case of unfeasible aggregated demand profiles finally a timedomain characterization of the set of feasible demand profiles using aggregate constraints is proposed suitable for optimization problems where the aggregate population behaviour is of interest less,On optimal coordinated dispatch for heterogeneous storage fleets with partial availability,"16 March, 2022"
280,David Fleet,data is the driving force of machine learning with the amount and quality of training data often being more important for the performance of a system than architecture and training details but collecting processing and annotating real data at scale is difficult expensive and frequently raises additional privacy fairness and legal concerns synthetic data is a powerful tool with the potential to address these shortcomings it is cheap supports rich groundtruth annotations offers full control over data and can circumvent or mitigate problems regarding bias privacy and licensing unfortunately software tools for effective data generation are less mature than those for architecture design and training which leads to fragmented generation efforts to address these problems we introduce kubric an opensource python framework that interfaces with pybullet and blender to generate photorealistic scenes with rich annotations and seamlessly scales to large jobs distributed over thousands of machines and generating tbs of data we demonstrate the effectiveness of kubric by presenting a series of different generated datasets for tasks ranging from studying d nerf models to optical flow estimation we release kubric the used assets all of the generation code as well as the rendered datasets for reuse and modification less,Kubric: A scalable dataset generator,"7 March, 2022"
281,David Fleet,emergency medical systems ems provide crucial prehospital care and transportation faster ems response time provides quicker prehospital care and thus increases survival rate we reduce response time by providing optimal ambulance stationing and routing decisions by solving two stage stochastic and robust linear programs although operational research on ambulance systems is decades old there is little opensource code and consistency in simulations we begin to bridge this gap by publishing openems in collaboration with the austintravis county ems atcems in texas an endtoend pipeline to optimize ambulance strategic decisions it includes data handling optimization and a calibrated simulation we hope this open source framework will foster future research with and for ems finally we provide a detailed case study on the city of austin texas we find that optimal stationing would increase response time by seconds further we design optimal strategies in the case where austin ems must permanently add or remove one ambulance from their fleet less,OpenEMS: an open-source Package for Two-Stage Stochastic and Robust Optimization for Ambulance Location and Routing with Applications to Austin-Travis County EMS Data,"26 January, 2022"
282,David Fleet,this paper presents the datadriven techniques and methodologies used to predict the remaining useful life rul of a fleet of aircraft engines that can suffer failures of diverse nature the solution presented is based on two deep convolutional neural networks dcnn stacked in two levels the first dcnn is used to extract a lowdimensional feature vector using the normalized raw data as input the second dcnn ingests a list of vectors taken from the former dcnn and estimates the rul model selection was carried out by means of bayesian optimization using a repeated random subsampling validation approach the proposed methodology was ranked in the third place of the phm conference data challenge less,A stacked deep convolutional neural network to predict the remaining useful life of a turbofan engine,"24 November, 2021"
283,David Fleet,future power grids will entail large fleets of storage devices capable of scheduling their chargingdischarging profiles so as to achieve lower peak demand and reduce energy bills by shifting absorption times in sync with the availability of renewable energy sources optimal management of such fleets entails large scale optimisation problems which are better dealt with in a hierarchical manner by clustering together individual devices into fleets leveraging on recent results characterizing the set of aggregate demand profiles of a heterogeneous fleet of charging or respectively discharging devices we propose a way to achieve optimality in a unit commitment problem by adopting a simplified formulation with a number of constraints for the fleet that scales linearly in the number of timeslots considered and is independent of the size of the fleet this is remarkable as it shows that under suitable conditions a heterogeneous fleet of any size can effectively be treated as a single storage unit less,Exact aggregate models for optimal management of heterogeneous fleets of storage devices,"3 November, 2021"
284,David Fleet,the vehicle fleet sizing positioning and routing problem with stochastic customers vfsprpsc consists on pairing strategic decisions of depot positioning and fleet sizing with operational vehicle routing decisions while taking into account the inherent uncertainty of demand we successfully solve the vfsprpsc with a methodology comprised of two main blocks i a scenario generation phase and ii a twostage stochastic program for the first block a set of scenarios is selected with a simulationbased approach that captures the behavior of the demand and allows us to come up with different solutions that could match different risk profiles the second block is comprised of a facility location and allocation model and a multi depot vehicle routing problem mdvrp assembled under a twostage stochastic program we propose several novel ideas within our methodology problem specific cuts that serve as an approximation of the expected second stage costs as a function of first stage decisions an activation paradigm that guides our main optimization procedure and a way of mapping feasible routes from one secondstage problem data into another among others we performed experiments for two cases the first case considers the expected value of the demand and the second case considers the right tail of the demand distribution seeking a conservative solution by using acceleration techniques we obtain solutions within to hours reasonable times considering the strategic nature of the decision for the expost evaluation we solve of the instances in less than minutes meaning that the methodology used to solve the mdvrp is well suited for daily operation less,"Vehicle Fleet Sizing, Positioning and Routing Problem with Stochastic Customers","20 September, 2021"
285,David Fleet,we demonstrate that the choice of optimizer neural network architecture and regularizer significantly affect the adversarial robustness of linear neural networks providing guarantees without the need for adversarial training to this end we revisit a known result linking maximally robust classifiers and minimum norm solutions and combine it with recent results on the implicit bias of optimizers first we show that under certain conditions it is possible to achieve both perfect standard accuracy and a certain degree of robustness simply by training an overparametrized model using the implicit bias of the optimization in that regime there is a direct relationship between the type of the optimizer and the attack to which the model is robust to the best of our knowledge this work is the first to study the impact of optimization methods such as sign gradient descent and proximal methods on adversarial robustness second we characterize the robustness of linear convolutional models showing that they resist attacks subject to a constraint on the fourierellinfty norm to illustrate these findings we design a novel fourierellinfty attack that finds adversarial examples with controllable frequencies we evaluate fourierellinfty robustness of adversariallytrained deep cifar models from the standard robustbench benchmark and visualize adversarial perturbations less,Bridging the Gap Between Adversarial Robustness and Optimization Bias,"7 June, 2021"
286,David Fleet,sales of new petrol and diesel passenger vehicles may not be permitted in the united kingdom uk post should this happen it is likely that vehicles presently powered by hydrocarbons will be progressively replaced by battery electric vehicles bevs this paper describes the use of mathematical modelling drawing on real time records of the uk electricity grid to investigate the likely performance of the grid when supplying power to a fleet of up to million bevs the model highlights the importance of understanding how the grid will cope when powering a bev fleet under conditions similar to those experienced during an extended wind lull during the rd week of january allowing a twoway flow of electricity between the bevs and the grid known as the vehicletogrid vg configuration turns out to be of key importance in minimising the need for additional gas turbine generation or energy storage during wind lulls this study has shown that with the use of vg it should be possible to provide power to about million bevs with the gas turbine capacity currently available without vg it is likely that the current capacity of the gas turbines and associated gas infrastructure might be overwhelmed by even a relatively small bev fleet since it is anticipated that of bev owners will be able to park the vehicles at their residences widespread vg will enable both the powering of residences when supply from the grid is constrained and the charging of bevs when supply is in excess the model shows that this configuration will maintain a constant load on the grid and avoid the use of either expensive alternative storage or hydrogen obtained by reforming methane there should be no insuperable problem in providing power to the of bev owners who do not have parking at their residences their power could come directly from the grid less,Predicting the Performance of a Future United Kingdom Grid and Wind Fleet When Providing Power to a Fleet of Battery Electric Vehicles,"23 December, 2020"
287,David Fleet,the classic dialaride problem darp aims at designing the minimumcost routing that accommodates a set of user requests under constraints at an operations planning level where users preferences and revenue management are often overlooked in this paper we present a mechanism for acceptingrejecting user requests in a demand responsive transportation drt context based on the representative utilities of alternative transportation modes we consider utilitymaximizing users and propose a mixedinteger programming formulation for a chance constrained darp ccdarp that captures users preferences in the long run via a logit model we further introduce classbased user groups and consider various pricing structures for drt services a customised local search based heuristic is developed to solve the proposed ccdarp we report numerical results for both darp benchmarking instances and a realistic case study based on new york city yellow taxi trip data computational experiments performed on benchmarking instances with up to nodes yield an average optimality gap of using the proposed local search heuristic the results obtained on the realistic case study reveal that a zonal fare structure is the best strategy in terms of optimising revenue and ridership the proposed ccdarp formulation provides a new decisionsupport tool to inform on revenue and fleet management for drt systems at a strategic planning level less,A chance-constrained dial-a-ride problem with utility-maximizing demand and multiple pricing structures,"17 November, 2020"
288,David Fleet,the cybernetic transportation systems cts is an urban mobility concept based on two ideas the car sharing and the automation of dedicated systems with doortodoor capabilities in the last decade many european projects have been developed in this context where some of the most important are cybercars cybercars cybermove cyberc and citymobil different companies have developed a first fleet of ctss in collaboration with research centers around europe asia and america considering these previous works the fp project citymobil is on progress since its goal is to solve some of the limitations found so far including the definition of the legal framework for autonomous vehicles on urban environment this work describes the different improvements adaptation and instrumentation of the cts prototypes involved in european cities results show tests in our facilities at inriarocquencourt france and the first showcase at len spain less,Description and Technical specification of Cybernetic Transportation Systems: an urban transportation concept,"15 August, 2020"
289,David Fleet,we present vehicle energy dataset ved a novel largescale dataset of fuel and energy data collected from personal cars in ann arbor michigan usa this open dataset captures gps trajectories of vehicles along with their timeseries data of fuel energy speed and auxiliary power usage a diverse fleet consisting of gasoline vehicles hevs and phevevs drove in realworld from nov to nov where the data were collected through onboard obdii loggers driving scenarios range from highways to trafficdense downtown area in various driving conditions and seasons in total ved accumulates approximately miles we discuss participant privacy protection and develop a method to deidentify personally identifiable information while preserving the quality of the data after the deidentification we conducted case studies on the dataset to investigate the impacts of factors known to affect fuel economy and identify energysaving opportunities that hybridelectric vehicles and ecodriving techniques can provide the case studies are supplemented with a number of examples to demonstrate how ved can be utilized for vehicle energy and behavior studies potential research opportunities include datadriven vehicle energy consumption modeling driver behavior modeling machine and deep learning calibration of traffic simulators optimal route choice modeling prediction of human driver behaviors and decision making of selfdriving cars we believe that ved can be an instrumental asset to the development of future automotive technologies the dataset can be accessed at httpsgithubcomgsohved less,"Vehicle Energy Dataset (VED), A Large-scale Dataset for Vehicle Energy Consumption Research","19 April, 2019"
290,David Fleet,we introduce a hierarchical architecture for video understanding that exploits the structure of real world actions by capturing targets at different levels of granularity we design the model such that it first learns simpler coarsegrained tasks and then moves on to learn more finegrained targets the model is trained with a joint loss on different granularity levels we demonstrate empirical results on the recent release of somethingsomething dataset which provides a hierarchy of targets namely coarsegrained action groups finegrained action categories and captions experiments suggest that models that exploit targets at different levels of granularity achieve better performance on all levels less,Hierarchical Video Understanding,"3 September, 2018"
291,Animesh Garg,the pipeline of current robotic pickandplace methods typically consists of several stages grasp pose detection finding inverse kinematic solutions for the detected poses planning a collisionfree trajectory and then executing the openloop trajectory to the grasp pose with a lowlevel tracking controller while these grasping methods have shown good performance on grasping static objects on a tabletop the problem of grasping dynamic objects in constrained environments remains an open problem we present neural motion fields a novel object representation which encodes both object point clouds and the relative task trajectories as an implicit value function parameterized by a neural network this objectcentric representation models a continuous distribution over the se space and allows us to perform grasping reactively by leveraging samplingbased mpc to optimize this value function less,Neural Motion Fields: Encoding Grasp Trajectories as Implicit Value Functions,"29 June, 2022"
292,Animesh Garg,traditional biological and pharmaceutical manufacturing plants are controlled by human workers or predefined thresholds modernized factories have advanced process control algorithms such as model predictive control mpc however there is little exploration of applying deep reinforcement learning to control manufacturing plants one of the reasons is the lack of high fidelity simulations and standard apis for benchmarking to bridge this gap we develop an easytouse library that includes five highfidelity simulation environments beerfmtenv reactorenv atropineenv pensimenv and mabenv which cover a wide range of manufacturing processes we build these environments on published dynamics models furthermore we benchmark online and offline modelbased and modelfree reinforcement learning algorithms for comparisons of followup research less,SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments,"17 June, 2022"
293,Animesh Garg,dexterous manipulation remains an open problem in robotics to coordinate efforts of the research community towards tackling this problem we propose a shared benchmark we designed and built robotic platforms that are hosted at mpi for intelligent systems and can be accessed remotely each platform consists of three robotic fingers that are capable of dexterous object manipulation users are able to control the platforms remotely by submitting code that is executed automatically akin to a computational cluster using this setup i we host robotics competitions where teams from anywhere in the world access our platforms to tackle challenging tasks ii we publish the datasets collected during these competitions consisting of hundreds of robot hours and iii we give researchers access to these platforms for their own projects less,Real Robot Challenge: A Robotics Competition in the Cloud,"10 June, 2022"
294,Animesh Garg,recent works in video prediction have mainly focused on passive forecasting and lowlevel actionconditional prediction which sidesteps the learning of interaction between agents and objects we introduce the task of semantic actionconditional video prediction which uses semantic action labels to describe those interactions and can be regarded as an inverse problem of action recognition the challenge of this new task primarily lies in how to effectively inform the model of semantic action information inspired by the idea of mixture of experts we embody each abstract label by a structured combination of various visual concept learners and propose a novel video prediction model modular action concept network mac our method is evaluated on two newly designed synthetic datasets clevrbuildingblocks and sapienkitchen and one realworld dataset called towercreation extensive experiments demonstrate that mac can correctly condition on given instructions and generate corresponding future frames without need of bounding boxes we further show that the trained model can make outofdistribution generalization be quickly adapted to new object categories and exploit its learnt features for object detection showing the progression towards higherlevel cognitive abilities more visualizations can be found at httpwwwpairtorontoedumac less,Modular Action Concept Grounding in Semantic Video Prediction,"26 April, 2022"
295,Animesh Garg,policy gradient methods have been frequently applied to problems in control and reinforcement learning with great success yet existing convergence analysis still relies on nonintuitive impractical and often opaque conditions in particular existing rates are achieved in limited settings under strict regularity conditions in this work we establish explicit convergence rates of policy gradient methods extending the convergence regime to weakly smooth policy classes with l integrable gradient we provide intuitive examples to illustrate the insight behind these new conditions notably our analysis also shows that convergence rates are achievable for both the standard policy gradient and the natural policy gradient algorithms under these assumptions lastly we provide performance guarantees for the converged policies less,Convergence and Optimality of Policy Gradient Methods in Weakly Smooth Settings,"7 April, 2022"
296,Animesh Garg,modelbased reinforcement learning mbrl is a sample efficient technique to obtain control policies yet unavoidable modeling errors often lead performance deterioration the model in mbrl is often solely fitted to reconstruct dynamics state observations in particular while the impact of model error on the policy is not captured by the training objective this leads to a mismatch between the intended goal of mbrl enabling good policy and value learning and the target of the loss function employed in practice future state prediction naive intuition would suggest that valueaware model learning would fix this problem and indeed several solutions to this objective mismatch problem have been proposed based on theoretical analysis however they tend to be inferior in practice to commonly used maximum likelihood mle based approaches in this paper we propose the valuegradient weighted model learning vagram a novel method for valueaware model learning which improves the performance of mbrl in challenging settings such as small model capacity and the presence of distracting state dimensions we analyze both mle and valueaware approaches and demonstrate how they fail to account for exploration and the behavior of function approximation when learning valueaware models and highlight the additional goals that must be met to stabilize optimization in the deep learning setting we verify our analysis by showing that our loss function is able to achieve high returns on the mujoco benchmark suite while being more robust than maximum likelihood based approaches less,Value Gradient weighted Model-Based Reinforcement Learning,"4 April, 2022"
297,Animesh Garg,robotic cutting of soft materials is critical for applications such as food processing household automation and surgical manipulation as in other areas of robotics simulators can facilitate controller verification policy learning and dataset generation moreover differentiable simulators can enable gradientbased optimization which is invaluable for calibrating simulation parameters and optimizing controllers in this work we present disect the first differentiable simulator for cutting soft materials the simulator augments the finite element method fem with a continuous contact model based on signed distance fields sdf as well as a continuous damage model that inserts springs on opposite sides of the cutting plane and allows them to weaken until zero stiffness enabling crack formation through various experiments we evaluate the performance of the simulator we first show that the simulator can be calibrated to match resultant forces and deformation fields from a stateoftheart commercial solver and realworld cutting datasets with generality across cutting velocities and object instances we then show that bayesian inference can be performed efficiently by leveraging the differentiability of the simulator estimating posteriors over hundreds of parameters in a fraction of the time of derivativefree methods next we illustrate that control parameters in the simulation can be optimized to minimize cutting forces via lateral slicing motions finally we conduct experiments on a real robot arm equipped with a slicing knife to infer simulation parameters from force measurements by optimizing the slicing motion of the knife we show on fruit cutting scenarios that the average knife force can be reduced by more than compared to a vertical cutting motion we publish code and additional materials on our project website at httpsdiffcuttingsimgithubio less,DiSECt: A Differentiable Simulator for Parameter Inference and Control in Robotic Cutting,"19 March, 2022"
298,Animesh Garg,in this work we study the problem of how to leverage instructional videos to facilitate the understanding of human decisionmaking processes focusing on training a model with the ability to plan a goaldirected procedure from realworld videos learning structured and plannable state and action spaces directly from unstructured videos is the key technical challenge of our task there are two problems first the appearance gap between the training and validation datasets could be large for unstructured videos second these gaps lead to decision errors that compound over the steps we address these limitations with planning transformer plate which has the advantage of circumventing the compounding prediction errors that occur with singlestep models during long modelbased rollouts our method simultaneously learns the latent state and action information of assigned tasks and the representations of the decisionmaking process from human demonstrations experiments conducted on realworld instructional videos and an interactive environment show that our method can achieve a better performance in reaching the indicated goal than previous algorithms we also validated the possibility of applying procedural tasks on a ur platform we make our code publicly available and support academic research purposes less,PlaTe: Visually-Grounded Planning with Transformers in Procedural Tasks,"2 March, 2022"
299,Animesh Garg,modelfree reinforcement learning rl for legged locomotion commonly relies on a physics simulator that can accurately predict the behaviors of every degree of freedom of the robot in contrast approximate reducedorder models are commonly used for many model predictive control strategies in this work we abandon the conventional use of highfidelity dynamics models in rl and we instead seek to understand what can be achieved when using rl with a much simpler centroidal model when applied to quadrupedal locomotion we show that rlbased control of the accelerations of a centroidal model is surprisingly effective when combined with a quadratic program to realize the commanded actions via ground contact forces it allows for a simple reward structure reduced computational costs and robust simtoreal transfer we show the generality of the method by demonstrating flatterrain gaits steppingstone locomotion twolegged inplace balance balance beam locomotion and direct simtoreal transfer less,GLiDE: Generalizable Quadrupedal Locomotion in Diverse Environments with a Centroidal Model,"15 February, 2022"
300,Animesh Garg,natural language provides an accessible and expressive interface to specify longterm tasks for robotic agents however nonexperts are likely to specify such tasks with highlevel instructions which abstract over specific robot actions through several layers of abstraction we propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations we propose a persistent spatial semantic representation method and show how it enables building an agent that performs hierarchical reasoning to effectively execute longterm tasks we evaluate our approach on the alfred benchmark and achieve stateoftheart results despite completely avoiding the commonly used stepbystep instructions less,A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution,"28 November, 2021"
301,Animesh Garg,auditing trained deep learning dl models prior to deployment is vital for preventing unintended consequences one of the biggest challenges in auditing is the lack of humaninterpretable specifications for the dl models that are directly useful to the auditor we address this challenge through a sequence of semanticallyaligned unit tests where each unit test verifies whether a predefined specification eg accuracy over is satisfied with respect to controlled and semantically aligned variations in the input space eg in face recognition the angle relative to the camera we enable such unit tests through variations in a semanticallyinterpretable latent space of a generative model further we conduct certified training for the dl model through a shared latent space representation with the generative model with evaluations on four different datasets covering images of chest xrays human faces imagenet classes and towers we show how auditai allows us to obtain controlled variations for certified training thus our framework auditai bridges the gap between semanticallyaligned formal verification and scalability a blog post accompanying the paper is at this link httpsdevelopernvidiacomblognvidiaresearchauditingaimodelsforverifieddeploymentundersemanticspecifications less,Auditing AI models for Verified Deployment under Semantic Specifications,"1 November, 2021"
302,Animesh Garg,exploration methods based on pseudocount of transitions or curiosity of dynamics have achieved promising results in solving reinforcement learning with sparse rewards however such methods are usually sensitive to environmental dynamicsirrelevant information eg whitenoise to handle such dynamicsirrelevant information we propose a dynamic bottleneck db model which attains a dynamicsrelevant representation based on the informationbottleneck principle based on the db model we further propose dbbonus which encourages the agent to explore stateaction pairs with high information gain we establish theoretical connections between the proposed dbbonus the upper confidence bound ucb for linear case and the visiting count for tabular case we evaluate the proposed method on atari suits with dynamicsirrelevant noises our experiments show that exploration with db bonus outperforms several stateoftheart exploration methods in noisy environments less,Dynamic Bottleneck for Robust Self-Supervised Exploration,"25 October, 2021"
303,Animesh Garg,solving the hamiltonjacobibellman equation is important in many domains including control robotics and economics especially for continuous control solving this differential equation and its extension the hamiltonjacobiisaacs equation is important as it yields the optimal policy that achieves the maximum reward on a give task in the case of the hamiltonjacobiisaacs equation which includes an adversary controlling the environment and minimizing the reward the obtained policy is also robust to perturbations of the dynamics in this paper we propose continuous fitted value iteration cfvi and robust fitted value iteration rfvi these algorithms leverage the nonlinear controlaffine dynamics and separable state and action reward of many continuous control problems to derive the optimal policy and optimal adversary in closed form this analytic expression simplifies the differential equations and enables us to solve for the optimal value function using value iteration for continuous actions and states as well as the adversarial case notably the resulting algorithms do not require discretization of states or actions we apply the resulting algorithms to the furuta pendulum and cartpole we show that both algorithms obtain the optimal policy the robustness simreal experiments on the physical systems show that the policies successfully achieve the task in the realworld when changing the masses of the pendulum we observe that robust value iteration is more robust compared to deep reinforcement learning algorithm and the nonrobust version of the algorithm videos of the experiments are shown at httpssitesgooglecomviewrfvi less,Continuous-Time Fitted Value Iteration for Robust Policies,"5 October, 2021"
304,Animesh Garg,in realworld multiagent systems agents with different capabilities may join or leave without altering the teams overarching goals coordinating teams with such dynamic composition is challenging the optimal team strategy varies with the composition we propose copa a coachplayer framework to tackle this problem we assume the coach has a global view of the environment and coordinates the players who only have partial views by distributing individual strategies specifically we adopt the attention mechanism for both the coach and the players propose a variational objective to regularize learning and design an adaptive communication method to let the coach decide when to communicate with the players we validate our methods on a resource collection task a rescue game and the starcraft micromanagement tasks we demonstrate zeroshot generalization to new team compositions our method achieves comparable or better performance than the setting where all players have a full view of the environment moreover we see that the performance remains high even when the coach communicates as little as of the time using the adaptive communication strategy less,Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition,"3 September, 2021"
305,Animesh Garg,we present a system for learning a challenging dexterous manipulation task involving moving a cube to an arbitrary dof pose with only fingers trained with nvidias isaacgym simulator we show empirical benefits both in simulation and simtoreal transfer of using keypoints as opposed to positionquaternion representations for the object pose in dof for policy observations and in reward calculation to train a modelfree reinforcement learning agent by utilizing domain randomization strategies along with the keypoint representation of the pose of the manipulated object we achieve a high success rate of on a remote trifinger system maintained by the organizers of the real robot challenge with the aim of assisting further research in learning inhand manipulation we make the codebase of our system along with trained checkpoints that come with billions of steps of experience available at httpssriggithubio less,Transferring Dexterous Manipulation from GPU Simulation to a Remote Real-World TriFinger,"22 August, 2021"
306,Animesh Garg,offline reinforcement learning proposes to learn policies from large collected datasets without interacting with the physical environment these algorithms have made it possible to learn useful skills from data that can then be deployed in the environment in realworld settings where interactions may be costly or dangerous such as autonomous driving or factories however current algorithms overfit to the dataset they are trained on and exhibit poor outofdistribution generalization to the environment when deployed in this paper we study the effectiveness of performing data augmentations on the state space and study different augmentation schemes and how they behave with existing offline rl algorithms we then combine the best data performing augmentation scheme with a stateoftheart qlearning technique and improve the function approximation of the qnetworks by smoothening out the learned stateaction space we experimentally show that using this surprisingly simple selfsupervision technique in rl srl we significantly improve over the current stateoftheart algorithms on offline robot learning environments such as metaworld and robosuite and benchmark datasets such as drl less,S4RL: Surprisingly Simple Self-Supervision for Offline Reinforcement Learning,"4 July, 2021"
307,Animesh Garg,effective control and prediction of dynamical systems often require appropriate handling of continuoustime and discrete eventtriggered processes stochastic hybrid systems shss common across engineering domains provide a formalism for dynamical systems subject to discrete possibly stochastic state jumps and multimodal continuoustime flows despite the versatility and importance of shss across applications a general procedure for the explicit learning of both discrete events and multimode continuous dynamics remains an open problem this work introduces neural hybrid automata nhas a recipe for learning shs dynamics without a priori knowledge on the number of modes and intermodal transition dynamics nhas provide a systematic inference method based on normalizing flows neural differential equations and selfsupervision we showcase nhas on several tasks including mode recovery and flow learning in systems with stochastic transitions and endtoend learning of hierarchical robot controllers less,Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions,"8 June, 2021"
308,Animesh Garg,robotic cutting of soft materials is critical for applications such as food processing household automation and surgical manipulation as in other areas of robotics simulators can facilitate controller verification policy learning and dataset generation moreover differentiable simulators can enable gradientbased optimization which is invaluable for calibrating simulation parameters and optimizing controllers in this work we present disect the first differentiable simulator for cutting soft materials the simulator augments the finite element method fem with a continuous contact model based on signed distance fields sdf as well as a continuous damage model that inserts springs on opposite sides of the cutting plane and allows them to weaken until zero stiffness enabling crack formation through various experiments we evaluate the performance of the simulator we first show that the simulator can be calibrated to match resultant forces and deformation fields from a stateoftheart commercial solver and realworld cutting datasets with generality across cutting velocities and object instances we then show that bayesian inference can be performed efficiently by leveraging the differentiability of the simulator estimating posteriors over hundreds of parameters in a fraction of the time of derivativefree methods finally we illustrate that control parameters in the simulation can be optimized to minimize cutting forces via lateral slicing motions we publish videos and additional results on our project website at httpsdiffcuttingsimgithubio less,DiSECt: A Differentiable Simulation Engine for Autonomous Robotic Cutting,"25 May, 2021"
309,Animesh Garg,one principled approach for provably efficient exploration is incorporating the upper confidence bound ucb into the value function as a bonus however ucb is specified to deal with linear and tabular settings and is incompatible with deep reinforcement learning drl in this paper we propose a principled exploration method for drl through optimistic bootstrapping and backward induction obi obi constructs a generalpurpose ucbbonus through nonparametric bootstrap in drl the ucbbonus estimates the epistemic uncertainty of stateaction pairs for optimistic exploration we build theoretical connections between the proposed ucbbonus and the lsviucb in a linear setting we propagate future uncertainty in a timeconsistent manner through episodic backward update which exploits the theoretical advantage and empirically improves the sampleefficiency our experiments in the mnist maze and atari suite suggest that obi outperforms several stateoftheart exploration approaches less,Principled Exploration via Optimistic Bootstrapping and Backward Induction,"16 May, 2021"
310,Animesh Garg,to quickly solve new tasks in complex environments intelligent agents need to build up reusable knowledge for example a learned world model captures knowledge about the environment that applies to new tasks similarly skills capture general behaviors that can apply to new tasks in this paper we investigate how these two approaches can be integrated into a single reinforcement learning agent specifically we leverage the idea of partial amortization for fast adaptation at test time for this actions are produced by a policy that is learned over time while the skills it conditions on are chosen using online planning we demonstrate the benefits of our design decisions across a suite of challenging locomotion tasks and demonstrate improved sample efficiency in single tasks as well as in transfer from one task to another as compared to competitive baselines videos are available at httpssitesgooglecomviewlatentskillplanning less,Latent Skill Planning for Exploration and Transfer,"2 May, 2021"
311,Animesh Garg,safe exploration presents a major challenge in reinforcement learning rl when active data collection requires deploying partially trained policies we must ensure that these policies avoid catastrophically unsafe regions while still enabling trial and error learning in this paper we target the problem of safe exploration in rl by learning a conservative safety estimate of environment states through a critic and provably upper bound the likelihood of catastrophic failures at every training iteration we theoretically characterize the tradeoff between safety and policy improvement show that the safety constraints are likely to be satisfied with high probability during training derive provable convergence guarantees for our approach which is no worse asymptotically than standard rl and demonstrate the efficacy of the proposed approach on a suite of challenging navigation manipulation and locomotion tasks empirically we show that the proposed approach can achieve competitive task performance while incurring significantly lower catastrophic failure rates during training than prior methods videos are at this url httpssitesgooglecomviewconservativesafetycriticshome less,Conservative Safety Critics for Exploration,"26 April, 2021"
312,Animesh Garg,despite widespread use in natural language processing nlp tasks word embeddings have been criticized for inheriting unintended gender bias from training corpora programmer is more closely associated with man and homemaker is more closely associated with woman such gender bias has also been shown to propagate in downstream tasks less,[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation,"14 April, 2021"
313,Animesh Garg,understanding the gap between simulation and reality is critical for reinforcement learning with legged robots which are largely trained in simulation however recent work has resulted in sometimes conflicting conclusions with regard to which factors are important for success including the role of dynamics randomization in this paper we aim to provide clarity and understanding on the role of dynamics randomization in learning robust locomotion policies for the laikago quadruped robot surprisingly in contrast to prior work with the same robot model we find that direct simtoreal transfer is possible without dynamics randomization or onrobot adaptation schemes we conduct extensive ablation studies in a simtosim setting to understand the key issues underlying successful policy transfer including other design decisions that can impact policy robustness we further ground our conclusions via simtoreal experiments with various gaits speeds and stepping frequencies additional details httpswwwpairtorontoeduunderstandingdr less,Dynamics Randomization Revisited:A Case Study for Quadrupedal Locomotion,"24 March, 2021"
314,Animesh Garg,multigoal reaching is an important problem in reinforcement learning needed to achieve algorithmic generalization despite recent advances in this field current algorithms suffer from three major challenges high sample complexity learning only a single way of reaching the goals and difficulties in solving complex motion planning tasks in order to address these limitations we introduce the concept of cumulative accessibility functions which measure the reachability of a goal from a given state within a specified horizon we show that these functions obey a recurrence relation which enables learning from offline interactions we also prove that optimal cumulative accessibility functions are monotonic in the planning horizon additionally our method can trade off speed and reliability in goalreaching by suggesting multiple paths to a single goal depending on the provided horizon we evaluate our approach on a set of multigoal discrete and continuous control tasks we show that our method outperforms stateoftheart goalreaching algorithms in success rate sample complexity and path optimality our code is available at httpsgithubcomlayerailabscae and additional visualizations can be found at httpssitesgooglecomviewlearningcae less,C-Learning: Horizon-Aware Cumulative Accessibility Estimation,"25 January, 2021"
315,Animesh Garg,evolution in nature illustrates that the creatures biological structure and their sensorimotor skills adapt to the environmental changes for survival likewise the ability to morph and acquire new skills can facilitate an embodied agent to solve tasks of varying complexities in this work we introduce a datadriven approach where effective hand designs naturally emerge for the purpose of grasping diverse objects jointly optimizing morphology and control imposes computational challenges since it requires constant evaluation of a blackbox function that measures the performance of a combination of embodiment and behavior we develop a novel bayesian optimization algorithm that efficiently codesigns the morphology and grasping skills jointly through learned latentspace representations we design the grasping tasks based on a taxonomy of three human grasp types power grasp pinch grasp and lateral grasp through experimentation and comparative study we demonstrate the effectiveness of our approach in discovering robust and costefficient hand morphologies for grasping novel objects less,Emergent Hand Morphology and Control from Optimizing Robust Grasps of Diverse Objects,"22 December, 2020"
316,Igor Gilitschenski,obtaining d object representations is important for creating photorealistic simulators and collecting assets for arvr applications neural fields have shown their effectiveness in learning a continuous volumetric representation of a scene from d images but acquiring object representations from these models with weak supervision remains an open challenge in this paper we introduce laterf a method for extracting an object of interest from a scene given d images of the entire scene and known camera poses a natural language description of the object and a small number of pointlabels of object and nonobject points in the input images to faithfully extract the object from the scene laterf extends the nerf formulation with an additional objectness probability at each d point additionally we leverage the rich latent space of a pretrained clip model combined with our differentiable object renderer to inpaint the occluded parts of the object we demonstrate highfidelity object extraction on both synthetic and real datasets and justify our design choices through an extensive ablation study less,LaTeRF: Label and Text Driven Object Radiance Fields,"5 July, 2022"
317,Igor Gilitschenski,experience replay plays a crucial role in improving the sample efficiency of deep reinforcement learning agents recent advances in experience replay propose using mixup zhang et al to further improve sample efficiency via synthetic sample generation we build upon this technique with neighborhood mixup experience replay nmer a geometricallygrounded replay buffer that interpolates transitions with their closest neighbors in stateaction space nmer preserves a locally linear approximation of the transition manifold by only applying mixup between transitions with vicinal stateaction features under nmer a given transitions set of state action neighbors is dynamic and episode agnostic in turn encouraging greater policy generalizability via interepisode interpolation we combine our approach with recent offpolicy deep reinforcement learning algorithms and evaluate on continuous control environments we observe that nmer improves sample efficiency by an average td and sac over baseline replay buffers enabling agents to effectively recombine previous experiences and learn from limited data less,Neighborhood Mixup Experience Replay: Local Convex Interpolation for Improved Sample Efficiency in Continuous Control Tasks,"17 May, 2022"
318,Igor Gilitschenski,simulation has the potential to transform the development of robust algorithms for mobile agents deployed in safetycritical scenarios however the poor photorealism and lack of diverse sensor modalities of existing simulation engines remain key hurdles towards realizing this potential here we present vista an open source datadriven simulator that integrates multiple types of sensors for autonomous vehicles using high fidelity realworld datasets vista represents and simulates rgb cameras d lidar and eventbased cameras enabling the rapid generation of novel viewpoints in simulation and thereby enriching the data available for policy learning with corner cases that are difficult to capture in the physical world using vista we demonstrate the ability to train and test perceptiontocontrol policies across each of the sensor types and showcase the power of this approach via deployment on a full scale autonomous vehicle the policies learned in vista exhibit simtoreal transfer without modification and greater robustness than those trained exclusively on realworld data less,"VISTA 2.0: An Open, Data-driven Simulator for Multimodal Sensing and Policy Learning for Autonomous Vehicles","23 November, 2021"
319,Igor Gilitschenski,modeling multimodal highlevel intent is important for ensuring diversity in trajectory prediction existing approaches explore the discrete nature of human intent before predicting continuous trajectories to improve accuracy and support explainability however these approaches often assume the intent to remain fixed over the prediction horizon which is problematic in practice especially over longer horizons to overcome this limitation we introduce hyper a general and expressive hybrid prediction framework that models evolving human intent by modeling traffic agents as a hybrid discretecontinuous system our approach is capable of predicting discrete intent changes over time we learn the probabilistic hybrid model via a maximum likelihood estimation problem and leverage neural proposal distributions to sample adaptively from the exponentially growing discrete space the overall approach affords a better tradeoff between accuracy and coverage we train and validate our model on the argoverse dataset and demonstrate its effectiveness through comprehensive ablation studies and comparisons with stateoftheart models less,HYPER: Learned Hybrid Trajectory Prediction via Factored Inference and Adaptive Sampling,"5 October, 2021"
320,Igor Gilitschenski,learning competitive behaviors in multiagent settings such as racing requires longterm reasoning about potential adversarial interactions this paper presents deep latent competition dlc a novel reinforcement learning algorithm that learns competitive visual control policies through selfplay in imagination the dlc agent imagines multiagent interaction sequences in the compact latent space of a learned world model that combines a joint transition function with opponent viewpoint prediction imagined selfplay reduces costly sample generation in the real world while the latent representation enables planning to scale gracefully with observation dimensionality we demonstrate the effectiveness of our algorithm in learning competitive behaviors on a novel multiagent racing benchmark that requires planning from image observations code and videos available at httpssitesgooglecomviewdeeplatentcompetition less,Deep Latent Competition: Learning to Race Using Visual Control Policies in Latent Space,"19 February, 2021"
321,Igor Gilitschenski,we present an efficient coresetsbased neural network compression algorithm that sparsifies the parameters of a trained fullyconnected neural network in a manner that provably approximates the networks output our approach is based on an importance sampling scheme that judiciously defines a sampling distribution over the neural network parameters and as a result retains parameters of high importance while discarding redundant ones we leverage a novel empirical notion of sensitivity and extend traditional coreset constructions to the application of compressing parameters our theoretical analysis establishes guarantees on the size and accuracy of the resulting compressed network and gives rise to generalization bounds that may provide new insights into the generalization properties of neural networks we demonstrate the practical effectiveness of our algorithm on a variety of neural network configurations and realworld data sets less,Data-Dependent Coresets for Compressing Neural Networks with Applications to Generalization Bounds,"17 May, 2019"
322,Igor Gilitschenski,we present an online landmark selection method for distributed longterm visual localization systems in bandwidthconstrained environments sharing a common map for online localization provides a fleet of au tonomous vehicles with the possibility to maintain and access a consistent map source and therefore reduce redundancy while increasing efficiency however connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data the wide range of varying appearance conditions encountered during longterm visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time motivated by this we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appear ance condition the ranking function this selection is based upon exploits landmark coobservability statistics collected in past traversals through the mapped area evaluation is per formed over different outdoor environments large timescales and varying appearance conditions including the extreme tran sition from daytime to nighttime demonstrating that with our appearancedependent selection method we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance less,Appearance-Based Landmark Selection for Efficient Long-Term Visual Localization,"8 August, 2018"
323,Igor Gilitschenski,in the absence of global positioning information place recognition is a key capability for enabling localization mapping and navigation in any environment most place recognition methods rely on images point clouds or a combination of both in this work we leverage a segment extraction and matching approach to achieve place recognition in light detection and ranging lidar based d point cloud maps one challenge related to this approach is the recognition of segments despite changes in point of view or occlusion we propose using a learning based method in order to reach a higher recall accuracy then previously proposed methods using convolutional neural networks cnns which are stateoftheart classifiers we propose a new approach to segment recognition based on learned descriptors in this paper we compare the effectiveness of three different structures and training methods for cnns we demonstrate through several experiments on realworld data collected in an urban driving scenario that the proposed learning based methods outperform handcrafted descriptors less,Learning 3D Segment Descriptors for Place Recognition,"24 April, 2018"
324,Igor Gilitschenski,over the last decades quaternions have become a crucial and very successful tool for attitude representation in robotics and aerospace however there is a major problem that is continuously causing trouble in practice when it comes to exchanging formulas or implementations there are two quaternion multiplications in common use hamiltons original multiplication and its flipped version which is often associated with nasas jet propulsion laboratory we believe that this particular issue is completely avoidable and only exists today due to a lack of understanding this paper explains the underlying problem for the popular passive world to body usage of rotation quaternions and derives an alternative solution compatible with hamiltons multiplication furthermore it argues for entirely discontinuing the flipped multiplication additionally it provides recipes for efficiently detecting relevant conventions and migrating formulas or algorithms between them less,Why and How to Avoid the Flipped Quaternion Multiplication,"5 February, 2018"
325,Igor Gilitschenski,in this paper we present libdirectional a matlab library for directional statistics and directional estimation it supports a variety of commonly used distributions on the unit circle such as the von mises wrapped normal and wrapped cauchy distributions furthermore various distributions on higherdimensional manifolds such as the unit hypersphere and the hypertorus are available based on these distributions several recursive filtering algorithms in libdirectional allow estimation on these manifolds the functionality is implemented in a clear welldocumented and objectoriented structure that is both easy to use and easy to extend less,Directional Statistics and Filtering Using libDirectional,"27 December, 2017"
326,Igor Gilitschenski,environmental conditions and external effects such as shocks have a significant impact on the calibration parameters of visualinertial sensor systems thus longterm operation of these systems cannot fully rely on factory calibration since the observability of certain parameters is highly dependent on the motion of the device using short data segments at device initialization may yield poor results when such systems are additionally subject to energy constraints it is also infeasible to use fullbatch approaches on a big dataset and careful selection of the data is of high importance in this paper we present a novel approach for resource efficient selfcalibration of visualinertial sensor systems this is achieved by casting the calibration as a segmentbased optimization problem that can be run on a small subset of informative segments consequently the computational burden is limited as only a predefined number of segments is used we also propose an efficient informationtheoretic selection to identify such informative motion segments in evaluations on a challenging dataset we show our approach to significantly outperform stateoftheart in terms of computational burden while maintaining a comparable accuracy less,Visual-inertial self-calibration on informative motion segments,"8 August, 2017"
327,Igor Gilitschenski,for recursive circular filtering based on circular statistics we introduce a general framework for estimation of a circular state based on different circular distributions specifically the wrapped normal distribution and the von mises distribution we propose an estimation method for circular systems with nonlinear system and measurement functions this is achieved by relying on efficient deterministic sampling techniques furthermore we show how the calculations can be simplified in a variety of important special cases such as systems with additive noise as well as identity system or measurement functions we introduce several novel key components particularly a distributionfree prediction algorithm a new and superior formula for the multiplication of wrapped normal densities and the ability to deal with nonadditive system noise all proposed methods are thoroughly evaluated and compared to several stateoftheart solutions less,Recursive Bayesian Filtering in Circular State Spaces,"21 January, 2015"
328,Igor Gilitschenski,the wrapped normal distribution arises when a the density of a onedimensional normal distribution is wrapped around the circle infinitely many times at first look evaluation of its probability density function appears tedious as an infinite series is involved in this paper we investigate the evaluation of two truncated series representations as one representation performs well for small uncertainties whereas the other performs well for large uncertainties we show that in all cases a small number of summands is sufficient to achieve high accuracy less,Efficient Evaluation of the Probability Density Function of a Wrapped Normal Distribution,"25 May, 2014"
329,Igor Gilitschenski,directional estimation is a common problem in many tracking applications traditional filters such as the kalman filter perform poorly because they fail to take the periodic nature of the problem into account we present a recursive filter for directional data based on the bingham distribution in two dimensions the proposed filter can be applied to circular filtering problems with degree symmetry ie rotations by degrees cannot be distinguished it is easily implemented using standard numerical techniques and suitable for realtime applications the presented approach is extensible to quaternions which allow tracking arbitrary threedimensional orientations we evaluate our filter in a challenging scenario and compare it to a traditional kalman filtering approach less,Recursive Estimation of Orientation Based on the Bingham Distribution,"30 April, 2013"
330,Anna Goldenberg,although reinforcement learning rl has tremendous success in many fields applying rl to realworld settings such as healthcare is challenging when the reward is hard to specify and no exploration is allowed in this work we focus on recovering clinicians rewards in treating patients we incorporate the whatif reasoning to explain the clinicians treatments based on their potential future outcomes we use generalized additive models gams a class of accurate interpretable models to recover the reward in both simulation and a realworld hospital dataset we show our model outperforms baselines finally our models explanations match several clinical guidelines when treating patients while we found the commonlyused linear model often contradicts them less,Extracting Expert's Goals by What-if Interpretable Modeling,"13 June, 2022"
331,Anna Goldenberg,realworld time series data are often generated from several sources of variation learning representations that capture the factors contributing to this variability enables a better understanding of the data via its underlying generative process and improves performance on downstream machine learning tasks this paper proposes a novel generative approach for learning representations for the global and local factors of variation in time series the local representation of each sample models nonstationarity over time with a stochastic process prior and the global representation of the sample encodes the timeindependent characteristics to encourage decoupling between the representations we introduce counterfactual regularization that minimizes the mutual information between the two variables in experiments we demonstrate successful recovery of the true local and global variability factors on simulated data and show that representations learned using our method yield superior performance on downstream tasks on realworld datasets we believe that the proposed way of defining representations is beneficial for data modelling and yields better insights into the complexity of realworld data less,Decoupling Local and Global Representations of Time Series,"11 February, 2022"
332,Anna Goldenberg,despite the success of machine learning applications in science industry and society in general many approaches are known to be nonrobust often relying on spurious correlations to make predictions spuriousness occurs when some features correlate with labels but are not causal relying on such features prevents models from generalizing to unseen environments where such correlations break in this work we focus on image classification and propose two data generation processes to reduce spuriousness given human annotations of the subset of the features responsible causal for the labels eg bounding boxes we modify this causal set to generate a surrogate image that no longer has the same label ie a counterfactual image we also alter noncausal features to generate images still recognized as the original labels which helps to learn a model invariant to these features in several challenging datasets our data generations outperform stateoftheart methods in accuracy when spurious correlations break and increase the saliency focus on causal features providing better explanations less,Towards Robust Classification Model by Counterfactual and Invariant Data Generation,"3 June, 2021"
333,Anna Goldenberg,modern deep unsupervised learning methods have shown great promise for detecting diseases across a variety of medical imaging modalities while previous generative modeling approaches successfully perform anomaly detection by learning the distribution of healthy d image slices they process such slices independently and ignore the fact that they are correlated all being sampled from a d volume we show that incorporating the d context and processing wholebody mri volumes is beneficial to distinguishing anomalies from their benign counterparts in our work we introduce a multichannel sliding window generative model to perform lesion detection in wholebody mri wbmri our experiments demonstrate that our proposed method significantly outperforms processing individual images in isolation and our ablations clearly show the importance of d reasoning moreover our work also shows that it is beneficial to include additional patientspecific features to further improve anomaly detection in pediatric scans less,3D Reasoning for Unsupervised Anomaly Detection in Pediatric WbMRI,"24 March, 2021"
334,Anna Goldenberg,explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature we propose fit a framework that evaluates the importance of observations for a multivariate timeseries blackbox model by quantifying the shift in the predictive distribution over time fit defines the importance of an observation based on its contribution to the distributional shift under a kldivergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved we also demonstrate the need to control for timedependent distribution shifts we compare with stateoftheart baselines on simulated and realworld clinical data and demonstrate that our approach is superior in identifying important time points and observations throughout the time series less,What went wrong and when? Instance-wise Feature Importance for Time-series Models,"28 October, 2020"
335,Anna Goldenberg,multitask learning mtl is a machine learning technique aiming to improve model performance by leveraging information across many tasks it has been used extensively on various data modalities including electronic health record ehr data however despite significant use on ehr data there has been little systematic investigation of the utility of mtl across the diverse set of possible tasks and training schemes of interest in healthcare in this work we examine mtl across a battery of tasks on ehr timeseries data we find that while mtl does suffer from common negative transfer we can realize significant gains via mtl pretraining combined with singletask finetuning we demonstrate that these gains can be achieved in a taskindependent manner and offer not only minor improvements under traditional learning but also notable gains in a fewshot learning context thereby suggesting this could be a scalable vehicle to offer improved performance in important healthcare contexts less,A Comprehensive Evaluation of Multi-task Learning and Multi-task Pre-training on EHR Time-series Data,"20 July, 2020"
336,Anna Goldenberg,imagine a patient in critical condition what and when should be measured to forecast detrimental events especially under the budget constraints we answer this question by deep reinforcement learning rl that jointly minimizes the measurement cost and maximizes predictive gain by scheduling strategicallytimed measurements we learn our policy to be dynamically dependent on the patients health history to scale our framework to exponentially large action space we distribute our reward in a sequential setting that makes the learning easier in our simulation our policy outperforms heuristicbased scheduling with higher predictive gain and lower cost in a realworld icu mortality prediction task mimic our policies reduce the total number of measurements by or improve predictive gain by a factor of as compared to physicians under the offpolicy policy evaluation less,Dynamic Measurement Scheduling for Event Forecasting using Deep RL,"7 June, 2019"
337,Anna Goldenberg,deep learning algorithms have increasingly been shown to lack robustness to simple adversarial examples advx an equally troubling observation is that these adversarial examples transfer between different architectures trained on different datasets we investigate the transferability of adversarial examples between models using the angle between the inputoutput jacobians of different models to demonstrate the relevance of this approach we perform case studies that involve jointly training pairs of models these case studies empirically justify the theoretical intuitions for why the angle between gradients is a fundamental quantity in advx transferability furthermore we consider the asymmetry of advx transferability between two models of the same architecture and explain it in terms of differences in gradient norms between the models lastly we provide a simple modification to existing training setups that reduces transferability of adversarial examples between pairs of models less,Reducing Adversarial Example Transferability Using Gradient Regularization,"16 April, 2019"
338,Anna Goldenberg,when an image classifier makes a prediction which parts of the image are relevant and why we can rephrase this question to ask which parts of the image if they were not seen by the classifier would most change its decision producing an answer requires marginalizing over images that could have been seen but werent we can sample plausible image infills by conditioning a generative model on the rest of the image we then optimize to find the image regions that most change the classifiers decision after infill our approach contrasts with adhoc infilling approaches such as blurring or injecting noise which generate inputs far from the data distribution and ignore informative relationships between different parts of the image our method produces more compact and relevant saliency maps with fewer artifacts compared to previous methods less,Explaining Image Classifiers by Counterfactual Generation,"25 February, 2019"
339,Anna Goldenberg,machine learning for healthcare often trains models on deidentified datasets with randomlyshifted calendar dates ignoring the fact that data were generated under hospital operation practices that change over time these changing practices induce definitive changes in observed data which confound evaluations which do not account for dates and limit the generalisability of dateagnostic models in this work we establish the magnitude of this problem on mimic a public hospital dataset and showcase a simple solution we augment mimic with the year in which care was provided and show that a model trained using standard feature representations will significantly degrade in quality over time we find a deterioration of auc when evaluating mortality prediction on data from years later we find a similar deterioration of auc for lengthofstay in contrast we demonstrate that clinicallyoriented aggregates of raw features significantly mitigate future deterioration our suggested aggregated representations when retrained yearly have prediction quality comparable to yearagnostic models less,Rethinking clinical prediction: Why machine learning must consider year of care and feature aggregation,"29 November, 2018"
340,Anna Goldenberg,we present two deep generative models based on variational autoencoders to improve the accuracy of drug response prediction our models perturbation variational autoencoder and its semisupervised extension drug response variational autoencoder drvae learn latent representation of the underlying gene states before and after drug application that depend on i druginduced biological change of each gene and ii overall treatment response outcome our vaebased models outperform the current published benchmarks in the field by anywhere from to auroc and to aupr in addition we found that better reconstruction accuracy does not necessarily lead to improvement in classification accuracy and that jointly trained models perform better than models that minimize reconstruction error independently less,Dr.VAE: Drug Response Variational Autoencoder,"6 July, 2017"
341,Anna Goldenberg,identifying genes associated with complex human diseases is one of the main challenges of human genetics and computational medicine to answer this question millions of genetic variants get screened to identify a few of importance to increase the power of identifying genes associated with diseases and to account for other potential sources of protein function aberrations we propose a novel factorgraph based model where much of the biological knowledge is incorporated through factors and priors our extensive simulations show that our method has superior sensitivity and precision compared to variantaggregating and differential expression methods our integrative approach was able to identify important genes in breast cancer identifying genes that had coding aberrations in some patients and regulatory abnormalities in others emphasizing the importance of data integration to explain the disease in a larger number of patients less,Combining exome and gene expression datasets in one graphical model of disease to empower the discovery of disease mechanisms,"29 August, 2015"
342,Anna Goldenberg,analysis of high dimensional noisy data is of essence across a variety of research fields feature selection techniques are designed to find the relevant feature subset that can facilitate classification or pattern detection traditional supervised feature selection methods utilize label information to guide the identification of relevant feature subsets in this paper however we consider the unsupervised feature selection problem without the label information it is particularly difficult to identify a small set of relevant features due to the noisy nature of realworld data which corrupts the intrinsic structure of the data our gradientbased laplacian feature selection glfs selects important features by minimizing the variance of the laplacian regularized least squares regression model with ell relaxation glfs can find a sparse subset of features that is relevant to the laplacian manifolds extensive experiments on simulated three realworld object recognition and two computational biology datasets have illustrated the power and superior performance of our approach over multiple stateoftheart unsupervised feature selection methods additionally we show that glfs selects a sparser set of more relevant features in a supervised setting outperforming the popular elastic net methodology less,Gradient-based Laplacian Feature Selection,"10 April, 2014"
343,Roger Grosse,while designing inductive bias in neural architectures has been widely studied we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks here we replace architecture engineering by encoding inductive bias in the form of datasets inspired by peirces view that deduction induction and abduction are the primitives of reasoning we design three synthetic tasks that are intended to require the model to have these three abilities we specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks this defines a new pretraining methodology called lime learning inductive bias for mathematical reasoning models trained with lime significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks unlike dominating the computation cost as traditional pretraining approaches lime requires only a small fraction of the computation cost of the typical downstream task the code for generating lime tasks is available at httpsgithubcomtonywulime less,LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning,"15 March, 2022"
344,Roger Grosse,smooth minimax games often proceed by simultaneous or alternating gradient updates although algorithms with alternating updates are commonly used in practice the majority of existing theoretical analyses focus on simultaneous algorithms for convenience of analysis in this paper we study alternating gradient descentascent altgda in minimax games and show that altgda is superior to its simultaneous counterpartsimgda in many settings we prove that altgda achieves a nearoptimal local convergence rate for strongly convexstrongly concave scsc problems while simgda converges at a much slower rate to our knowledge this is the emphfirst result of any setting showing that altgda converges faster than simgda by more than a constant we further adapt the theory of integral quadratic constraints iqc and show that altgda attains the same rate emphglobally for a subclass of scsc minimax problems empirically we demonstrate that alternating updates speed up gan training significantly and the use of optimism only helps for simultaneous algorithms less,Near-optimal Local Convergence of Alternating Gradient Descent-Ascent for Minimax Optimization,"12 February, 2022"
345,Roger Grosse,annealed importance sampling ais and related algorithms are highly effective tools for marginal likelihood estimation but are not fully differentiable due to the use of metropolishastings correction steps differentiability is a desirable property as it would admit the possibility of optimizing marginal likelihood as an objective using gradientbased methods to this end we propose differentiable ais dais a variant of ais which ensures differentiability by abandoning the metropolishastings corrections as a further advantage dais allows for minibatch gradients we provide a detailed convergence analysis for bayesian linear regression which goes beyond previous analyses by explicitly accounting for the sampler not having reached equilibrium using this analysis we prove that dais is consistent in the fullbatch setting and provide a sublinear convergence rate furthermore motivated by the problem of learning from largescale datasets we study a stochastic variant of dais that uses minibatch gradients surprisingly stochastic dais can be arbitrarily bad due to a fundamental incompatibility between the goals of lastiterate convergence to the posterior and elimination of the accumulated stochastic error this is in stark contrast with other settings such as gradientbased optimization and langevin dynamics where the effect of gradient noise can be washed out by taking smaller steps this indicates that annealingbased marginal likelihood estimation with stochastic gradients may require new ideas less,Differentiable Annealed Importance Sampling and the Perils of Gradient Noise,"26 October, 2021"
346,Roger Grosse,our ability to know when to trust the decisions made by machine learning systems has not kept up with the staggering improvements in their performance limiting their applicability in highstakes domains we introduce proververifier games pvgs a gametheoretic framework to encourage learning agents to solve decision problems in a verifiable manner the pvg consists of two learners with competing objectives a trusted verifier network tries to choose the correct answer and a more powerful but untrusted prover network attempts to persuade the verifier of a particular answer regardless of its correctness the goal is for a reliable justification protocol to emerge from this game we analyze variants of the framework including simultaneous and sequential games and narrow the space down to a subset of games which provably have the desired equilibria we develop instantiations of the pvg for two algorithmic tasks and show that in practice the verifier learns a robust decision rule that is able to receive useful and reliable information from an untrusted prover importantly the protocol still works even when the verifier is frozen and the provers messages are directly optimized to convince the verifier less,Learning to Give Checkable Answers with Prover-Verifier Games,"26 August, 2021"
347,Roger Grosse,linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent sgd typically leads to a monotonic decrease in the training objective this monotonic linear interpolation mli property first observed by goodfellow et al persists in spite of the nonconvex objectives and highly nonlinear training dynamics of neural networks extending this work we evaluate several hypotheses for this property that to our knowledge have not yet been explored using tools from differential geometry we draw connections between the interpolated paths in function space and the monotonicity of the network providing sufficient conditions for the mli property under mean squared error while the mli property holds under various settings eg network architectures and learning problems we show in practice that networks violating the mli property can be produced systematically by encouraging the weights to move far from initialization the mli property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties less,Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes,"23 April, 2021"
348,Roger Grosse,while uncertainty estimation is a wellstudied topic in deep learning most such work focuses on marginal uncertainty estimates ie the predictive mean and variance at individual input locations but it is often more useful to estimate predictive correlations between the function values at different input locations in this paper we consider the problem of benchmarking how accurately bayesian models can estimate predictive correlations we first consider a downstream task which depends on posterior predictive correlations transductive active learning tal we find that tal makes better use of models uncertainty estimates than ordinary active learning and recommend this as a benchmark for evaluating bayesian models since tal is too expensive and indirect to guide development of algorithms we introduce two metrics which more directly evaluate the predictive correlations and which can be computed efficiently metacorrelations ie the correlations between the models correlation estimates and the true values and crossnormalized likelihoods xll we validate these metrics by demonstrating their consistency with tal performance and obtain insights about the relative performance of current bayesian neural net and gaussian process models less,Beyond Marginal Uncertainty: How Accurately can Bayesian Regression Models Estimate Posterior Predictive Correlations?,"28 February, 2021"
349,Roger Grosse,overparameterization has been shown to benefit both the optimization and generalization of neural networks but large networks are resource hungry at both training and test time network pruning can reduce testtime resource requirements but is typically applied to trained networks and therefore cannot avoid the expensive training process we aim to prune networks at initialization thereby saving resources at training time as well specifically we argue that efficient training requires preserving the gradient flow through the network this leads to a simple but effective pruning criterion we term gradient signal preservation grasp we empirically investigate the effectiveness of the proposed method with extensive experiments on cifar cifar tinyimagenet and imagenet using vggnet and resnet architectures our method can prune of the weights of a vgg network on imagenet at initialization with only a drop in top accuracy moreover our method achieves significantly better performance than the baseline at extreme sparsity levels less,Picking Winning Tickets Before Training by Preserving Gradient Flow,"6 August, 2020"
350,Roger Grosse,under some dimension restrictions we prove that totally umbilical hypersurfaces of spinc manifolds carrying a parallel real or imaginary killing spinor are of constant mean curvature this extends to the spinc case the result of o kowalski stating that every totally umbilical hypersurface of an einstein manifold of dimension greater or equal to is of constant mean curvature as an application we prove that there are no extrinsic hypersheres in complete riemannian spin manifolds of nonconstant sectional curvature carrying a parallel killing or imaginary killing spinor less,Totally umbilical hypersurfaces of Spin$^c$ manifolds carrying special spinor fields,"13 November, 2019"
351,Roger Grosse,posterior collapse in variational autoencoders vaes arises when the variational posterior distribution closely matches the prior for a subset of latent variables this paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear vaes and their direct correspondence with probabilistic pca ppca we explain how posterior collapse may occur in ppca due to local maxima in the log marginal likelihood unexpectedly we prove that the elbo objective for the linear vae does not introduce additional spurious local maxima relative to log marginal likelihood we show further that training a linear vae with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions empirically we find that our linear analysis is predictive even for highcapacity nonlinear vaes and helps explain the relationship between the observation noise local maxima and posterior collapse in deep gaussian vaes less,Don't Blame the ELBO! A Linear VAE Perspective on Posterior Collapse,"6 November, 2019"
352,Roger Grosse,increasing the batch size is a popular way to speed up neural network training but beyond some critical batch size larger batch sizes yield diminishing returns in this work we study how the critical batch size changes based on properties of the optimization algorithm including acceleration and preconditioning through two different lenses large scale experiments and analysis of a simple noisy quadratic model nqm we experimentally demonstrate that optimization algorithms that employ preconditioning specifically adam and kfac result in much larger critical batch sizes than stochastic gradient descent with momentum we also demonstrate that the nqm captures many of the essential features of real neural network training despite being drastically simpler to work with the nqm predicts our results with preconditioned optimizers previous results with accelerated gradient descent and other results around optimal learning rates and large batch training making it a useful tool to generate testable predictions about neural network optimization less,Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model,"28 October, 2019"
353,Roger Grosse,momentum is a simple and widely used trick which allows gradientbased optimizers to pick up speed along low curvature directions its performance depends crucially on a damping coefficient large values can potentially deliver much larger speedups but are prone to oscillations and instability hence one typically resorts to small values such as or we propose aggregated momentum aggmo a variant of momentum which combines multiple velocity vectors with different parameters aggmo is trivial to implement but significantly dampens oscillations enabling it to remain stable even for aggressive values such as we reinterpret nesterovs accelerated gradient descent as a special case of aggmo and analyze rates of convergence for quadratic objectives empirically we find that aggmo is a suitable dropin replacement for other momentum methods and frequently delivers faster convergence less,Aggregated Momentum: Stability Through Passive Damping,"1 May, 2019"
354,Roger Grosse,variational bayesian neural networks bnns perform variational inference over weights but it is difficult to specify meaningful priors and approximate posteriors in a highdimensional weight space we introduce functional variational bayesian neural networks fbnns which maximize an evidence lower bound elbo defined directly on stochastic processes ie distributions over functions we prove that the kl divergence between stochastic processes equals the supremum of marginal kl divergences over all finite sets of inputs based on this we introduce a practical training objective which approximates the functional elbo using finite measurement sets and the spectral stein gradient estimator with fbnns we can specify priors entailing rich structures including gaussian processes and implicit stochastic processes empirically we find fbnns extrapolate well using various structured priors provide reliable uncertainty estimates and scale to large datasets less,Functional Variational Bayesian Neural Networks,"13 March, 2019"
355,Roger Grosse,variational bayesian neural networks combine the flexibility of deep learning with bayesian uncertainty estimation however inference procedures for flexible variational posteriors are computationally expensive a recently proposed method noisy natural gradient is a surprisingly simple method to fit expressive posteriors by adding weight noise to regular natural gradient updates noisy kfac is an instance of noisy natural gradient that fits a matrixvariate gaussian posterior with minor changes to ordinary kfac nevertheless a matrixvariate gaussian posterior does not capture an accurate diagonal variance in this work we extend on noisy kfac to obtain a more flexible posterior distribution called eigenvalue corrected matrixvariate gaussian the proposed method computes the full diagonal rescaling factor in kroneckerfactored eigenbasis empirically our approach consistently outperforms existing algorithms eg noisy kfac on regression and classification tasks less,Eigenvalue Corrected Noisy Natural Gradient,"29 November, 2018"
356,Roger Grosse,recurrent neural networks rnns provide stateoftheart performance in processing sequential data but are memory intensive to train limiting the flexibility of rnn models which can be trained reversible rnnsrnns for which the hiddentohidden transition can be reversedoffer a path to reduce the memory requirements of training as hidden states need not be stored and instead can be recomputed during backpropagation we first show that perfectly reversible rnns which require no storage of the hidden activations are fundamentally limited because they cannot forget information from their hidden state we then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of we extend our technique to attentionbased sequencetosequence models where it maintains performance while reducing activation memory cost by a factor of in the encoder and a factor of in the decoder less,Reversible Recurrent Neural Networks,"25 October, 2018"
357,Roger Grosse,the generalization properties of gaussian processes depend heavily on the choice of kernel and this choice remains a dark art we present the neural kernel network nkn a flexible family of kernels represented by a neural network the nkn architecture is based on the composition rules for kernels so that each unit of the network corresponds to a valid kernel it can compactly approximate compositional kernel structures such as those used by the automatic statistician lloyd et al but because the architecture is differentiable it is endtoend trainable with gradientbased optimization we show that the nkn is universal for the class of stationary kernels empirically we demonstrate pattern discovery and extrapolation abilities of nkn on several tasks that depend crucially on identifying the underlying structure including time series and texture extrapolation as well as bayesian optimization less,Differentiable Compositional Kernel Learning for Gaussian Processes,"5 August, 2018"
358,Roger Grosse,stochastic neural net weights are used in a variety of contexts including regularization bayesian neural nets exploration in reinforcement learning and evolution strategies unfortunately due to the large number of weights all the examples in a minibatch typically share the same weight perturbation thereby limiting the variance reduction effect of large minibatches we introduce flipout an efficient method for decorrelating the gradients within a minibatch by implicitly sampling pseudoindependent weight perturbations for each example empirically flipout achieves the ideal linear variance reduction for fully connected networks convolutional networks and rnns we find significant speedups in training neural networks with multiplicative gaussian perturbations we show that flipout is effective at regularizing lstms and outperforms previous methods flipout also enables us to vectorize evolution strategies in our experiments a single gpu with flipout can handle the same throughput as at least cpu cores using existing methods equivalent to a factorof cost reduction on amazon web services less,Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches,"2 April, 2018"
359,Roger Grosse,deep residual networks resnets have significantly pushed forward the stateoftheart on image classification increasing in performance as networks grow both deeper and wider however memory consumption becomes a bottleneck as one needs to store the activations in order to calculate gradients using backpropagation we present the reversible residual network revnet a variant of resnets where each layers activations can be reconstructed exactly from the next layers therefore the activations for most layers need not be stored in memory during backpropagation we demonstrate the effectiveness of revnets on cifar cifar and imagenet establishing nearly identical classification accuracy to equallysized resnets even though the activation storage requirements are independent of depth less,The Reversible Residual Network: Backpropagation Without Storing Activations,"13 July, 2017"
360,Roger Grosse,the variational autoencoder vae kingma welling is a recently proposed generative model pairing a topdown generative network with a bottomup recognition network which approximates posterior inference it typically makes strong assumptions about posterior inference for instance that the posterior distribution is approximately factorial and that its parameters can be approximated with nonlinear regression from the observations as we show empirically the vae objective can lead to overly simplified representations which fail to use the networks entire modeling capacity we present the importance weighted autoencoder iwae a generative model with the same architecture as the vae but which uses a strictly tighter loglikelihood lower bound derived from importance weighting in the iwae the recognition network uses multiple samples to approximate the posterior giving it increased flexibility to model complex posteriors which do not fit the vae modeling assumptions we show empirically that iwaes learn richer latent space representations than vaes leading to improved test loglikelihood on density estimation benchmarks less,Importance Weighted Autoencoders,"7 November, 2016"
361,Roger Grosse,computing the marginal likelihood ml of a model requires marginalizing out all of the parameters and latent variables a difficult highdimensional summation or integration problem to make matters worse it is often hard to measure the accuracy of ones ml estimates we present bidirectional monte carlo a technique for obtaining accurate logml estimates on data simulated from a model this method obtains stochastic lower bounds on the logml using annealed importance sampling or sequential monte carlo and obtains stochastic upper bounds by running these same algorithms in reverse starting from an exact posterior sample the true value can be sandwiched between these two stochastic bounds with high probability using the ground truth logml estimates obtained from our method we quantitatively evaluate a wide variety of existing ml estimators on several latent variable models clustering a low rank approximation and a binary attributes model these experiments yield insights into how to accurately estimate marginal likelihoods less,Sandwiching the marginal likelihood using bidirectional Monte Carlo,"8 November, 2015"
362,Tovi Grossman,mobile user interface summarization generates succinct language descriptions of mobile screens for conveying important contents and functionalities of the screen which can be useful for many languagebased application scenarios we present screenwords a novel screen summarization approach that automatically encapsulates essential information of a ui screen into a coherent language phrase summarizing mobile screens requires a holistic understanding of the multimodal data of mobile uis including text image structures as well as ui semantics motivating our multimodal learning approach we collected and analyzed a largescale screen summarization dataset annotated by human workers our dataset contains more than k language summarization across simk unique ui screens we then experimented with a set of deep models with different configurations our evaluation of these models with both automatic accuracy metrics and human rating shows that our approach can generate highquality summaries for mobile screens we demonstrate potential use cases of screenwords and opensource our dataset and model to lay the foundations for further bridging language and user interfaces less,Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning,"6 August, 2021"
363,Tovi Grossman,"Transitioning from block-based programming environments to conventional text-based programming languages is a challenge faced by many learners as they progress in their computer science education. In this paper, we introduce CodeStruct, a new intermediary programming environment for novices designed to support children who have prior experience with block-based programming to ease the eventual transition to text-based programming. We describe the development of CodeStruct and its key design features. We then present the results from a two-week long programming class with 26 high school students (ages 12-16; M= 14 years) investigating how CodeStruct supported learners in transitioning from Scratch to Python. Our findings reveal how learners used the scaffolds designed into CodeStruct to support their transition from blocks to text, and that transitioning to CodeStruct reduced completion time",CodeStruct: Design and Evaluation of an Intermediary Programming Environment for Novices to Transition from Scratch to Python,2022-06-27 00:00:00
364,Tovi Grossman,"In early stages of creative processes, practitioners externalize and combine inspirational materials, using strategies such as mood board creation to achieve a desired vision and aesthetic. Yet, collecting and combining materials can be difficult:(1) mood boards bias towards 2D images, neglecting audio, video, and 3D models;(2) alternative externalizations such as prototypes are best suited for later stages and can be time-consuming and tedious to create; and (3) online searches lead to disjointed sources between different websites and assets in the file system. To address these challenges, we created MoodCubes, a system for rapid creation and manipulation of multimedia content. When adding content, MoodCubes decomposes objects (eg, extracting colour palettes), suggests new materials without the need to search (eg, 3D models, images, lighting effects), and provides filters to change the scene’s aesthetic","MoodCubes: Immersive Spaces for Collecting, Discovering and Envisioning Inspiration Materials",2022-06-13 00:00:00
365,Tovi Grossman,"Information regarding application usage on an actor device may be provided through activity notifications and activity reports. An activity notification describing current application activity on an actor device is sent, via wireless connection, to an observer device which displays the activity notification. Activity notifications provide different granularity levels of information based on a received level selection or based on a distance (proximity) between the actor device and the observer device. An activity report representing the history of application usage on an actor device may be displayed on the actor device. For example, the activity report may be triggered to by displayed when the actor device is placed flat. The activity report provides a graphical representation of the application usage on the actor device for a predetermined time period of prior usage. The graphical representation may comprise a plurality of stripes ",Sharing computer application activities,2022-05-03 00:00:00
366,Tovi Grossman,"An electronic device tracks, for a user performing a target acquisition movement within a 3D space, movement parameters of a plurality of input devices of the user. The electronic device predicts, for the user, a region of interest within the 3D space, based on the movement parameters. The region of interest includes a plurality of targets in close proximity. The electronic device predicts an endpoint of the target acquisition movement, within the region of interest. In some embodiments, the plurality of input devices includes an eye tracking input device, each input device corresponds to a predefined input device type, and the movement parameters include gaze data from the eye tracking input device. In some embodiments, input devices includes an eye tracking input device, a head-mounted display, and a hand-held controller, and the user's eye, hand, and head movements are coordinated.",Multimodal Kinematic Template Matching and Regression Modeling for Ray Pointing Prediction in Virtual Reality,2022-04-28 00:00:00
367,Tovi Grossman,"How-to videos are often shot using camera angles that may not be optimal for learning motor tasks, with a prevalent use of third-person perspective. We present immersivePOV, an approach to film how-to videos from an immersive first-person perspective using a head-mounted 360 action camera. immersivePOV how-to videos can be viewed in a Virtual Reality headset, giving the viewer an eye-level viewpoint with three Degrees of Freedom. We evaluated our approach with two everyday motor tasks against a baseline first-person perspective and a third-person perspective. In a between-subjects study, participants were assigned to watch the task videos and then replicate the tasks. Results suggest that immersivePOV reduced perceived cognitive load and facilitated task learning. We discuss how immersivePOV can also streamline the video production process for content creators. Altogether, we conclude that",immersivePOV: Filming How-To Videos with a Head-Mounted 360 Action Camera,2022-04-27 00:00:00
368,Tovi Grossman,"Online synchronous tutoring allows for immediate engagement between instructors and audiences over distance. However, tutoring physical skills remains challenging because current telepresence approaches may not allow for adequate spatial awareness, viewpoint control of the demonstration activities scattered across an entire work area, and the instructor’s sufficient awareness of the audience. We present Asteroids, a novel approach for tangible robotic telepresence, to enable workbench-scale physical embodiments of remote people and tangible interactions by the instructor. With Asteroids, the audience can actively control a swarm of mini-telepresence robots, change camera positions, and switch to other robots’ viewpoints. Demonstrators can perceive the audiences’ physical presence while using tangible manipulations to control the audience’s viewpoints and presentation flow. We conducted an",ASTEROIDS: Exploring Swarms of Mini-Telepresence Robots for Physical Skill Demonstration,2021-09-29 00:00:00
369,Tovi Grossman,"A method for traversing a streaming video file includes receiving a representative streaming video file that includes less information than a higher-resolution streaming video file and spans the entire streaming video file. Based on navigation information associated with the representative streaming video file, a playback engine navigates to a different portion of the streaming video file. The navigation information may be based on input information received from a viewer of the streaming video file. One advantage of the disclosed method is that it enables fast and accurate navigation of a streaming video.",Real-time scrubbing of online videos,2022-04-26 00:00:00
370,Tovi Grossman,"One embodiment of the present invention sets forth a technique for performing tasks associated with a construction project. The technique includes transmitting to a worker, via a mobile computing device worn by the worker, a first instruction related to performing a first task included in a plurality of tasks associated with a construction project, and transmitting to a light-emitting device a command to provide a visual indicator to the worker that facilitates performing the first task, based on an input received from the mobile computing device, determining that the worker has completed the first task of the construction project, selecting, from a database that tracks eligibility of each of the plurality of tasks, a second task included in the plurality of tasks that the worker is eligible to perform, and transmitting to the worker, via the mobile computing device, a second instruction related to performing the second task.",Automated supervision of construction operations in an intelligent workspace,2022-04-05 00:00:00
371,Tovi Grossman,"Step-by-step tutorials have emerged as a key means for learning complex software, but they are typically designed for individuals learning independently. In contrast, cooperative learning, where learners can help each other as they work, is a fundamental pedagogical technique with many established benefits. To extend these benefits to learning 3D-design software, this work investigates the design of remote cooperative software tutorial systems. We first conduct an observational study of pairs of participants working on 3D-design tutorials, which reveals a range of potential benefits, challenges, and strategies for cooperation. Our findings inform the design of TwoTorials, a cooperative step-by-step tutorial system that helps pairs of remote users establish shared 3D context, maintain awareness of each other’s activities, and coordinate their efforts. A user study reveals several benefits to this approach, including enhanced cooperation between learners, reduced effort and mental demand, increased awareness of peer activities, and higher subjective engagement with the tutorial.",TwoTorials: A Remote Cooperative Tutorial System for 3D Design Software,2022-03-27 00:00:00
372,Tovi Grossman,"A hybrid workstation enables a virtual reality (VR) interface, a traditional (TD) interface, and transitions between the interfaces. The VR interface comprises three-dimensional (3D)-based software and hardware components. The TD interface comprises two-dimensional (2D)-based software and hardware components. The state of the hybrid workstation is defined by three parameters comprising interface (VR interface or TD interface), position (seated or standing), and movement (stationary or room-scale). The hybrid workstation detects a transition from a current state to a next state upon determining that any of the three parameters have changed. The hybrid workstation then determines a transition response based on the particular transition that is detected. The transition response comprises a set of operations that are performed on the VR interface and/or the TD interface that mitigate the disruption and inefficiency ",Transitions between states in a hybrid virtual reality desktop computing environment,2022-03-03 00:00:00
373,Tovi Grossman,A technique for generating designs includes: causing one or more candidate designs to be displayed within a virtual-reality (VR) environment; receiving a user input associated with a first candidate design included in the one or more candidate designs via the VR environment; generating a modified design based at least on the user input and the first candidate design; and generating a plurality of output designs via a generative design process based on the modified design.,Artificial intelligence-based techniques for design generation in virtual environments,2022-03-03 00:00:00
374,Tovi Grossman,"Via no-handed inputs to a smartwatch, an end-user can initiate various smartwatch operations when either or both hands of the end-user are occupied. The smartwatch includes a suite of sensors for capturing inertial, acoustic, and optical data. The smartwatch also interfaces with a handheld mobile computing device and/or a shoe-mounted pedometry device to capture additional sensor data. A control engine executing on the smartwatch processes captured sensor data and identifies no-handed inputs performed by the end-user. The control engine maps these no-handed inputs to specific commands that can be executed on the smartwatch to initiate smartwatch operations.",No-handed smartwatch interaction techniques,2022-03-01 00:00:00
375,Tovi Grossman,"An electronic device tracks, for a user performing a target acquisition movement within a 3D space, movement parameters of a plurality of input devices of the user. The electronic device predicts, for the user, a region of interest within the 3D space, using a regression model, based on the movement parameters. The region of interest includes a plurality of targets in close proximity. The electronic device predicts an endpoint of the target acquisition movement, within the region of interest, using a pointer facilitation technique. In some embodiments, the plurality of input devices includes an eye tracking input device, each input device corresponds to a predefined input device type, and the movement parameters include gaze data from the eye tracking input device. In some embodiments, input devices includes an eye tracking input device, a head-mounted display, and a hand-held controller, and the user's eye, hand",Multimodal kinematic template matching and regression modeling for ray pointing prediction in virtual reality,2022-02-22 00:00:00
376,Tovi Grossman,"Information regarding application usage on an actor device may be provided through activity notifications and activity reports. An activity notification describing current application activity on an actor device is sent, via wireless connection, to an observer device which displays the activity notification. Activity notifications provide different granularity levels of information based on a received level selection or based on a distance (proximity) between the actor device and the observer device. An activity report representing the history of application usage on an actor device may be displayed on the actor device. For example, the activity report may be triggered to by displayed when the actor device is placed flat. The activity report provides a graphical representation of the application usage on the actor device for a predetermined time period of prior usage. The graphical representation may comprise a plurality of stripes ",Sharing computer application activities,2021-11-30 00:00:00
377,Tovi Grossman,learning musical instruments using online instructional videos has become increasingly prevalent however prerecorded videos lack the instantaneous feedback and personal tailoring that human tutors provide in addition existing video navigations are not optimized for instrument learning making the learning experience encumbered guided by our formative interviews with guitar players and prior literature we designed soloist a mixedinitiative learning framework that automatically generates customizable curriculums from offtheshelf guitar video lessons soloist takes raw videos as input and leverages deeplearning based audio processing to extract musical information this backend processing is used to provide an interactive visualization to support effective video navigation and realtime feedback on the users performance creating a guided learning experience we demonstrate the capabilities and specific usecases of soloist within the domain of learning electric guitar solos using instructional youtube videos a remote user study conducted to gather feedback from guitar players shows encouraging results as the users unanimously preferred learning with soloist over unconverted instructional videos less,Soloist: Generating Mixed-Initiative Tutorials from Existing Guitar Instructional Videos Through Audio Processing,"21 January, 2021"
378,Vassos Hadzilacos,we consider the problem of implementing linearizable objects that support both read and readmodifywrite rmw operations in messagepassing systems with process crashes since in many systems read operations vastly outnumber rmw operations we are interested in implementations that emphasize the efficiency of read operations we present a parametrized algorithm for partially synchronous systems where processes have access to external clocks that are synchronized within with this algorithm every read operation is local intuitively it does not trigger messages if a read is not concurrent with a conflicting rmw it is performed immediately with no waiting furthermore even with a concurrent conflicting rmw a read experiences very little delay in the worstcase for example the algorithms parameters can be set to ensure that every read takes time in the worstcase to the best of our knowledge this is the first algorithm to achieve this bound in the partially synchronous systems that we assume here our parametrized algorithm generalizes the nonparameterized leasebased algorithm of chandra et al where the worstcase time for reads is where is the maximum message delay the algorithms parameters can be used to tradeoff the worstcase times for read and rmw operations they can also be used to take advantage of the fact that in many messagepassing systems the delay of most messages is order of magnitudes smaller than the maximum message delay for example the parameters can be set so that in nice periods where message delays are ll reads take at most time while rmws take at most time less,Parameterized algorithm for replicated objects with local reads,"4 April, 2022"
379,Vassos Hadzilacos,in this paper we first propose a new liveness requirement for shared objects and data structures we then give a shared queue algorithm that satisfies this requirement and we prove its correctness we also implement this algorithm and compare it to a wellknown shared queue algorithm that is used in practice in addition to having a stronger worstcase progress guarantee our experimental results suggest that at the cost of a marginal decrease in throughput our algorithm is significantly fairer by a natural definition of fairness that we introduce here less,Differentiated nonblocking: a new progress condition and a matching queue algorithm,"22 March, 2021"
380,Vassos Hadzilacos,motivated by recent distributed systems technology aguilera et al introduced a hybrid model of distributed computing called messageandmemory model or mm model for short in this model processes can communicate by message passing and also by accessing some shared memory eg through some rdma connections we first consider the basic problem of implementing an atomic singlewriter multireader swmr register shared by all the processes in mm systems specifically we give an algorithm that implements such a register in mm systems and show that it is optimal in the number of process crashes that it can tolerate this generalizes the wellknown implementation of an atomic swmr register in a pure messagepassing system we then combine our register implementation for mm systems with the wellknown randomized consensus algorithm of aspnes and herlihy and obtain a randomized consensus algorithm for mm systems that is also optimal in the number of process crashes that it can tolerate finally we determine the minimum number of rdma connections that is sufficient to implement a swmr register or solve randomized consensus in an mm system with t process crashes for any given t less,On Atomic Registers and Randomized Consensus in M&M Systems,"13 December, 2020"
381,Vassos Hadzilacos,"It is well-known that, for deterministic algorithms, linearizable objects can be used as if they were atomic objects. As pointed out by Golab, Higham, and Woelfel, however, a randomized algorithm that works with atomic objects may lose some of its properties if we replace the atomic objects that it uses with objects that are only linearizable. It was not known whether the properties that can be lost include the all-important property of termination (with probability 1). In this paper, we first show that a randomized algorithm can indeed lose its termination property if we replace the atomic registers that it uses with linearizable ones.",On Register Linearizability and Termination,23 July 2021
382,Vassos Hadzilacos,"In this paper, we first propose a new liveness requirement for shared objects and data structures, we then give a shared queue algorithm that satisfies this requirement and we prove its correctness. We also implement this algorithm and compare it to a well-known shared queue algorithm that is used in practice. In addition to having a stronger worst-case progress guarantee, our experimental results suggest that, at the cost of a marginal decrease in throughput, our algorithm is significantly fairer, by a natural definition of fairness that we introduce here.",Differentiated nonblocking: a new progress condition and a matching queue algorithm,22 Mar 2021
383,Vassos Hadzilacos,"We prove that in asynchronous message-passing systems where at most one process may crash, there is no lock-free strongly linearizable implementation of a weak object that we call Test-or-Set (ToS). This object allows a single distinguished process to apply the set operation once, and a different distinguished process to apply the test operation also once. Since this weak object can be directly implemented by a single-writer single-reader (SWSR) register (and other common objects such as max-register, snapshot and counter), this result implies that there is no 1-resilient lock-free strongly linearizable implementation of a SWSR register (and of these other objects) in message-passing systems.",An Impossibility Result on Strong Linearizability in Message-Passing Systems, 3 Aug 2021
384,Vassos Hadzilacos,the wellknown randomized consensus algorithm by aspnes and herlihy for asynchronous sharedmemory systems was proved to work even against a strong adversary under the assumption that the registers that it uses are atomic registers with atomic registers every read or write operation is instantaneous and thus indivisible as pointed out by golab et al however a randomized algorithm that works with atomic registers does not necessarily work if we replace the atomic registers that it uses with linearizable implementations of registers this raises the following question does the randomized consensus algorithm by aspnes and herlihy still work against a strong adversary if we replace its atomic registers with linearizable registers we show that the answer is affirmative in fact we show that even linearizable registers are not necessary more precisely we prove that the algorithm by aspnes and herlihy works against a strong adversary even if the algorithm uses only regular registers less,Randomized Consensus with Regular Registers,"11 June, 2020"
385,Graeme Hirst,we present the project dialogism novel corpus or pdnc an annotated dataset of quotations for english literary texts pdnc contains annotations for quotations across fulllength novels and is by an order of magnitude the largest corpus of its kind each quotation is annotated for the speaker addressees type of quotation referring expression and character mentions within the quotation text the annotated attributes allow for a comprehensive evaluation of models of quotation attribution and coreference for literary texts less,The Project Dialogism Novel Corpus: A Dataset for Quotation Attribution in Literary Texts,"12 April, 2022"
386,Graeme Hirst,we present a textbased framework for investigating moral sentiment change of the public via longitudinal corpora our framework is based on the premise that language use can inform peoples moral perception toward right or wrong and we build our methodology by exploring moral biases learned from diachronic word embeddings we demonstrate how a parameterfree model supports inference of historical shifts in moral sentiment toward concepts such as slavery and democracy over centuries at three incremental levels moral relevance moral polarity and finegrained moral dimensions we apply this methodology to visualizing moral time courses of individual concepts and analyzing the relations between psycholinguistic variables and rates of moral sentiment change at scale our work offers opportunities for applying natural language processing toward characterizing moral sentiment change in society less,Text-based inference of moral sentiment change,"20 January, 2020"
387,Graeme Hirst,a surprising property of word vectors is that word analogies can often be solved with vector arithmetic however it is unclear why arithmetic operators correspond to nonlinear embedding models such as skipgram with negative sampling sgns we provide a formal explanation of this phenomenon without making the strong assumptions that past theories have made about the vector space and word distribution our theory has several implications past work has conjectured that linear substructures exist in vector spaces because relations can be represented as ratios we prove that this holds for sgns we provide novel justification for the addition of sgns word vectors by showing that it automatically downweights the more frequent word as weighting schemes do ad hoc lastly we offer an information theoretic interpretation of euclidean distance in vector spaces justifying its use in capturing word dissimilarity less,Towards Understanding Linear Word Analogies,"12 August, 2019"
388,Graeme Hirst,current approaches to crosslingual sentiment analysis try to leverage the wealth of labeled english data using bilingual lexicons bilingual vector space embeddings or machine translation systems here we show that it is possible to use a single linear transformation with as few as word pairs to capture finegrained sentiment relationships between words in a crosslingual setting we apply these crosslingual sentiment models to a diverse set of tasks to demonstrate their functionality in a nonenglish context by effectively leveraging english sentiment knowledge without the need for accurate translation we can analyze and extract features from other languages with scarce data at a very low cost thus making sentiment and related analyses for many languages inexpensive less,Cross-Lingual Sentiment Analysis Without (Good) Translation,"24 October, 2017"
389,Graeme Hirst,previous attempts at rststyle discourse segmentation typically adopt features centered on a single token to predict whether to insert a boundary before that token in contrast we develop a discourse segmenter utilizing a set of pairing features which are centered on a pair of adjacent tokens in the sentence by equally taking into account the information from both tokens moreover we propose a novel set of global features which encode characteristics of the segmentation as a whole once we have an initial segmentation we show that both the pairing and global features are useful on their own and their combination achieved an f of of identifying insentence discourse boundaries which is a errorrate reduction over the stateoftheart performance approaching of human performance in addition similar improvement is observed across different classification frameworks less,Two-pass Discourse Segmentation with Pairing and Global Features,"30 July, 2014"
390,Graeme Hirst,the automatic ranking of word pairs as per their semantic relatedness and ability to mimic human notions of semantic relatedness has widespread applications measures that rely on raw data distributional measures and those that use knowledgerich ontologies both exist although extensive studies have been performed to compare ontological measures with human judgment the distributional measures have primarily been evaluated by indirect means this paper is a detailed study of some of the major distributional measures it lists their respective merits and limitations new measures that overcome these drawbacks that are more in line with the human notions of semantic relatedness are suggested the paper concludes with an exhaustive comparison of the distributional and ontologybased measures along the way significant research problems are identified work on these problems may lead to a better understanding of how semantic relatedness is to be measured less,Distributional Measures as Proxies for Semantic Relatedness,"8 March, 2012"
391,Graeme Hirst,i take issue with ai formalizations of context primarily the formalization by mccarthy and buvac that regard context as an undefined primitive whose formalization can be the same in many different kinds of ai tasks in particular any theory of context in natural language must take the special nature of natural language into account and cannot regard context simply as an undefined primitive i show that there is no such thing as a coherent theory of context simpliciter context pure and simple and that context in natural language is not the same kind of thing as context in kr in natural language context is constructed by the speaker and the interpreter and both have considerable discretion in so doing therefore a formalization based on predefined contexts and predefined lifting axioms cannot account for how context is used in realworld language less,Context as a Spurious Concept,"9 December, 1997"
392,Graeme Hirst,drawing appropriate defeasible inferences has been proven to be one of the most pervasive puzzles of natural language processing and a recurrent problem in pragmatics this paper provides a theoretical framework called stratified logic that can accommodate defeasible pragmatic inferences the framework yields an algorithm that computes the conversational conventional scalar clausal and normal state implicatures and the presuppositions that are associated with utterances the algorithm applies equally to simple and complex utterances and sequences of utterances less,A Uniform Treatment of Pragmatic Inferences in Simple and Complex Utterances and Sequences of Utterances,"25 April, 1995"
393,Alec Jacobson,drawing a direct analogy with the wellstudied vibration or elastic modes we introduce an objects fracture modes which constitute its preferred or most natural ways of breaking we formulate a sparsified eigenvalue problem which we solve iteratively to obtain the n lowestenergy modes these can be precomputed for a given shape to obtain a prefracture pattern that can substitute the state of the art for realtime applications at no runtime cost but significantly greater realism furthermore any realtime impact can be projected onto our modes to obtain impactdependent fracture patterns without the need for any online crack propagation simulation we not only introduce this theoretically novel concept but also show its fundamental and practical superiority in a diverse set of examples and contexts less,Breaking Good: Fracture Modes for Realtime Destruction,"4 July, 2022"
394,Alec Jacobson,neural implicit representations which encode a surface as the level set of a neural network applied to spatial coordinates have proven to be remarkably effective for optimizing compressing and generating d geometry although these representations are easy to fit it is not clear how to best evaluate geometric queries on the shape such as intersecting against a ray or finding a closest point the predominant approach is to encourage the network to have a signed distance property however this property typically holds only approximately leading to robustness issues and holds only at the conclusion of training inhibiting the use of queries in loss functions instead this work presents a new approach to perform queries directly on general neural implicit functions for a wide range of existing architectures our key tool is the application of range analysis to neural networks using automatic arithmetic rules to bound the output of a network over a region we conduct a study of range analysis on neural networks and identify variants of affine arithmetic which are highly effective we use the resulting bounds to develop geometric queries including ray casting intersection testing constructing spatial hierarchies fast mesh extraction closestpoint evaluation evaluating bulk properties and more our queries can be efficiently evaluated on gpus and offer concrete accuracy guarantees even on randomlyinitialized networks enabling their use in training objectives and beyond we also show a preliminary application to inverse rendering less,Spelunking the Deep: Guaranteed Queries on General Neural Implicit Surfaces via Range Analysis,"24 June, 2022"
395,Alec Jacobson,learning to autonomously assemble shapes is a crucial skill for many robotic applications while the majority of existing part assembly methods focus on correctly posing semantic parts to recreate a whole object we interpret assembly more literally as mating geometric parts together to achieve a snug fit by focusing on shape alignment rather than semantic cues we can achieve acrosscategory generalization in this paper we introduce a novel task pairwise d geometric shape mating and propose neural shape mating nsm to tackle this problem given the point clouds of two object parts of an unknown category nsm learns to reason about the fit of the two parts and predict a pair of d poses that tightly mate them together we couple the training of nsm with an implicit shape reconstruction task to make nsm more robust to imperfect point cloud observations to train nsm we present a selfsupervised data collection pipeline that generates pairwise shape mating data with ground truth by randomly cutting an object mesh into two parts resulting in a dataset that consists of k shape mating pairs from numerous object meshes with diverse cut types we train nsm on the collected dataset and compare it with several point cloud registration methods and one part assembly baseline extensive experimental results and ablation studies under various settings demonstrate the effectiveness of the proposed algorithm additional materialis available at httpsneuralshapematinggithubio less,Neural Shape Mating: Self-Supervised Object Assembly with Adversarial Shape Priors,"30 May, 2022"
396,Alec Jacobson,neural implicit fields have recently emerged as a useful representation for d shapes these fields are commonly represented as neural networks which map latent descriptors and d coordinates to implicit function values the latent descriptor of a neural field acts as a deformation handle for the d shape it represents thus smoothness with respect to this descriptor is paramount for performing shapeediting operations in this work we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the fields lipschitz constant compared with prior lipschitz regularized networks ours is computationally fast can be implemented in four lines of code and requires minimal hyperparameter tuning for geometric applications we demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from d point clouds showing both qualitative and quantitative improvements over existing stateoftheart and nonregularized baselines less,Learning Smooth Neural Functions via Lipschitz Regularization,"10 May, 2022"
397,Alec Jacobson,communicating linear algebra in written form is challenging mathematicians must choose between writing in languages that produce wellformatted but semanticallyunderdefined representations such as latex or languages with welldefined semantics but notation unlike conventional math such as ceigen in both cases the underlying linear algebra is obfuscated by the requirements of esoteric language syntax as in latex or awkward apis due to language semantics as in c the gap between representations results in communication challenges including underspecified and irreproducible research results difficulty teaching math concepts underlying complex numerical code as well as repeated redundant and errorprone translations from communicated linear algebra to executable code we introduce iheartsuitla a language with syntax designed to closely mimic conventionallywritten linear algebra while still ensuring an unambiguous compilable interpretation inspired by markdown a language for writing naturallystructured plain text files that translate into valid html iheartsuitla allows users to write linear algebra in text form and compile the same source into latex ceigen pythonnumpyscipy and matlab with easy extension to further math programming environments we outline the principles of our language design and highlight design decisions that balance between readability and precise semantics and demonstrate through case studies the ability for iheartsuitla to bridge the semantic gap between conventionallywritten linear algebra and unambiguous interpretation in math programming environments less,I$\heartsuit$LA: Compilable Markdown for Linear Algebra,"24 September, 2021"
398,Alec Jacobson,we present a new approach for modelling musculoskeletal anatomy unlike previous methods we do not model individual muscle shapes as geometric primitives polygonal meshes nurbs etc instead we adopt a volumetric segmentation approach where every point in our volume is assigned to a muscle fat or bone tissue we provide an interactive modelling tool where the user controls the segmentation via muscle curves and we visualize the muscle shapes using volumetric rendering muscle curves enable intuitive yet powerful control over the muscle shapes this representation allows us to automatically handle intersections between different tissues musclemuscle musclebone and muscleskin during the modelling and automates computation of muscle fiber fields we further introduce a novel algorithm for converting the volumetric muscle representation into tetrahedral or surface geometry for use in downstream tasks additionally we introduce an interactive skeleton authoring tool that allows the users to create skeletal anatomy starting from only a skin mesh using a library of bone parts less,Interactive Modelling of Volumetric Musculoskeletal Anatomy,"9 June, 2021"
399,Alec Jacobson,we propose a novel algorithm to efficiently generate hidden structures to support arrangements of floating rigid objects our optimization finds a small set of rods and wires between objects and each other or a supporting surface eg wall or ceiling that hold all objects in force and torque equilibrium our objective function includes a sparsity inducing total volume term and a linear visibility term based on efficiently precomputed montecarlo integration to encourage solutions that are ashiddenaspossible the resulting optimization is convex and the global optimum can be efficiently recovered via a linear program our representation allows for a usercontrollable mixture of tension compression and shearresistant rods or tensiononly wires we explore applications to theatre set design museum exhibit curation and other artistic endeavours less,Levitating Rigid Objects with Hidden Rods and Wires,"9 February, 2021"
400,Alec Jacobson,emu is an efficient and scalable model to simulate bulk musculoskeletal motion with heterogenous materials first emu requires no model reductions or geometric coarsening thereby producing results visually accurate when compared to an fem simulation second emu is efficient and scales much better than stateoftheart fem with the number of elements in the mesh and is more easily parallelizable third emu can handle heterogeneously stiff meshes with an arbitrary constitutive model thus allowing it to simulate soft muscles stiff tendons and even stiffer bones all within one unified system these three key characteristics of emu enable us to efficiently orchestrate muscle activated skeletal movements we demonstrate the efficacy of our approach via a number of examples with tendons muscles bones and joints less,EMU: Efficient Muscle Simulation In Deformation Space,"19 November, 2020"
401,Alec Jacobson,we introduce a novel solver to significantly reduce the size of a geometric operator while preserving its spectral properties at the lowest frequencies we use chordal decomposition to formulate a convex optimization problem which allows the user to control the operator sparsity pattern this allows for a tradeoff between the spectral accuracy of the operator and the cost of its application we efficiently minimize the energy with a change of variables and achieve stateoftheart results on spectral coarsening our solver further enables novel applications including volumetosurface approximation and detaching the operator from the mesh ie one can produce a mesh tailormade for visualization and optimize an operator separately for computation less,Chordal Decomposition for Spectral Coarsening,"14 September, 2020"
402,Alec Jacobson,we present a d stylization algorithm that can turn an input shape into the style of a cube while maintaining the content of the original shape the key insight is that cubic style sculptures can be captured by the asrigidaspossible energy with an lregularization on rotated surface normals minimizing this energy naturally leads to a detailpreserving cubic geometry our optimization can be solved efficiently without any mesh surgery our method serves as a nonrealistic modeling tool where one can incorporate many artistic controls to create stylized geometries less,Cubic Stylization,"27 June, 2020"
403,Alec Jacobson,this paper introduces neural subdivision a novel framework for datadriven coarsetofine geometry modeling during inference our method takes a coarse triangle mesh as input and recursively subdivides it to a finer geometry by applying the fixed topological updates of loop subdivision but predicting vertex positions using a neural network conditioned on the local geometry of a patch this approach enables us to learn complex nonlinear subdivision schemes beyond simple linear averaging used in classical techniques one of our key contributions is a novel selfsupervised training setup that only requires a set of highresolution meshes for learning network weights for any training shape we stochastically generate diverse lowresolution discretizations of coarse counterparts while maintaining a bijective mapping that prescribes the exact target position of every new vertex during the subdivision process this leads to a very efficient and accurate loss function for conditional mesh generation and enables us to train a method that generalizes across discretizations and favors preserving the manifold structure of the output during training we optimize for the same set of network weights across all local mesh patches thus providing an architecture that is not constrained to a specific input mesh fixed genus or category our network encodes patch geometry in a local frame in a rotation and translationinvariant manner jointly these design choices enable our method to generalize well and we demonstrate that even when trained on a single highresolution mesh our method generates reasonable subdivisions for novel shapes less,Neural Subdivision,"4 May, 2020"
404,Alec Jacobson,in this technical report we investigate efficient representations of articulated objects eg human bodies which is an important problem in computer vision and graphics to deform articulated geometry existing approaches represent objects as meshes and deform them using skinning techniques the skinning operation allows a wide range of deformations to be achieved with a small number of control parameters this paper introduces a method to invert the deformations undergone via traditional skinning techniques via a neural network parameterized by pose the ability to invert these deformations allows values eg distance function signed distance function occupancy to be precomputed at rest pose and then efficiently queried when the character is deformed we leave empirical evaluation of our approach to future work less,NiLBS: Neural Inverse Linear Blend Skinning,"6 April, 2020"
405,Alec Jacobson,computer animation in conjunction with d printing has the potential to positively impact traditional stopmotion animation as d printing every frame of a computer animation is prohibitively slow and expensive d printed stopmotion can only be viable if animations can be faithfully reproduced using a compact library of d printed and efficiently assemblable parts we thus present the first system for processing computer animation sequences typically faces to produce an optimal set of replacement parts for use in d printed stopmotion animation given an input animation sequence of topology invariant deforming meshes our problem is to output a library of replacement parts and peranimationframe assignment of the parts such that we maximally approximate the input animation while minimizing the amount of d printing and assembly inspired by current stopmotion workflows a user manually indicates which parts of the model are preferred for segmentation then we find curves with minimal deformation along which to segment the mesh we then present a novel algorithm to zero out deformations along the segment boundaries so that replacement sets for each part can be interchangeably and seamlessly assembled together the part boundaries are designed to ease d printing and instrumentation for assembly each part is then independently optimized using a graphcut technique to find a set of replacements whose size can be user defined or automatically computed to adhere to a printing budget or allowed deviation from the original animation our evaluation is threefold we show results on a variety of facial animations both digital and d printed critiqued by a professional animator we show the impact of various algorithmic parameters and compare our results to naive solutions our approach can reduce the printing time and cost significantly for stopmotion animated films less,A system for efficient 3D printed stop-motion face animation,"23 July, 2019"
406,Alec Jacobson,we introduce a novel approach to measure the behavior of a geometric operator before and after coarsening by comparing eigenvectors of the input operator and its coarsened counterpart we can quantitatively and visually analyze how well the spectral properties of the operator are maintained using this measure we show that standard mesh simplification and algebraic coarsening techniques fail to maintain spectral properties in response we introduce a novel approach for spectral coarsening we show that it is possible to significantly reduce the sampling density of an operator derived from a d shape without affecting the lowfrequency eigenvectors by marrying techniques developed within the algebraic multigrid and the functional maps literatures we successfully coarsen a variety of isotropic and anisotropic operators while maintaining sparsity and positive semidefiniteness we demonstrate the utility of this approach for applications including operatorsensitive sampling shape matching and graph pooling for convolutional neural networks less,Spectral Coarsening of Geometric Operators,"8 May, 2019"
407,Alec Jacobson,we present the first algorithm for designing volumetric michell trusses our method uses a parametrization approach to generate trusses made of structural elements aligned with the primary direction of an objects stress field such trusses exhibit high strengthtoweight ratios we demonstrate the structural robustness of our designs via a posteriori physical simulation we believe our algorithm serves as an important complement to existing structural optimization tools and as a novel standalone design tool itself less,Designing Volumetric Truss Structures,"28 October, 2018"
408,Alec Jacobson,in geometry processing smoothness energies are commonly used to model scattered data interpolation dense data denoising and regularization during shape optimization the squared laplacian energy is a popular choice of energy and has a corresponding standard implementation squaring the discrete laplacian matrix for compact domains when values along the boundary are not known in advance this construction bakes in loworder boundary conditions this causes the geometric shape of the boundary to strongly bias the solution for many applications this is undesirable instead we propose using the squared frobenious norm of the hessian as a smoothness energy unlike the squared laplacian energy this energys natural boundary conditions those that best minimize the energy correspond to meaningful highorder boundary conditions these boundary conditions model free boundaries where the shape of the boundary should not bias the solution locally our analysis begins in the smooth setting and concludes with discretizations using finitedifferences on d grids or mixed finite elements for triangle meshes we demonstrate the core behavior of the squared hessian as a smoothness energy for various tasks less,Natural Boundary Conditions for Smoothing in Geometry Processing,"13 July, 2017"
409,Alec Jacobson,the generalized winding number function measures insideness for arbitrary oriented triangle meshes exploiting this i similarly generalize binary boolean operations to act on such meshes the resulting operations for union intersection difference etc avoid volumetric discretization or preprocessing less,Boolean Operations using Generalized Winding Numbers,"28 January, 2016"
410,Nick Koudas,the problem of identifying the kshortest paths ksps for short in a dynamic road network is essential to many locationbased services road networks are dynamic in the sense that the weights of the edges in the corresponding graph constantly change over time representing evolving traffic conditions very often such services have to process numerous ksp queries over large road networks at the same time thus there is a pressing need to identify distributed solutions for this problem however most existing approaches are designed to identify ksps on a static graph in a sequential manner ie the ith shortest path is generated based on the ith shortest path restricting their scalability and applicability in a distributed setting we therefore propose kspdg a distributed algorithm for identifying kshortest paths in a dynamic graph it is based on partitioning the entire graph into smaller subgraphs and reduces the problem of determining ksps into the computation of partial ksps in relevant subgraphs which can execute in parallel on a cluster of servers a distributed twolevel index called dtlp is developed to facilitate the efficient identification of relevant subgraphs a salient feature of dtlp is that it indexes a set of virtual paths that are insensitive to varying traffic conditions leading to very low maintenance cost in dynamic road networks this is the first treatment of the problem of processing ksp queries over dynamic road networks extensive experiments conducted on real road networks confirm the superiority of our proposal over baseline methods less,Distributed Processing of k Shortest Path Queries over Dynamic Road Networks,"13 May, 2022"
411,Nick Koudas,set similarity search is a problem of central interest to a wide variety of applications such as data cleaning and web search past approaches on set similarity search utilize either heavy indexing structures incurring large search costs or indexes that produce large candidate sets in this paper we design a learningbased exact set similarity search approach les our approach first partitions sets into groups and then utilizes a lightweight bitmaplike indexing structure called tokengroup matrix tgm to organize groups and prune out candidates given a query set in order to optimize pruning using the tgm we analytically investigate the optimal partitioning strategy under certain distributional assumptions using these results we then design a learningbased partitioning approach called lp and an associated data representation encoding ptr to identify the partitions we conduct extensive experiments on real and synthetic datasets to fully study les establishing the effectiveness and superiority over other applicable approaches less,LES3: Learning-based Exact Set Similarity Search,"21 July, 2021"
412,Nick Koudas,machine learning ml applications are proliferating in the enterprise relational data which are prevalent in enterprise applications are typically normalized as a result data has to be denormalized via primaryforeignkey joins to be provided as input to ml algorithms in this paper we study the implementation of popular nonlinear ml models gaussian mixture models gmm and neural networks nn over normalized data addressing both cases of binary and multiway joins over normalized relations for the case of gmm we show how it is possible to decompose computation in a systematic way both for binary joins and for multiway joins to construct mixture models we demonstrate that by factoring the computation one can conduct the training of the models much faster compared to other applicable approaches without any loss in accuracy for the case of nn we propose algorithms to train the network taking normalized data as the input similarly we present algorithms that can conduct the training of the network in a factorized way and offer performance advantages the redundancy introduced by denormalization can be exploited for certain types of activation functions however we demonstrate that attempting to explore this redundancy is helpful up to a certain point exploring redundancy at higher layers of the network will always result in increased costs and is not recommended we present the results of a thorough experimental evaluation varying several parameters of the input relations involved and demonstrate that our proposals for the training of gmm and nn yield drastic performance improvements typically starting at which become increasingly higher as parameters of the underlying data vary without any loss in accuracy less,Efficient Construction of Nonlinear Models over Normalized Data,"19 March, 2021"
413,Nick Koudas,recent advances in computer vision and deep learning made possible the efficient extraction of a schema from frames of streaming video as such a stream of objects and their associated classes along with unique object identifiers derived via object tracking can be generated providing unique objects as they are captured across frames in this paper we initiate a study of temporal queries involving objects and their cooccurrences in video feeds for example queries that identify video segments during which the same two red cars and the same two humans appear jointly for five minutes are of interest to many applications ranging from law enforcement to security and safety we take the first step and define such queries in a way that they incorporate certain physical aspects of video capture such as object occlusion we present an architecture consisting of three layers namely object detectiontracking intermediate data generation and query evaluation we propose two techniquesmfs and ssg to organize all detected objects in the intermediate data generation layer which effectively given the queries minimizes the number of objects and frames that have to be considered during query evaluation we also introduce an algorithm called state traversal st that processes incoming frames against the ssg and efficiently prunes objects and frames unrelated to query evaluation while maintaining all states required for succinct query evaluation we present the results of a thorough experimental evaluation utilizing both real and synthetic data establishing the tradeoffs between mfs and ssg we stress various parameters of interest in our evaluation and demonstrate that the proposed query evaluation methodology coupled with the proposed algorithms is capable to evaluate temporal queries over video feeds efficiently achieving orders of magnitude performance benefits less,Evaluating Temporal Queries Over Video Feeds,"5 March, 2020"
414,Nick Koudas,data is generated at an unprecedented rate surpassing our ability to analyze them the database community has pioneered many novel techniques for approximate query processing aqp that could give approximate results in a fraction of time needed for computing exact results in this work we explore the usage of deep learning dl for answering aggregate queries specifically for interactive applications such as data exploration and visualization we use deep generative models an unsupervised learning based approach to learn the data distribution faithfully such that aggregate queries could be answered approximately by generating samples from the learned model the model is often compact few hundred kbs so that arbitrary aqp queries could be answered on the client side without contacting the database server our other contributions include identifying model bias and minimizing it through a rejection sampling based approach and an algorithm to build model ensembles for aqp for improved accuracy our extensive experiments show that our proposed approach can provide answers with high accuracy and low latency less,Approximate Query Processing using Deep Generative Models,"18 November, 2019"
415,Nick Koudas,peer to peer marketplaces such as airbnb enable transactional exchange of services directly between people in such platforms those providing a service hosts in airbnb are faced with various choices for example in airbnb although some amenities in a property attributes of the property are fixed others are relatively flexible and can be provided without significant effort providing an amenity is usually associated with a cost naturally different sets of amenities may have a different gains for a host consequently given a limited budget deciding which amenities attributes to offer is challenging in this paper we formally introduce and define the problem of gain maximization over flexible attributes gmfa we first prove that the problem is nphard and show that identifying an approximate algorithm with a constant approximate ratio is unlikely we then provide a practically efficient exact algorithm to the gmfa problem for the general class of monotonic gain functions which quantify the benefit of sets of attributes as the next part of our contribution we focus on the design of a practical gain function for gmfa we introduce the notion of frequentitem based count fbc which utilizes the existing tuples in the database to define the notion of gain and propose an efficient algorithm for computing it we present the results of a comprehensive experimental evaluation of the proposed techniques on real dataset from airbnb and demonstrate the practical relevance and utility of our proposal less,Assisting Service Providers In Peer-to-peer Marketplaces: Maximizing Gain Over Flexible Attributes,"6 October, 2017"
416,Nick Koudas,analysis of large data collections using popular machine learning and statistical algorithms has been a topic of increasing research interest a typical analysis workload consists of applying an algorithm to build a model on a data collection and subsequently refining it based on the results in this paper we introduce model materialization and incremental model reuse as first class citizens in the execution of analysis workloads we materialize built models instead of discarding them in a way that can be reused in subsequent computations at the same time we consider manipulating an existing model adding or deleting data from it in order to build a new one we discuss our approach in the context of popular machine learning models we specify the details of how to incrementally maintain models as well as outline the suitable optimizations required to optimally use models and their incremental adjustments to build new ones we detail our techniques for linear regression naive bayes and logistic regression and present the suitable algorithms and optimizations to handle these models in our framework we present the results of a detailed performance evaluation using real and synthetic data sets our experiments analyze the various trade offs inherent in our approach and demonstrate vast performance benefits less,Processing Analytical Workloads Incrementally,"16 September, 2015"
417,Nick Koudas,numerous generalization techniques have been proposed for privacy preserving data publishing most existing techniques however implicitly assume that the adversary knows little about the anonymization algorithm adopted by the data publisher consequently they cannot guard against privacy attacks that exploit various characteristics of the anonymization mechanism this paper provides a practical solution to the above problem first we propose an analytical model for evaluating disclosure risks when an adversary knows everything in the anonymization process except the sensitive values based on this model we develop a privacy principle transparent ldiversity which ensures privacy protection against such powerful adversaries we identify three algorithms that achieve transparent ldiversity and verify their effectiveness and efficiency through extensive experiments with real data less,Transparent Anonymization: Thwarting Adversaries Who Know the Algorithm,"26 March, 2010"
418,David Levin,we present a new method for computing a smooth minimum distance function based on the logsumexp function for point clouds edge meshes triangle meshes and combinations of all three we derive blending weights and a modified barneshut acceleration approach that ensure our method approximates the true distance and is conservative points outside the zero isosurface are guaranteed to be outside the surface and efficient to evaluate for all the above data types this in combination with its ability to smooth sparsely sampled and noisy data like point clouds shortens the gap between data acquisition and simulation and thereby enables new applications such as direct codimensional rigid body simulation using unprocessed lidar data less,Fast Evaluation of Smooth Distance Constraints on Co-Dimensional Geometry,"18 May, 2022"
419,David Levin,the compact muon solenoid collaboration is designing a new highgranularity endcap calorimeter hgcal to be installed later this decade as part of this development work a prototype system was built with an electromagnetic section consisting of doublesided structures providing sampling layers each sampling layer has an hexagonal module where a multipad largearea silicon sensor is glued between an electronics circuit board and a metal baseplate the sensor pads of approximately cm are wirebonded to the circuit board and are readout by custom integrated circuits the prototype was extensively tested with beams at cerns super proton synchrotron in based on the data collected with beams of positrons with energies ranging from to gev measurements of the energy resolution and linearity the position and angular resolutions and the shower shapes are presented and compared to a detailed geant simulation less,Response of a CMS HGCAL silicon-pad electromagnetic calorimeter prototype to 20-300 GeV positrons,"31 March, 2022"
420,David Levin,we present a markov chain on the ndimensional hypercube n which satisfies trm mix n o this markov chain alternates between random and deterministic moves and we prove that the chain has cutoff with a window of size at most on where the deterministic moves correspond to a linear shift register less,Fast mixing of a randomized shift-register Markov chain,"6 February, 2022"
421,David Levin,using bayesian analyses we study the solar electron density with the nanograv year pulsar timing array pta dataset our model of the solar wind is incorporated into a global fit starting from pulse timesofarrival we introduce new tools developed for this global fit including analytic expressions for solar electron column densities and open source models for the solar wind that port into existing pta software we perform an ab initio recovery of various solar wind model parameters we then demonstrate the richness of information about the solar electron density ne that can be gleaned from pta data including higher order corrections to the simple r model associated with a freestreaming wind which are informative probes of coronal acceleration physics quarterly binned measurements of ne and a continuous timevarying model for ne spanning approximately one solar cycle period finally we discuss the importance of our model for chromatic noise mitigation in gravitationalwave analyses of pulsar timing data and the potential of developing synergies between sophisticated pta solar electron density models and those developed by the solar physics community less,Bayesian Solar Wind Modeling with Pulsar Timing Arrays,"17 November, 2021"
422,David Levin,we describe directed searches for continuous gravitational waves in data from the sixth ligo science data run the targets were nine young supernova remnants not associated with pulsars eight of the remnants are associated with nonpulsing suspected neutron stars one targets parameters are uncertain enough to warrant two searches for a total of ten each search covered a broad band of frequencies and first and second frequency derivatives for a fixed sky direction the searches coherently integrated data from the two ligo interferometers over time spans from days using the matchedfiltering fstatistic we found no credible gravitationalwave signals we set confidence upper limits as strong low as times on intrinsic strain times on fiducial ellipticity and times on rmode amplitude these beat the indirect limits from energy conservation and are within the range of theoretical predictions for neutronstar ellipticities and rmode amplitudes less,Searches for continuous gravitational waves from nine young supernova remnants,"12 August, 2021"
423,David Levin,we consider a problem of great practical interest the repairing and recovery of a lowdimensional manifold embedded in highdimensional space from noisy scattered data suppose that we observe a point cloud sampled from the lowdimensional manifold with noise and let us assume that there are holes in the data can we recover missing information inside the holes while in lowdimension the problem was extensively studied manifold repairing in high dimension is still an open problem we introduce a new approach called repairing manifold locally optimal projection rmlop that expands the mlop method introduced by faigenbaumgolovin et al in to cope with manifold repairing in low and highdimensional cases the proposed method can deal with multiple holes in a manifold we prove the validity of the proposed method and demonstrate the effectiveness of our approach by considering different manifold topologies for single and multiple holes repairing in low and high dimensions less,"Manifold Repairing, Reconstruction and Denoising from Scattered Data in High-Dimension","2 February, 2021"
424,David Levin,emu is an efficient and scalable model to simulate bulk musculoskeletal motion with heterogenous materials first emu requires no model reductions or geometric coarsening thereby producing results visually accurate when compared to an fem simulation second emu is efficient and scales much better than stateoftheart fem with the number of elements in the mesh and is more easily parallelizable third emu can handle heterogeneously stiff meshes with an arbitrary constitutive model thus allowing it to simulate soft muscles stiff tendons and even stiffer bones all within one unified system these three key characteristics of emu enable us to efficiently orchestrate muscle activated skeletal movements we demonstrate the efficacy of our approach via a number of examples with tendons muscles bones and joints less,EMU: Efficient Muscle Simulation In Deformation Space,"19 November, 2020"
425,David Levin,in some applications one is interested in reconstructing a function f from its fourier series coefficients the problem is that the fourier series is slowly convergent if the function is nonperiodic or is nonsmooth in this paper we suggest a method for deriving high order approximation to f using a padlike method namely by fitting some fourier coefficients of the approximant to the given fourier coefficients of f given the fourier series coefficients of a function on a rectangular domain in mathbbrd assuming the function is piecewise smooth we approximate the function by piecewise high order spline functions first the singularity structure of the function is identified for example in the d case we find high accuracy approximation to the curves separating between smooth segments of f secondly simultaneously we find the approximations of all the different segments of f we start by developing and demonstrating a high accuracy algorithm for the d case and we use this algorithm to step up to the multidimensional case less,Reconstruction of piecewise-smooth multivariate functions from Fourier data,"12 April, 2020"
426,David Levin,in order to avoid the curse of dimensionality frequently encountered in big data analysis there was a vast development in the field of linear and nonlinear dimension reduction techniques in recent years these techniques sometimes referred to as manifold learning assume that the scattered input data is lying on a lower dimensional manifold thus the high dimensionality problem can be overcome by learning the lower dimensionality behavior however in real life applications data is often very noisy in this work we propose a method to approximate mathcalm a ddimensional cm smooth submanifold of mathbbrn d ll n based upon noisy scattered data points ie a data cloud we assume that the data points are located near the lower dimensional manifold and suggest a nonlinear moving leastsquares projection on an approximating ddimensional manifold under some mild assumptions the resulting approximant is shown to be infinitely smooth and of high approximation order ie ohm where h is the fill distance and m is the degree of the local polynomial approximation the method presented here assumes no analytic knowledge of the approximated manifold and the approximation algorithm is linear in the large dimension n furthermore the approximating manifold can serve as a framework to perform operations directly on the high dimensional data in a computationally efficient manner this way the preparatory step of dimension reduction which induces distortions to the data can be avoided altogether less,Manifold Approximation by Moving Least-Squares Projection (MMLS),"26 February, 2020"
427,David Lindell,unsupervised learning of daware generative adversarial networks gans using only collections of singleview d photographs has very recently made much progress these d gans however have not been demonstrated for human bodies and the generated radiance fields of existing frameworks are not directly editable limiting their applicability in downstream tasks we propose a solution to these challenges by developing a d gan framework that learns to generate radiance fields of human bodies or faces in a canonical pose and warp them using an explicit deformation field into a desired body pose or facial expression using our framework we demonstrate the first highquality radiance field generation results for human bodies moreover we show that our deformationaware training procedure significantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a d gan that is not trained with explicit deformations less,Generative Neural Articulated Radiance Fields,"28 June, 2022"
428,David Lindell,learned graph neural networks gnns have recently been established as fast and accurate alternatives for principled solvers in simulating the dynamics of physical systems in many application domains across science and engineering however we are not only interested in a forward simulation but also in solving inverse problems with constraints defined by a partial differential equation pde here we explore gnns to solve such pdeconstrained inverse problems given a sparse set of measurements we are interested in recovering the initial condition or parameters of the pde we demonstrate that gnns combined with autodecoderstyle priors are wellsuited for these tasks achieving more accurate estimates of initial conditions or physical parameters than other learned approaches when applied to the wave equation or navierstokes equations we also demonstrate computational speedups of up to x using gnns compared to principled solvers project page httpscyanzhaogithubiolearninverseproblem less,Learning to Solve PDE-constrained Inverse Problems with Graph Networks,"1 June, 2022"
429,David Lindell,millions of images of human faces are captured every single day but these photographs portray the likeness of an individual with a fixed pose expression and appearance portrait image animation enables the postcapture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subjects likeness or identity still current methods for portrait image animation are typically based on d warping operations or manipulations of a d generative adversarial network gan and lack explicit mechanisms to enforce multiview consistency thus these methods may significantly alter the identity of the subject especially when the viewpoint relative to the camera is changed in this work we leverage newly developed d gans which allow explicit control over the pose of the image subject with multiview consistency we propose a supervision strategy to flexibly manipulate expressions with d morphable models and we show that the proposed method also supports editing appearance attributes such as age or hairstyle by interpolating within the latent space of the gan the proposed technique for portrait image animation outperforms previous methods in terms of image quality identity preservation and pose transfer while also supporting attribute editing less,3D GAN Inversion for Controllable Portrait Image Animation,"25 March, 2022"
430,David Lindell,numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications among these applications neural volume rendering has recently been proposed as a new paradigm for view synthesis achieving photorealistic image quality however a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference millions of rays each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with monte carlo sampling here we propose automatic integration a new framework for learning efficient closedform solutions to integrals using coordinatebased neural networks for training we instantiate the computational graph corresponding to the derivative of the network the graph is fitted to the signal to integrate after optimization we reassemble the graph to obtain a network that represents the antiderivative by the fundamental theorem of calculus this enables the calculation of any definite integral in two evaluations of the network applying this approach to neural rendering we improve a tradeoff between rendering speed and image quality improving render times by greater than times with a tradeoff of slightly reduced image quality less,AutoInt: Automatic Integration for Fast Neural Volume Rendering,"22 May, 2021"
431,David Lindell,nonlineofsight nlos imaging and tracking is an emerging technology that allows the shape or position of objects around corners or behind diffusers to be recovered from transient timeofflight measurements however existing nlos approaches require the imaging system to scan a large area on a visible surface where the indirect light paths of hidden objects are sampled in many applications such as robotic vision or autonomous driving optical access to a large scanning area may not be available which severely limits the practicality of existing nlos techniques here we propose a new approach dubbed keyhole imaging that captures a sequence of transient measurements along a single optical path for example through a keyhole assuming that the hidden object of interest moves during the acquisition time we effectively capture a series of timeresolved projections of the objects shape from unknown viewpoints we derive inverse methods based on expectationmaximization to recover the objects shape and location using these measurements then with the help of long exposure times and retroreflective tape we demonstrate successful experimental results with a prototype keyhole imaging system less,Keyhole Imaging: Non-Line-of-Sight Imaging and Tracking of Moving Objects Along a Single Optical Path,"5 January, 2021"
432,David Lindell,"Unsupervised learning of 3D-aware generative adversarial networks (GANs) using only collections of single-view 2D photographs has very recently made much progress. These 3D GANs, however, have not been demonstrated for human bodies and the generated radiance fields of existing frameworks are not directly editable, limiting their applicability in downstream tasks. We propose a solution to these challenges by developing a 3D GAN framework that learns to generate radiance fields of human bodies or faces in a canonical pose and warp them using an explicit deformation field into a desired body pose or facial expression. Using our framework, we demonstrate the first high-quality radiance field generation results for human bodies. Moreover, we show that our deformation-aware training procedure significantly improves the quality of generated bodies or faces when editing their poses or facial expressions compared to a 3D GAN that is not trained with explicit deformations.",Generative Neural Articulated Radiance Fields,2022-06-28 00:00:00
433,David Lindell,"Learned graph neural networks (GNNs) have recently been established as fast and accurate alternatives for principled solvers in simulating the dynamics of physical systems. In many application domains across science and engineering, however, we are not only interested in a forward simulation but also in solving inverse problems with constraints defined by a partial differential equation (PDE). Here we explore GNNs to solve such PDE-constrained inverse problems. Given a sparse set of measurements, we are interested in recovering the initial condition or parameters of the PDE. We demonstrate that GNNs combined with autodecoder-style priors are well-suited for these tasks, achieving more accurate estimates of initial conditions or physical parameters than other learned approaches when applied to the wave equation or Navier-Stokes equations. We also demonstrate computational speedups of up to 90x using GNNs compared to principled solvers. ",Learning to Solve PDE-constrained Inverse Problems with Graph Networks,2022-06-01 00:00:00
434,David Lindell,"Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON offer some control over the frequency spectrum used to represent continuous signals such as images or 3D volumes. Yet, they are not readily applicable to problems for which coarse-to-fine estimation is required, including various inverse problems in which coarse-to-fine optimization plays a key role in avoiding poor local minima. We introduce a new coordinate network architecture and training scheme that enables coarse-to-fine optimization with fine-grained control over the frequency support of learned reconstructions. This is achieved with two key innovations. First, we incorporate skip connections so that structure at one scale is preserved when fitting finer-scale structure. Second, we propose a novel initialization scheme to provide control over the model frequency spectrum at each stage of optimization. We demonstrate how these modifications enable multiscale optimization for coarse-to-fine fitting to natural images. We then evaluate our model on synthetically generated datasets for the the problem of single-particle cryo-EM reconstruction. We learn high resolution multiscale structures, on par with the state-of-the art.",Residual Multiplicative Filter Networks for Multiscale Reconstruction,2022-06-01 00:00:00
435,David Lindell,"We propose using a coordinate network as a decoder for MRI super-resolution. The continuous signal representation of coordinate networks enables this approach to be scale-agnostic, ie training over a continuous range of scales and querying at arbitrary resolutions. We evaluate the benefits of denoising for coordinate networks and also compare our method to a convolutional decoder using image quality metrics and a radiologist study.",Scale-Agnostic Super-Resolution in MRI using Feature-Based Coordinate Networks,2022-04-22 00:00:00
436,David Lindell,"Millions of images of human faces are captured every single day; but these photographs portray the likeness of an individual with a fixed pose, expression, and appearance. Portrait image animation enables the post-capture adjustment of these attributes from a single image while maintaining a photorealistic reconstruction of the subject's likeness or identity. Still, current methods for portrait image animation are typically based on 2D warping operations or manipulations of a 2D generative adversarial network (GAN) and lack explicit mechanisms to enforce multi-view consistency. Thus these methods may significantly alter the identity of the subject, especially when the viewpoint relative to the camera is changed. In this work, we leverage newly developed 3D GANs, which allow explicit control over the pose of the image subject with multi-view consistency. We propose a supervision strategy to flexibly manipulate expressions with 3D morphable models, and we show that the proposed method also supports editing appearance attributes, such as age or hairstyle, by interpolating within the latent space of the GAN. The proposed technique for portrait image animation outperforms previous methods in terms of image quality, identity preservation, and pose transfer while also supporting attribute editing.",3D GAN Inversion for Controllable Portrait Image Animation,2022-03-25 00:00:00
437,David Lindell,"Coordinate-based networks have emerged as a powerful tool for 3D representation and scene reconstruction. These networks are trained to map continuous input coordinates to the value of a signal at each point. Still, current architectures are black boxes: their spectral characteristics cannot be easily analyzed, and their behavior at unsupervised points is difficult to predict. Moreover, these networks are typically trained to represent a signal at a single scale, so naive downsampling or upsampling results in artifacts. We introduce band-limited coordinate networks (BACON), a network architecture with an analytical Fourier spectrum. BACON has constrained behavior at unsupervised points, can be designed based on the spectral characteristics of the represented signal, and can represent signals at multiple scales without per-scale supervision. We demonstrate BACON for multiscale neural representation of images, radiance fields, and 3D scenes using signed distance functions and show that it outperforms conventional single-scale coordinate networks in terms of interpretability and quality.",Bacon: Band-limited coordinate networks for multiscale scene representation,2022
438,David Lindell,"Neural representations have emerged as a new paradigm for applications in rendering, imaging, geometric modeling, and simulation. Compared to traditional representations such as meshes, point clouds, or volumes they can be flexibly incorporated into differentiable learning-based pipelines. While recent improvements to neural representations now make it possible to represent signals with fine details at moderate resolutions (e.g., for images and 3D shapes), adequately representing large-scale or complex scenes has proven a challenge. Current neural representations fail to accurately represent images at resolutions greater than a megapixel or 3D scenes with more than a few hundred thousand polygons. Here, we introduce a new hybrid implicit-explicit network architecture and training strategy that adaptively allocates resources during training and inference based on the local complexity of a signal of interest. Our approach uses a multiscale block-coordinate decomposition, similar to a quadtree or octree, that is optimized during training. The network architecture operates in two stages: using the bulk of the network parameters, a coordinate encoder generates a feature grid in a single forward pass. Then, hundreds or thousands of samples within each block can be efficiently evaluated using a lightweight feature decoder. With this hybrid implicit-explicit network architecture, we demonstrate the first experiments that fit gigapixel images to nearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in scale of over 1000x compared to the resolution of previously demonstrated image-fitting experiments. Moreover, our approach ",Acorn: Adaptive coordinate networks for neural scene representation,2021-05-06 00:00:00
439,David Lindell,"Numerical integration is a foundational technique in scientific computing and is at the core of many computer vision applications. Among these applications, neural volume rendering has recently been proposed as a new paradigm for view synthesis, achieving photorealistic image quality. However, a fundamental obstacle to making these methods practical is the extreme computational and memory requirements caused by the required volume integrations along the rendered rays during training and inference. Millions of rays, each requiring hundreds of forward passes through a neural network are needed to approximate those integrations with Monte Carlo sampling. Here, we propose automatic integration, a new framework for learning efficient, closed-form solutions to integrals using coordinate-based neural networks. For training, we instantiate the computational graph corresponding to the derivative of the coordinate-based network. The graph is fitted to the signal to integrate. After optimization, we reassemble the graph to obtain a network that represents the antiderivative. By the fundamental theorem of calculus, this enables the calculation of any definite integral in two evaluations of the network. Applying this approach to neural rendering, we improve a tradeoff between rendering speed and image quality: improving render times by greater than 10x with a tradeoff of reduced image quality.",Autoint: Automatic integration for fast neural volume rendering,2021
440,David Lindell,"Non-line-of-sight (NLOS) imaging and tracking is an emerging technology that allows the shape or position of objects around corners or behind diffusers to be recovered from transient, time-of-flight measurements. However, existing NLOS approaches require the imaging system to scan a large area on a visible surface, where the indirect light paths of hidden objects are sampled. In many applications, such as robotic vision or autonomous driving, optical access to a large scanning area may not be available, which severely limits the practicality of existing NLOS techniques. Here, we propose a new approach, dubbed keyhole imaging, that captures a sequence of transient measurements along a single optical path, for example, through a keyhole. Assuming that the hidden object of interest moves during the acquisition time, we effectively capture a series of time-resolved projections of the object's shape from ",Keyhole imaging: non-line-of-sight imaging and tracking of moving objects along a single optical path,2020-12-22 00:00:00
441,David Lindell,"Optical imaging techniques, such as light detection and ranging (LiDAR), are essential tools in remote sensing, robotic vision, and autonomous driving. However, the presence of scattering places fundamental limits on our ability to image through fog, rain, dust, or the atmosphere. Conventional approaches for imaging through scattering media operate at microscopic scales or require a priori knowledge of the target location for 3D imaging. We introduce a technique that co-designs single-photon avalanche diodes, ultra-fast pulsed lasers, and a new inverse method to capture 3D shape through scattering media. We demonstrate acquisition of shape and position for objects hidden behind a thick diffuser (≈6 transport mean free paths) at macroscopic scales. Our technique, confocal diffuse tomography, may be of considerable value to the aforementioned applications.",Three-dimensional imaging through scattering media based on confocal diffuse tomography,2020-09-09 00:00:00
442,David Lindell,active d imaging systems have broad applications across disciplines including biological imaging remote sensing and robotics applications in these domains require fast acquisition times high timing resolution and high detection sensitivity singlephoton avalanche diodes spads have emerged as one of the most promising detector technologies to achieve all of these requirements however these detectors are plagued by measurement distortions known as pileup which fundamentally limit their precision in this work we develop a probabilistic image formation model that accurately models pileup we devise inverse methods to efficiently and robustly estimate scene depth and reflectance from recorded photon counts using the proposed model along with statistical priors with this algorithm we not only demonstrate improvements to timing accuracy by more than an order of magnitude compared to the stateoftheart but this approach is also the first to facilitate subpicosecondaccurate photonefficient d imaging in practical scenarios where widelyvarying photon counts are observed less,Sub-picosecond photon-efficient 3D imaging using single-photon sensors,"8 June, 2018"
443,Fan Long,"In decentralized finance (DeFi) ecosystem, lenders can offer flash loans to borrowers, i.e., loans that are only valid within a blockchain transaction and must be repaid with some fees by the end of that transaction. Unlike normal loans, flash loans allow borrowers to borrow a large amount of assets without upfront collaterals deposits. Malicious adversaries can use flash loans to gather large amount of assets to launch costly exploitations targeting DeFi protocols. In this paper, we introduce a new framework for automated synthesis of adversarial contracts that exploit DeFi protocols using flash loans. To bypass the complexity of a DeFi protocol, we propose a new technique to approximate DeFi protocol functional behaviors using numerical methods. Then, we propose a novel algorithm to find an adversarial attack which constitutes of a sequence of invocations of functions in a DeFi protocol with the optimized parameters for profits. We implemented our framework in a tool called FlashSyn. We run FlashSyn on 5 DeFi protocols that were victims to flash loan attacks and DeFi protocols from Damn Vulnerable DeFi challenges. FlashSyn automatically synthesizes an adversarial attack for each one of them.",FlashSyn: Flash Loan Attack Synthesis via Counter Example Driven Approximation,2022-06-21 00:00:00
444,Fan Long,"With the emergence of decentralized finance, smart contracts and their users become more and more susceptible to expensive exploitations. This paper investigates the price gouging transaction order dependency vulnerabilities in smart contracts. A static analysis based approach is proposed to automatically locate and rectify such vulnerabilities, and a prototype tool using Slither, a static analyzer for Solidity, is also developed. All in all, empirical results on a benchmark suite containing 51 Solidity smart contracts show that the proposed methodology can be used successfully to both detect such vulnerabilities and rectify them, or to certify that a Solidity smart contract under question does not contain such vulnerabilities.",Automated Auditing of Price Gouging TOD Vulnerabilities in Smart Contracts,2022-05-02 00:00:00
445,Fan Long,"We present the Layered Merkle Patricia Trie (LMPT), a performant storage data structure for processing transactions in high-throughput systems when com-pared to traditional Merkle Patricia Tries used in Ethereum clients. LMPTs keep smaller intermediary tries in memory to alleviate read and write amplification from high-latency disk storage. As an additional feat, they also allow for the I/O and transaction verifier threads to be scheduled in parallel and independently. LMPTs can ultimately reduce significant I/O traffic that happens on the critical path of transaction processing. Empirical results presented here confirm that LMPTs can process up to × 6 more transactions per second on real-life workloads when compared to existing Ethereum clients.",LMPTs: Eliminating Storage Bottlenecks for Processing Blockchain Transactions,2022-05-02 00:00:00
446,Fan Long,"We present automatic horizontal fusion, a novel optimization technique that complements the standard kernel fusion techniques for GPU programs. Unlike the standard fusion, whose goal is to eliminate intermediate data round trips, our horizontal fusion technique aims to increase the thread-level parallelism to hide instruction latencies. We also present HFUSE, a new source to source CUDA compiler that implements automatic horizontal fusion. Our experimental results show that the horizontal fusion can speed up the running time by 2.5% 60.8%. Our results reveal that the horizontal fusion is especially beneficial for fusing kernels with instructions that require different kinds of GPU resources (e.g., a memory-intensive kernel and a compute-intensive kernel).",Automatic horizontal fusion for GPU kernels,2022-04-02 00:00:00
447,Fan Long,"Traditional public blockchain systems typically had very limited transaction throughput due to the bottleneck of the consensus protocol itself. With recent advances in consensus technology, the performance limit has been greatly lifted, typically to thousands of transactions per second. With this, transaction execution has become a new performance bottleneck. Exploiting parallelism in transaction execution is a clear and direct way to address this and further increase transaction throughput. Although some recent literature introduced concurrency control mechanisms to execute smart contract transactions in parallel, the reported speedup that they can achieve is far from ideal. The main reason is that the proposed parallel execution mechanisms cannot effectively deal with the conflicts inherent in many blockchain applications. In this work, we thoroughly study the historical transaction execution traces in Ethereum. We observe that application-inherent conflicts are the major factors that limit the exploitable parallelism during execution. We propose to use partitioned counters and special commutative instructions to break up the application conflict chains in order to maximize the potential speedup. During our evaluations, these techniques doubled the parallel speedup achievable to an 18x overall speedup compared to serial execution, approaching the optimum. We also propose an OCC scheduler with deterministic aborts, which makes it suitable for practical integration into public blockchain systems.",Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,2022-01-11 00:00:00
448,Fan Long,"Smart contracts facilitate the execution of programmable code on a blockchain. The cost for executing smart contract code is metered using gas - the exact amount of which is based on the computational complexity of the underlying smart contract. Hence, it is imperative to optimize smart contract code to reduce gas consumption and, in some instances, to even avoid malicious attacks. In this paper, we propose an approach to optimize the gas consumption of smart contracts, specifically loop control structures. We present a prototype implementation of our approach using off-the-shelf tools for Solidity smart contracts. We experimentally evaluate our technique using 72 Solidity smart contracts. Our evaluation demonstrates the average gas cost savings per transaction to be around 23,943 gas units, or an equivalent 21% decrease in gas costs. Although the approach causes a slight increase in deployment costs due to",Smart Contracts Refinement for Gas Optimization,2021-09-27 00:00:00
449,Fan Long,"Zero-knowledge proof (ZKP) is a promising cryptographic protocol for both computation integrity and privacy. It can be used in many privacy-preserving applications including verifiable cloud outsourcing and blockchains. The major obstacle of using ZKP in practice is its time-consuming step for proof generation, which consists of large-size polynomial computations and multi-scalar multiplications on elliptic curves. To efficiently and practically support ZKP in real-world applications, we propose PipeZK, a pipelined accelerator with two subsystems to handle the aforementioned two intensive compute tasks, respectively. The first subsystem uses a novel dataflow to decompose large kernels into smaller ones that execute on bandwidth-efficient hardware modules, with optimized off-chip memory accesses and on-chip compute resources. The second subsystem adopts a lightweight dynamic work dispatch mechanism to ",Pipezk: Accelerating zero-knowledge proof with a pipelined architecture,2021-06-14 00:00:00
450,Fan Long,"This paper presents SigVM, a novel blockchain virtual machine that supports an event-driven execution model, enabling developers to build fully autonomous smart contracts. SigVM introduces another way for a contract to interact with another. Contracts in SigVM can emit signal events, on which other contracts can listen. Once an event is triggered, corresponding handler functions are automatically executed as signal transactions. We built an end-to-end blockchain platform SigChain and a contract language compiler SigSolid to realize the potential of SigVM. Experimental results show that SigVM enables contracts in our benchmark applications to be reimplemented in a fully autonomous way, eliminating the dependency on unreliable mechanisms like off-chain relay servers. SigVM can significantly simplify the execution flow of our benchmark applications, and can avoid security risks such as front-run attacks.",SigVM: Toward Fully Autonomous Smart Contracts,2021-02-22 00:00:00
451,Fan Long,"Global economic digitization continues to advance at exponential speed. This development is in sharp contrast to the financial sector and payment systems that still operate on legacy infrastructure that lacks the flexibility to serve those technology needs. Further, the emergence of Decentralized Finance demonstrates the capacity to disrupt the financial sector, impact national sovereignty, and affect established monetary transmission channels. Hence, it is no surprise that nation-states and tech-firms alike are now building new digital infrastructures that circumvent the legacy practices. Central banks, in particular, are racing to explore the issuance of Central Bank-issued Digital Currencies (CBDCs) in an attempt to rediscover the very essence and use of fiat cash.",Central bank digital loonie: Canadian cash for a new global economy,2021-02-11 00:00:00
452,Fan Long,"This paper presents SigVM, a novel blockchain virtual machine that supports an event-driven execution model, enabling developers to build autonomous smart contracts. Contracts in SigVM can emit signal events, on which other contracts can listen. Once an event is triggered, corresponding handler functions are automatically executed as signal transactions. We build an end-to-end blockchain platform SigChain and a contract language compiler SigSolid to realize the potential of SigVM. Experimental results show that our benchmark applications can be reimplemented with SigVM in an autonomous way, eliminating the dependency on unreliable mechanisms like off-chain relay servers. The development effort of reimplementing these contracts with SigVM is small, ie, we modified on average 2.6% of the contract code.",SigVM: Enabling Event-Driven Execution for Autonomous Smart Contracts,2021/2
453,Fan Long,The success of Bitcoin and Ethereum has attracted many efforts to build high-throughput blockchain systems. This paper focuses on transaction dissemination---a rather overlooked issue in these systems. We argue that efficient transaction dissemination is the key for a blockchain system to sustain at high-throughput---usually thousands of transactions per second---and the existing solutions fell short at doing so.,Shrec: Bandwidth-efficient transaction relay in high-throughput blockchain systems,2020-10-12 00:00:00
454,Fan Long,"Existing Byzantine fault tolerance (BFT) protocols face significant challenges in safety, scalability, throughput, and latency. We present a new BFT protocol, Gosig, for the consortium blockchains. Gosig guarantees safety even in asynchronous networks fully controlled by adversaries, by combining secret leader selection with multi-round voting. We co-design both the consensus protocol and the underlying gossip network to optimize performance. In particular, we adopt transmission pipelining to fully utilize the network bandwidth while use aggregated signature gossip to reduce the number of messages. These optimizations help Gosig to achieve unprecedented single-chain performance. On a public cloud testbed spanning multiple data centers consisting of 280 nodes across 14 cities on five continents, Gosig achieves over 15,000 transactions per second with 15.8-second confirmation time. When the system scales",Gosig: a scalable and high-performance byzantine consensus for consortium blockchains,2020-10-12 00:00:00
455,Fan Long,"Proof-of-work blockchains need to be carefully designed so as to create the proper incentives for miners to faithfully maintain the network in a sustainable way. This paper describes how the economic engineering of the Conflux Network, a high throughput proof-of-work blockchain, leads to sound economic incentives that support desirable and sustainable mining behavior. In detail, this paper parameterizes the level of income, and thus network security, that Conflux can generate, and it describes how this depends on user behavior and “policy variables” such as block and interest inflation. It also discusses how the underlying economic engineering design makes the Conflux Network resilient against double spending and selfish mining attacks.",Engineering economics in the conflux network,2020-09-28 00:00:00
456,Fan Long,"We present Solythesis, a source to source Solidity compiler which takes a smart contract code and a user specified invariant as the input and produces an instrumented contract that rejects all transactions that violate the invariant. The design of Solythesis is driven by our observation that the consensus protocol and the storage layer are the primary and the secondary performance bottlenecks of Ethereum, respectively. Solythesis operates with our novel delta update and delta check techniques to minimize the overhead caused by the instrumented storage access statements. Our experimental results validate our hypothesis that the overhead of runtime validation, which is often too expensive for other domains, is in fact negligible for smart contracts. The CPU overhead of Solythesis is only 0.1% on average for our 23 benchmark contracts.",Securing smart contract with runtime validation,2020-06-11 00:00:00
457,Fan Long,"This paper presents Conflux, a scalable and decentralized blockchain system with high throughput and fast confirmation. Conflux operates with a novel consensus protocol which optimistically processes concurrent blocks without discarding any as forks and adaptively assigns weights to blocks based on their topologies in the Conflux ledger structure (called Tree-Graph). The adaptive weight mechanism enables Conflux to detect and thwart liveness attack by automatically switching between an optimistic strategy for fast confirmation in normal scenarios and a conservative strategy to ensure consensus progress during liveness attacks. We evaluated Conflux on Amazon EC2 clusters with up to 12k full nodes. The consensus protocol of Conflux achieves a block throughput of 9.6 Mbps with 20Mbps network bandwidth limit per node. On a combined workload of payment transactions and Ethereum history transactions, the end-to-end system of Conflux achieves the throughput of up to 3480 transactions per second while confirming transactions under one minute.",A decentralized blockchain with high throughput and fast confirmation,2020
458,Chris Maddison,cutting planes are essential for solving mixedinteger linear problems milps because they facilitate bound improvements on the optimal solution value for selecting cuts modern solvers rely on manually designed heuristics that are tuned to gauge the potential effectiveness of cuts we show that a greedy selection rule explicitly looking ahead to select cuts that yield the best bound improvement delivers strong decisions for cut selection but is too expensive to be deployed in practice in response we propose a new neural architecture neuralcut for imitation learning on the lookahead expert our model outperforms standard baselines for cut selection on several synthetic milp benchmarks experiments with a bc solver for neural network verification further validate our approach and exhibit the potential of learning methods in this setting less,Learning To Cut By Looking Ahead: Cutting Plane Selection via Imitation Learning,"27 June, 2022"
459,Chris Maddison,supervised learning can improve the design of stateoftheart solvers for combinatorial problems but labelling large numbers of combinatorial instances is often impractical due to exponential worstcase complexity inspired by the recent success of contrastive pretraining for images we conduct a scientific study of the effect of augmentation design on contrastive pretraining for the boolean satisfiability problem while typical graph contrastive pretraining uses labelagnostic augmentations our key insight is that many combinatorial problems have wellstudied invariances which allow for the design of labelpreserving augmentations we find that labelpreserving augmentations are critical for the success of contrastive pretraining we show that our representations are able to achieve comparable test accuracy to fullysupervised learning while using only of the labels we also demonstrate that our representations are more transferable to larger problems from unseen domains our code is available at httpsgithubcomhduancontrastivesat less,Augment with Care: Contrastive Learning for Combinatorial Problems,"20 June, 2022"
460,Chris Maddison,machine learning systems often experience a distribution shift between training and testing in this paper we introduce a simple variational objective whose optima are exactly the set of all representations on which risk minimizers are guaranteed to be robust to any distribution shift that preserves the bayes predictor eg covariate shifts our objective has two components first a representation must remain discriminative for the task ie some predictor must be able to simultaneously minimize the source and target risk second the representations marginal support needs to be the same across source and target we make this practical by designing selfsupervised objectives that only use unlabelled data and augmentations to train robust representations our objectives give insights into the robustness of clip and further improve clips representations to achieve sota results on domainbed less,Optimal Representations for Covariate Shift,"14 March, 2022"
461,Chris Maddison,training largescale mixture of experts models efficiently on modern hardware requires assigning datapoints in a batch to different experts each with a limited capacity recently proposed assignment procedures lack a probabilistic interpretation and use biased estimators for training as an alternative we propose two unbiased estimators based on principled stochastic assignment procedures one that skips datapoints which exceed expert capacity and one that samples perfectly balanced assignments using an extension of the gumbelmatching distribution both estimators are unbiased as they correct for the used sampling procedure on a toy experiment we find the skipestimator is more effective than the balanced sampling one and both are more robust in solving the task than biased alternatives less,Unbiased Gradient Estimation with Balanced Assignments for Mixtures of Experts,"8 December, 2021"
462,Chris Maddison,latent variable models have been successfully applied in lossless compression with the bitsback coding algorithm however bitsback suffers from an increase in the bitrate equal to the kl divergence between the approximate posterior and the true posterior in this paper we show how to remove this gap asymptotically by deriving bitsback coding algorithms from tighter variational bounds the key idea is to exploit extended space representations of monte carlo estimators of the marginal likelihood naively applied our schemes would require more initial bits than the standard bitsback coder but we show how to drastically reduce this additional cost with couplings in the latent space when parallel architectures can be exploited our coders can achieve better rates than bitsback with little additional cost we demonstrate improved lossless compression rates in a variety of settings especially in outofdistribution or sequential data compression less,Improving Lossless Compression Rates via Monte Carlo Bits-Back Coding,"14 June, 2021"
463,Chris Maddison,source code spends most of its time in a broken or incomplete state during software development this presents a challenge to machine learning for code since highperforming models typically rely on graph structured representations of programs derived from traditional program analyses such analyses may be undefined for broken or incomplete code we extend the notion of program graphs to workinprogress code by learning to predict edge relations between tokens training on wellformed code before transferring to workinprogress code we consider the tasks of code completion and localizing and repairing variable misuse in a workinprocess scenario we demonstrate that training relationaware models with finetuned edges consistently leads to improved performance on both tasks less,Learning to Extend Program Graphs to Work-in-Progress Code,"28 May, 2021"
464,Chris Maddison,the origin of the inner dust cavities observed in transition discs remains unknown the segregation of dust and size of the cavity is expected to vary depending on which clearing mechanism dominates grain evolution we present the results from the discs down under program an mm continuum australia telescope compact array atca survey targeting transition discs with large au cavities and compare the resulting dust emission to atacama large millimetresubmillimetre array alma observations our atca observations resolve the inner cavity for of the detected discs we fit the visibilities and reconstruct d radial brightness models for sources with a sn sigma we find that for sources with a resolved cavity in both wavebands the mm and submm brightness distributions peak at the same radius from the star we suggest that a similar cavity size for mm and submm dust grains is due to a dust trap induced by the presence of a companion less,Dust Traps and the Formation of Cavities in Transition Discs: A millimetre to sub-millimetre comparison survey,"3 February, 2021"
465,Chris Maddison,direct optimization is an appealing framework that replaces integration with optimization of a random objective for approximating gradients in models with discrete random variables astar sampling is a framework for optimizing such random objectives over large spaces we show how to combine these techniques to yield a reinforcement learning algorithm that approximates a policy gradient by finding trajectories that optimize a random objective we call the resulting algorithms direct policy gradient dirpg algorithms a main benefit of dirpg algorithms is that they allow the insertion of domain knowledge in the form of upper bounds on returntogo at training time like is used in heuristic search while still directly computing a policy gradient we further analyze their properties showing there are cases where dirpg has an exponentially larger probability of sampling informative gradients compared to reinforce we also show that there is a builtin variance reduction technique and that a parameter that was previously viewed as a numerical approximation can be interpreted as controlling risk sensitivity empirically we evaluate the effect of key degrees of freedom and show that the algorithm performs well in illustrative domains compared to baselines less,Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces,"23 October, 2020"
466,Chris Maddison,propositional model counting or sat is the problem of computing the number of satisfying assignments of a boolean formula and many discrete probabilistic inference problems can be translated into a model counting problem to be solved by sat solvers generic exact sat solvers however are often not scalable to industriallevel instances in this paper we present neuro an approach for learning branching heuristics for exact sat solvers via evolution strategies es to reduce the number of branching steps the solver takes to solve an instance we experimentally show that our approach not only reduces the step count on similarly distributed heldout instances but it also generalizes to much larger instances from the same problem family the gap between the learned and the vanilla solver on larger instances is sometimes so wide that the learned solver can even overcome the run time overhead of querying the model and beat the vanilla in wallclock time by orders of magnitude less,Learning Branching Heuristics for Propositional Model Counting,"7 July, 2020"
467,Chris Maddison,the variational autoencoder vae is a popular method for learning a generative model and embeddings of the data many real datasets are hierarchically structured however traditional vaes map data in a euclidean latent space which cannot efficiently embed treelike structures hyperbolic spaces with negative curvature can we therefore endow vaes with a poincar ball model of hyperbolic geometry as a latent space and rigorously derive the necessary methods to work with two main gaussian generalisations on that space we empirically show better generalisation to unseen data than the euclidean counterpart and can qualitatively and quantitatively better recover hierarchical structures less,Continuous Hierarchical Representations with Poincaré Variational Auto-Encoders,"25 November, 2019"
468,Chris Maddison,we provide theoretical and empirical evidence that using tighter evidence lower bounds elbos can be detrimental to the process of learning an inference network by reducing the signaltonoise ratio of the gradient estimator our results call into question common implicit assumptions that tighter elbos are better variational objectives for simultaneous model learning and inference amortization schemes based on our insights we introduce three new algorithms the partially importance weighted autoencoder piwae the multiply importance weighted autoencoder miwae and the combination importance weighted autoencoder ciwae each of which includes the standard importance weighted autoencoder iwae as a special case we show that each can deliver improvements over iwae even when performance is measured by the iwae target itself furthermore our results suggest that piwae may be able to deliver simultaneous improvements in the training of both the inference and generative networks less,Tighter Variational Bounds are Not Necessarily Better,"5 March, 2019"
469,Chris Maddison,simulating samples from arbitrary probability distributions is a major research program of statistical computing recent work has shown promise in an old idea that sampling from a discrete distribution can be accomplished by perturbing and maximizing its mass function yet it has not been clearly explained how this research project relates to more traditional ideas in the monte carlo literature this chapter addresses that need by identifying a poisson process model that unifies the perturbation and acceptreject views of monte carlo simulation many existing methods can be analyzed in this framework the chapter reviews poisson processes and defines a poisson process model for monte carlo methods this model is used to generalize the perturbation trick to infinite spaces by constructing gumbel processes random functions whose maxima are located at samples over infinite spaces the model is also used to analyze a sampling and os two methods from distinct monte carlo families less,A Poisson process model for Monte Carlo,"12 April, 2016"
470,Chris Maddison,we study the problem of building generative models of natural source code nsc that is source code written and understood by humans our primary contribution is to describe a family of generative models for nsc that have three key properties first they incorporate both sequential and hierarchical structure second we learn a distributed representation of source code elements finally they integrate closely with a compiler which allows leveraging compiler logic and abstractions when building structure into the model we also develop an extension that includes more complex structure refining how the model generates identifier tokens based on what variables are currently in scope our models can be learned efficiently and we show empirically that including appropriate structure greatly improves the models measured by the probability of generating test programs less,Structured Generative Models of Natural Source Code,"20 June, 2014"
471,Chris Maddison,grains in disks around young stars grow from interstellar submicron sizes to planetesimals over the course of several myr thermal emission of large grains or pebbles can be best observed at cm wavelengths however other emission mechanisms can contribute we aim to determine the mechanisms of cm emission for t tauri stars ww cha and ru lup were recently found to have grain growth at least up to mm sizes in their circumstellar disks cs cha has similar indications for grain growth in its circumbinary disk the t tauri stars ww cha and ru lup were monitored over several years at mm and cm wavelengths using atca the new atca mm system was also used to observe cs cha ww cha was detected on several occasions at and mm we obtained one detection of ww cha at cm and upper limits only at cm the emission at mm was stable over days months and years but the emission at cm is found to be variable ru lup was detected at mm it was observed at mm times and at and cm times and found to be variable in all wavebands cs cha was detected at mm but the sn was too low to resolve the gap in the circumbinary disk the emission at and mm for ww cha is well explained by thermal emission from mm and cmsized pebbles the cm spectral index is consistent with the emission from an opticallythick ionised wind but the high variability of the cm emission points to a nonthermal contribution the seds of ru lup and cs cha from to mm are consistent with thermal emission from mmsized grains the variability of the longerwavelength emission for ru lup and the negative spectral index suggest nonthermal emission less,"Large grains in disks around young stars: ATCA observations of WW Cha, RU Lup, and CS Cha","19 December, 2008"
472,Peter Marbach,We study an optimal bidding and allocation problem that builds upon the earlier work of [4]. The model we study is of interest in online real-time advertising where the demand of a large number of advertisers is aggregated by an intermediary who bids in the advertising auction market on their behalf. This intermediary is referred to as a Demand Side Platform (DSP).,Real-time Bidding for Time Constrained Impression Contracts in First and Second Price Auctions - Theory and Algorithms,06 June 2022
473,Peter Marbach,"We study problems arising in real-time auction markets, common in e-commerce and computational advertising, where bidders face the problem of calculating optimal bids. We focus upon a contract management problem where a demand aggregator is subject to multiple contractual obligations requiring them to acquire items of heterogeneous types at a specified rate, which they will seek to fulfill at minimum cost. Our main results show that, through a transformation of variables, this problem can be formulated as a convex optimization problem, for both first and second price auctions. Convexity results in efficient algorithms for solving instances of this problem, and the resulting duality theory admits rich structure and interpretations. Additionally, we show that the transformation of variables used to formulate this problem as a convex program can also be used to guarantee the convexity of optimal bidding problems studied by other authors (who did not leverage convexity). Finally, we show how the expected cost of bidding in second price auctions is formally identical to certain transaction costs when submitting market orders in limit order book markets. This fact is used to analyze a Markowitz portfolio problem which accounts for these transaction costs, establishing an interesting connection between finance and optimal bidding.",Convexity and Duality in Optimum Real-time Bidding and Related Problems,22 Jun 2022
474,Peter Marbach,"We analyze the problem of how to optimally bid for ad spaces in online ad auctions. For this we consider the general case of multiple ad campaigns with overlapping targeting criteria. In our analysis we first characterize the structure of an optimal bidding strategy. In particular, we show that an optimal bidding strategies decomposes the problem into disjoint sets of campaigns and targeting groups. In addition, we show that pure bidding strategies that use only a single bid value for each campaign are not optimal when the supply curves are not continuous. For this case, we derive a lower-bound on the optimal cost of any bidding strategy, as well as mixed bidding strategies that either achieve the lower-bound or can get arbitrarily close to it.",Optimal Bidding Strategies for Online Ad Auctions with Overlapping Targeting Criteria,12 June 2020
475,Peter Marbach,"In this paper, we study the problem of detecting communities in real-world user-item graphs, i.e., bipartite graphs that represent interactions between a user and an item. Instead of developing a generic clustering algorithm for arbitrary graphs, we tailor our algorithm for user-item graphs by taking advantage of the inherent structural properties that exist in real-world networks. Assuming the existence of the core-periphery structure that has been experimentally and theoretically studied, our algorithm is able to extract the vast majority of the communities existing in the network by performing dramatically less computational work compared to conventional graph-clustering algorithms. The proposed algorithm achieves a subquadratic runtime (with respect to the number of vertices) for processing the entire graph, which makes it highly practical for processing large-scale graphs which typically arise in real-world applications. The performance of the proposed algorithm, in terms of both community-detection accuracy and efficiency, is experimentally evaluated with real-world datasets.",Efficient Community Detection by Exploiting Structural Properties of Real-World User-Item Graphs,20 December 2020
476,Peter Marbach,"In this paper we study social exclusion in social (information) networks using a game-theoretic approach, and study the stability of a certain class community structures that are a Nash equilibrium. The main result of our analysis shows that all stable community structures (Nash equilibria) in this class are community structures under which some agents are socially excluded, and do not belong to any of the communities. This result is quite striking as it suggests that social exclusion might be the “norm” (an expected outcome) in social networks, rather than an anomaly.",Stable Community Structures and Social Exclusion,07 October 2020
477,Peter Marbach,"Using a game-theoretic framework, we characterize the community structure that emerges in a social (information) network. Our analysis generalizes the results in [1, 2] that were obtained for the case of a continuous population model for the agents in the social network, to the case of a discrete agent population model. We note that a discrete agent set reflects more accurately real-life information networks, and are needed in order to get additional insights into the community structure, such as for example the connectivity (graph structure) within in a community, as well as information dissemination within a community.",Community Structures in Information Networks for a Discrete Agent Population,02 June 2020
478,Peter Marbach,"Communities are an important feature of social networks. In fact, it seems that communities are necessary for a social network to be efficient. However, there exist very few formal studies of the actual role of communities in social networks, how they emerge, and how they are structured. The goal of this paper is to propose a mathematical model to study communities in social networks. For this, we consider a particular case of a social network, namely information networks. We assume that there is a population of agents who are interested in obtaining content. Agents differ in the type of content they are interested in. The goal of agents is to form communities in order to maximize their utility for obtaining and producing content. We use this model to characterize the structure of communities that emerge in this setting.",The structure of communities in information networks,31 January 2016
479,Peter Marbach,"Over the last decade there has been growing interest in the understanding of complex networks such as the Internet, the World Wide Web and social networks. A large part of the research in this area has focused on macroscopic properties and models for complex networks such as the power law distribution of edge degrees and the small world phenomenon. Less attention has been paid to microscopic properties and models that try to model and explain the interaction and dynamics between individual vertices in a network. In this paper we discuss why such microscopic models play an important part in understanding complex networks. In particular we present examples of how microscopic generative models can be used to design efficient algorithms for complex networks.",Microscopic generative models for complex networks,21 March 2014
480,Peter Marbach,we study problems arising in realtime auction markets common in ecommerce and computational advertising where bidders face the problem of calculating optimal bids we focus upon a contract management problem where a demand aggregator is subject to multiple contractual obligations requiring them to acquire items of heterogeneous types at a specified rate which they will seek to fulfill at minimum cost our main results show that through a transformation of variables this problem can be formulated as a convex optimization problem for both first and second price auctions convexity results in efficient algorithms for solving instances of this problem and the resulting duality theory admits rich structure and interpretations additionally we show that the transformation of variables used to formulate this problem as a convex program can also be used to guarantee the convexity of optimal bidding problems studied by other authors who did not leverage convexity finally we show how the expected cost of bidding in second price auctions is formally identical to certain transaction costs when submitting market orders in limit order book markets this fact is used to analyze a markowitz portfolio problem which accounts for these transaction costs establishing an interesting connection between finance and optimal bidding less,Convexity and Duality in Optimum Real-time Bidding and Related Problems,"22 June, 2022"
481,Peter Marbach,we analyze the problem of how to optimally bid for ad spaces in online ad auctions for this we consider the general case of multiple ad campaigns with overlapping targeting criteria in our analysis we first characterize the structure of an optimal bidding strategy in particular we show that an optimal bidding strategies decomposes the problem into disjoint sets of campaigns and targeting groups in addition we show that pure bidding strategies that use only a single bid value for each campaign are not optimal when the supply curves are not continuous for this case we derive a lowerbound on the optimal cost of any bidding strategy as well as mixed bidding strategies that either achieve the lowerbound or can get arbitrarily close to it less,Optimal Bidding Strategies for Online Ad Auctions with Overlapping Targeting Criteria,"15 April, 2020"
482,Alex Mariakakis,we investigate users perspectives on an online reflective question activity rqa that prompts people to externalize their underlying emotions on a troubling situation inspired by principles of cognitive behavioral therapy our minute activity encourages selfreflection without a human or automated conversational partner a deployment of our rqa on amazon mechanical turk suggests that people perceive several benefits from our rqa including structured awareness of their thoughts and problemsolving around managing their emotions quantitative evidence from a randomized experiment suggests people find that our rqa makes them feel less worried by their selected situation and worth the minimal time investment a further twoweek technology probe deployment with participants indicates that people see benefits to doing this activity repeatedly although the activity may get monotonous over time in summary this work demonstrates the promise of online reflection activities that carefully leverage principles of psychology in their design less,Understanding User Perspectives on Prompts for Brief Reflection on Troubling Emotions,"20 December, 2021"
483,Alex Mariakakis,"The COVID-19 (SARS-CoV-2) pandemic has placed the world in a state of emergency for the better part of two years. COVID-19 can range from being asymptomatic to causing potentially fatal complications, such as acute respiratory distress syndrome. Acute COVID-19 infections typically last for approximately 2 weeks and common symptoms include cough, fatigue, and shortness of breath. A warning sign of potential deterioration from COVID-19 used by healthcare practitioners is an objective decrease in oxygen saturation. In this work, we explore the prediction of low oxygen saturation in ambulatory patients with COVID-19.",Predicting Low Oxygen Saturation of COVID-19 Patients Using a Random Forest Classifier,2022/5
484,Alex Mariakakis,"Standard sleep quality assessment methods require custom hardware and professional observation, limiting the diagnosis of sleep disorders to specialized sleep clinics. In this work, we leverage the internal and external microphones present in active noise-cancelling earbuds to distinguish sounds associated with poor or disordered sleep, thereby enabling athome continuous sleep sound monitoring. The sleep sounds our system is able to recognize include, but are not limited to, snoring, teeth grinding, and restless movement. We analyze the resulting dual-channel audio using a lightweight deep learning model built around a variation of the temporal shift module that has been optimized for audio. The model was designed to have a low memory and computational footprint, making it suitable to be run on a smartphone or the earbuds themselves. We evaluate our approach on a dataset of 8 sound categories ",Sleep Sound Classification Using ANC-Enabled Earbuds,2022-03-21 00:00:00
485,Alex Mariakakis,"Many point-of-care tests rely on visual changes in color, shape, and size to convey results that can be read by the naked eye. One category of such tests is an agglutination test (AT), which relies on the clumping of micro-particles or cells in the presence of a target analyte. Although visual inspection is convenient and fast, it is subjective, prone to errors, and limits decision-making to coarse-grained results. We present an open-source software framework designed to facilitate the development and interpretation of ATs. This framework includes a web-based annotation interface for curating new image datasets, a computer vision pipeline that extracts informative AT features, and a machine learning module that allows AT developers to study how an AT agglutinates over time during future experiments. We present two case studies of our framework being used to develop and interpret tests.",Machine Learning to Automate the Visual Interpretation of Chemical Agglutination Tests,2022-03-21 00:00:00
486,Alex Mariakakis,"Acoustic speech characteristics have previously been identified as possible indicators of respiratory disease when recorded in controlled lab settings. However, the ability to measure and leverage these indicators during people’s everyday lives has been largely under-explored. In this study, we use continuous audio data from smartwatches worn by individuals suffering from COPD, as well as symptom information through daily self-reports. By applying pre-trained models for voice activity detection and speaker verification models, we are able to isolate moments of the user’s own speech and extract important speech features. We then use those features in an isolation forest outlier detector to discriminate between days with normal and worsening symptoms, achieving an AUC of nearly 0.60 on this challenging problem.",Unobtrusive monitoring of COPD patients using speech collected from smartwatches in the wild,2022-03-21 00:00:00
487,Alex Mariakakis,"We proposed a method called health concept surveying for untangling the causal relationships that people develop around conceptual HITs. In health concept surveying, investigators gather reactions to design concepts through a scenario-based survey instrument. As the investigator manipulates characteristics related to their HIT, the survey instrument also measures proximal cognitive factors according to a health behavior change model to project how HIT design decisions may affect the adoption and acceptability of an HIT. Responses to the survey instrument were analyzed using path analysis to untangle the causal effects of these factors on the outcome variables.",Using Health Concept Surveying to Elicit Usable Evidence: Case Studies of a Novel Evaluation Methodology,2022-01-03 00:00:00
488,Alex Mariakakis,"We investigate users' perspectives on an online reflective question activity (RQA) that prompts people to externalize their underlying emotions on a troubling situation. Inspired by principles of cognitive behavioral therapy, our 15-minute activity encourages self-reflection without a human or automated conversational partner. A deployment of our RQA on Amazon Mechanical Turk suggests that people perceive several benefits from our RQA, including structured awareness of their thoughts and problem-solving around managing their emotions. Quantitative evidence from a randomized experiment suggests people find that our RQA makes them feel less worried by their selected situation and worth the minimal time investment. A further two-week technology probe deployment with 11 participants indicates that people see benefits to doing this activity repeatedly, although the activity may get monotonous over time. In summary, this work demonstrates the promise of online reflection activities that carefully leverage principles of psychology in their design.",Understanding User Perspectives on Prompts for Brief Reflection on Troubling Emotions,2021-12-20 00:00:00
489,Alex Mariakakis,"Blood typing, donor compatibility testing, and hematocrit analysis are common tests that are important in many clinical applications, including those found in high-stakes settings such as the trauma center. These tests are typically performed in centralized laboratories with sample batching; the minutes that are lost in this mode can lead to adverse outcomes, especially for critical-care patients. As a step toward providing rapid results at the bedside, we developed a point-of-care hemagglutination system relying on digital microfluidics (DMF) and a unique, automated readout tool, droplet agglutination assessment using digital microfluidics (DAAD).","Digital microfluidic hemagglutination assays for blood typing, donor compatibility testing, and hematocrit analysis",2021/12
490,Alex Mariakakis,"Poultry farming is a significant income-generating activity in sub-Saharan African (SSA) households. Poultry farmers frequently have to overcome extreme environmental conditions to maintain their chickens’ wellbeing. Prior research has proposed automating poultry farming activities to control environmental conditions (eg, temperature and humidity). However, these interventions have never been implemented, in this context, to understand how they would work and participants’ perceptions. Further, chicken coops in SSA have different configurations that would make technology automation difficult. To explore how technology can be used to address this problem, we worked with local collaborators to design and deploy “NkhukuProbe”—a low-cost sensor-based technology that poultry farmers can interact with via USSD (Unstructured Supplementary Service Data) to monitor and adjust chicken coop conditions. ",NkhukuProbe: Using a Sensor-Based Technology Probe to Support Poultry Farming Activities in Malawi,2021-06-28 00:00:00
491,Alex Mariakakis,"Rapid diagnostic tests are point-of-care medical tests that are used by clinicians and community healthcare workers to get quicker results at a better cost compared to traditional diagnostic tests. Distributing rapid diagnostic tests to people outside of the healthcare industry would significantly improve access to diagnostic testing; however, there are concerns that novices may administer rapid diagnostic tests incorrectly and thus be left with invalid results. In response to this concern, we propose RDTCheck—a mobile application that guides users through the instructions of Quidel’s QuickVue Influenza A+ B test and ensures adherence to the procedure using computer vision. RDTCheck provides users with real-time feedback so that they may either correct their mistakes or re-administer their test. In this work, we conducted findings from a pilot study that demonstrates how well RDTCheck is able to detect common",RDTCheck: A Smartphone App for Monitoring Rapid Diagnostic Test Administration,2021-05-08 00:00:00
492,Alex Mariakakis,"Sleep is critical to human function, mediating factors like memory, mood, energy, and alertness; therefore, it is commonly conjectured that a good night’s sleep is important for job performance. However, both real-world sleep behavior and job performance are difficult to measure at scale. In this work, we demonstrate that people’s everyday interactions with online mobile apps can reveal insights into their job performance in real-world contexts. We present an observational study in which we objectively tracked the sleep behavior and job performance of salespeople (N= 15) and athletes (N= 19) for 18 months, leveraging a mattress sensor and online mobile app to conduct the largest study of this kind to date. We first demonstrate that cumulative sleep measures are significantly correlated with job performance metrics, showing that an hour of daily sleep loss for a week was associated with a 9.0% average reduction in ",Online mobile app usage as an indicator of sleep behavior and job performance,2021-04-19 00:00:00
493,Alex Mariakakis,"Rapid diagnostic tests (RDTs) provide point-of-care medical screening without the need for expensive laboratory equipment. RDTs are theoretically straightforward to use, yet their analog colorimetric output leaves room for diagnostic uncertainty and error. Furthermore, RDT results within a community are kept isolated unless they are aggregated by healthcare workers, limiting the potential that RDTs can have in supporting public health efforts. In light of these issues, we present a system called RDTScan for detecting and interpreting lateral flow RDTs with a smartphone. RDTScan provides real-time guidance for clear RDT image capture and automatic interpretation for accurate diagnostic decisions. RDTScan is structured to be quickly configurable to new RDT designs by requiring only a template image and some metadata about how the RDT is supposed to be read, making it easier to extend than a data-driven",The design and evaluation of a mobile system for rapid diagnostic test interpretation,2021-03-29 00:00:00
494,Alex Mariakakis,"Super-resolution (SR) is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health. Existing SR algorithms rely on deep learning models with significant memory requirements, so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time. This shortcoming prevents existing SR methods from being used in applications that require near real-time latency. In this work, we demonstrate state-of-the-art latency and accuracy for on-device super-resolution using a novel hybrid architecture called SplitSR and a novel lightweight residual block called SplitSRBlock. The SplitSRBlock supports channel-splitting, allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension. SplitSR has a hybrid design consisting of standard convolutional blocks and lightweight residual",Splitsr: An end-to-end approach to super-resolution on mobile devices,2021-03-29 00:00:00
495,Alex Mariakakis,"Emoji suggestion systems based on typed text have been proposed to encourage emoji usage and enrich text messaging; however, such systems’ actual effects on the chat experience are unknown. We built an Android keyboard with both lexical (word-based) and semantic (meaning-based) emoji suggestion capabilities and compared these in two different studies. To investigate the effect of emoji suggestion in online conversations, we conducted a laboratory text-messaging study with 24 participants and a 15-day longitudinal field deployment with 18 participants. We found that participants picked more semantic suggestions than lexical suggestions and perceived the semantic suggestions as more relevant to the message content. Our subjective data showed that although the suggestion mechanism did not affect the chatting experience significantly, different mechanisms could change the composing ",A Comparative Study of Lexical and Semantic Emoji Suggestion Systems,2021-03-17 00:00:00
496,Alex Mariakakis,"One of the promising opportunities of digital health is its potential to lead to more holistic understandings of diseases by interacting with the daily life of patients and through the collection of large amounts of real world data. Validating and benchmarking indicators of disease severity in the home setting is difficult, however, given the large number of confounders present in the real world and the challenges in collecting ground truth data in the home. Here we leverage two datasets with continuous wrist-worn accelerometer data coupled with frequent symptom reports in the home setting, to develop digital biomarkers of symptom severity. Using these data, we performed a public benchmarking challenge in which participants were asked to build measures of severity across 3 symptoms (on/off medication, dyskinesia, and tremor). 42 teams participated and performance was improved over baseline models for each subchallenge. Additional ensemble modeling across submissions further improved performance, and the top models validated in a subset of patients whose symptoms were observed and rated by trained clinicians.",Developing better digital health measures of parkinson’s disease using free living data and a crowdsourced data analysis challenge,2021-01-01 00:00:00
497,Alex Mariakakis,"Diagnostic tests for influenza in Australia are currently only authorised for use in clinical settings. At-home diagnostic testing for influenza could reduce the need for patient contact with healthcare services, which potentially could contribute to symptomatic improvement and reduced spread of influenza. We aim to determine the accuracy of an app-guided nasal self-swab combined with a lateral flow immunoassay for influenza conducted by individuals with influenza-like illness (ILI).","Diagnostic accuracy of an app-guided, self-administered test for influenza among individuals presenting to general practice with influenza-like illness: study protocol",2020-11-01 00:00:00
498,Alex Mariakakis,"Year-round ultraviolet exposure silently causes skin damage that goes unnoticed until sunburn. Current personal wearables for monitoring UV exposure have not seen significant uptake, which may be attributed to their one-size-fits-all aesthetic or inapplicability to people with different skin tones. We present EcoPatches, inkjet-printable chemical patches that mediate a person’s relationship with their environment by allowing them to create designs and formulations that resonate with them. Supporting human-and machine-interpretability for EcoPatches’ visual changes means that users can glance at their EcoPatch during the day to see large exposure changes or take a picture of their EcoPatch with a smartphone app for more accurate and precise readings. We conducted an online survey to elicit visual design recommendations that support these features. We also evaluated both interpretation methods, finding that they achieved strong Pearson correlation coefficients with the EcoPatches’ known exposure levels (human: 0.79, app: 0.90).",EcoPatches: Maker-Friendly Chemical-Based UV Sensing,2020-07-03 00:00:00
499,Alex Mariakakis,superresolution sr is a coveted image processing technique for mobile apps ranging from the basic camera apps to mobile health existing sr algorithms rely on deep learning models with significant memory requirements so they have yet to be deployed on mobile devices and instead operate in the cloud to achieve feasible inference time this shortcoming prevents existing sr methods from being used in applications that require near realtime latency in this work we demonstrate stateoftheart latency and accuracy for ondevice superresolution using a novel hybrid architecture called splitsr and a novel lightweight residual block called splitsrblock the splitsrblock supports channelsplitting allowing the residual blocks to retain spatial information while reducing the computation in the channel dimension splitsr has a hybrid design consisting of standard convolutional blocks and lightweight residual blocks allowing people to tune splitsr for their computational budget we evaluate our system on a lowend arm cpu demonstrating both higher accuracy and up to times faster inference than previous approaches we then deploy our model onto a smartphone in an app called zoomsr to demonstrate the firstever instance of ondevice deep learningbased sr we conducted a user study with participants to have them assess the perceived quality of images that were postprocessed by splitsr relative to bilinear interpolation the existing standard for ondevice sr participants showed a statistically significant preference when looking at both images z p and text z p less,SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices,"20 January, 2021"
500,Sheila McIlraith,deep reinforcement learning has shown promise in discrete domains requiring complex reasoning including games such as chess go and hanabi however this type of reasoning is less often observed in longhorizon continuous domains with highdimensional observations where instead rl research has predominantly focused on problems with simple highlevel structure eg opening a drawer or moving a robot as fast as possible inspired by combinatorially hard optimization problems we propose a set of robotics tasks which admit many distinct solutions at the highlevel but require reasoning about states and rewards thousands of steps into the future for the best performance critically while rl has traditionally suffered on complex longhorizon tasks due to sparse rewards our tasks are carefully designed to be solvable without specialized exploration nevertheless our investigation finds that standard rl methods often neglect longterm effects due to discounting while generalpurpose hierarchical rl approaches struggle unless additional abstract domain knowledge can be exploited less,Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks,"3 June, 2022"
501,Sheila McIlraith,reinforcement learning rl methods usually treat reward functions as black boxes as such these methods must extensively interact with the environment in order to discover rewards and optimal policies in most rl applications however users have to program the reward function and hence there is the opportunity to make the reward function visible to show the reward functions code to the rl agent so it can exploit the functions internal structure to learn optimal policies in a more sample efficient manner in this paper we show how to accomplish this idea in two steps first we propose reward machines a type of finite state machine that supports the specification of reward functions while exposing reward function structure we then describe different methodologies to exploit this structure to support learning including automated reward shaping task decomposition and counterfactual reasoning with offpolicy learning experiments on tabular and continuous domains across different tasks and rl agents show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies finally by virtue of being a form of finite state machine reward machines have the expressive power of a regular language and as such support loops sequences and conditionals as well as the expression of temporally extended properties typical of linear temporal logic and nonmarkovian reward specification less,Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning,"17 January, 2022"
502,Sheila McIlraith,many ai applications involve the interaction of multiple autonomous agents requiring those agents to reason about their own beliefs as well as those of other agents however planning involving nested beliefs is known to be computationally challenging in this work we address the task of synthesizing plans that necessitate reasoning about the beliefs of other agents we plan from the perspective of a single agent with the potential for goals and actions that involve nested beliefs nonhomogeneous agents copresent observations and the ability for one agent to reason as if it were another we formally characterize our notion of planning with nested belief and subsequently demonstrate how to automatically convert such problems into problems that appeal to classical planning technology for solving efficiently our approach represents an important step towards applying the wellestablished field of automated planning to the challenging task of planning involving nested beliefs of multiple agents less,Efficient Multi-agent Epistemic Planning: Teaching Planners About Nested Belief,"5 October, 2021"
503,Sheila McIlraith,human beings even small children quickly become adept at figuring out how to use applications on their mobile devices learning to use a new app is often achieved via trialanderror accelerated by transfer of knowledge from past experiences with like apps the prospect of building a smarter smartphone one that can learn how to achieve tasks using mobile apps is tantalizing in this paper we explore the use of reinforcement learning rl with the goal of advancing this aspiration we introduce an rlbased framework for learning to accomplish tasks in mobile apps rl agents are provided with states derived from the underlying representation of onscreen elements and rewards that are based on progress made in the task agents can interact with screen elements by tapping or typing our experimental results over a number of mobile apps show that rl agents can learn to accomplish multistep tasks as well as achieve modest generalization across different apps more generally we develop a platform which addresses several engineering challenges to enable an effective rl training environment our appbuddy platform is compatible with openai gym and includes a suite of mobile apps and benchmark tasks that supports a diversity of rl research in the mobile app setting less,AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement Learning,"6 June, 2021"
504,Sheila McIlraith,learning taskagnostic dynamics models in highdimensional observation spaces can be challenging for modelbased rl agents we propose a novel way to learn latent world models by learning to predict sequences of future actions conditioned on task completion these taskconditioned models adaptively focus modeling capacity on taskrelevant dynamics while simultaneously serving as an effective heuristic for planning with sparse rewards we evaluate our method on challenging visual goal completion tasks and show a substantial increase in performance compared to prior modelfree approaches less,Planning from Pixels using Inverse Dynamics Models,"4 December, 2020"
505,Sheila McIlraith,reinforcement learning rl agents typically learn memoryless policiespolicies that only consider the last observation when selecting actions learning memoryless policies is efficient and optimal in fully observable environments however some form of memory is necessary when rl agents are faced with partial observability in this paper we study a lightweight approach to tackle partial observability in rl we provide the agent with an external memory and additional actions to control what if anything is written to the memory at every step the current memory state is part of the agents observation and the agent selects a tuple of actions one action that modifies the environment and another that modifies the memory when the external memory is sufficiently expressive optimal memoryless policies yield globally optimal solutions unfortunately previous attempts to use external memory in the form of binary memory have produced poor results in practice here we investigate alternative forms of memory in support of learning effective memoryless policies our novel forms of memory outperform binary and lstmbased memory in wellestablished partially observable domains less,The act of remembering: a study in partially observable reinforcement learning,"4 October, 2020"
506,Sheila McIlraith,synthesizing a program that realizes a logical specification is a classical problem in computer science we examine a particular type of program synthesis where the objective is to synthesize a strategy that reacts to a potentially adversarial environment while ensuring that all executions satisfy a linear temporal logic ltl specification unfortunately exact methods to solve socalled ltl synthesis via logical inference do not scale in this work we cast ltl synthesis as an optimization problem we employ a neural network to learn a qfunction that is then used to guide search and to construct programs that are subsequently verified for correctness our method is unique in combining search with deep learning to realize ltl synthesis in our experiments the learned qfunction provides effective guidance for synthesis problems with relatively small specifications less,Towards Neural-Guided Program Synthesis for Linear Temporal Logic Specifications,"31 December, 2019"
507,Sheila McIlraith,in this paper we investigate the problem of synthesizing strategies for linear temporal logic ltl specifications that are interpreted over finite traces a problem that is central to the automated construction of controllers robot programs and business processes we study a natural variant of the finite ltl synthesis problem in which strategy guarantees are predicated on specified environment behavior we further explore a quantitative extension of ltl that supports specification of quality measures utilizing it to synthesize highquality strategies we propose new notions of optimality and associated algorithms that yield strategies that best satisfy specified quality measures our algorithms utilize an automatagame approach positioning them well for future implementation via existing stateoftheart techniques less,Finite LTL Synthesis with Environment Assumptions and Quality Measures,"31 August, 2018"
508,Sheila McIlraith,the reverse water gas shift system rwgs is a complex physical system designed to produce oxygen from the carbon dioxide atmosphere on mars if sent to mars it would operate without human supervision thus requiring a reliable automated system for monitoring and control the rwgs presents many challenges typical of realworld systems including noisy and biased sensors nonlinear behavior effects that are manifested over different time granularities and unobservability of many important quantities in this paper we model the rwgs using a hybrid discretecontinuous dynamic bayesian network dbn where the state at each time slice contains discrete and continuous variables we show how the system state can be tracked using probabilistic inference over the model we discuss how to deal with the various challenges presented by the rwgs providing a suite of techniques that are likely to be useful in a wide range of applications in particular we describe a general framework for dealing with nonlinear behavior using numerical integration techniques extending the successful unscented filter we also show how to use a fixedpoint computation to deal with effects that develop at different time scales specifically rapid changes occurring during slowly changing processes we test our model using real data collected from the rwgs demonstrating the feasibility of hybrid dbns for monitoring complex realworld physical systems less,Monitoring a Complez Physical System using a Hybrid Dynamic Bayes Net,"12 December, 2012"
509,Sheila McIlraith,in this paper we address the problem of generating preferred plans by combining the procedural control knowledge specified by hierarchical task networks htns with rich qualitative user preferences the outcome of our work is a language for specifyin user preferences tailored to htn planning together with a provably optimal preferencebased planner htnplan that is implemented as an extension of shop to compute preferred plans we propose an approach based on forwardchaining heuristic search our heuristic uses an admissible evaluation function measuring the satisfaction of preferences over partial plans our empirical evaluation demonstrates the effectiveness of our htnplan heuristics we prove our approach sound and optimal with respect to the plans it generates by appealing to a situation calculus semantics of our preference language and of htn planning while our implementation builds on shop the language and techniques proposed here are relevant to a broad range of htn planners less,On Planning with Preferences in HTN,"3 September, 2009"
510,Maryam Mehri Dehnavi,this work proposes a timeefficient natural gradient descent method called tengrad with linear convergence guarantees computing the inverse of the neural networks fisher information matrix is expensive in ngd because the fisher matrix is large approximate ngd methods such as kfac attempt to improve ngds running time and practical application by reducing the fisher matrix inversion cost with approximation however the approximations do not reduce the overall time significantly and lead to less accurate parameter updates and loss of curvature information tengrad improves the time efficiency of ngd by computing fisher block inverses with a computationally efficient covariance factorization and reuse method it computes the inverse of each block exactly using the woodbury matrix identity to preserve curvature information while admitting linear fast convergence rates our experiments on image classification tasks for stateoftheart deep neural architecture on cifar cifar and fashionmnist show that tengrad significantly outperforms stateoftheart ngd methods and often stochastic gradient descent in wallclock time less,TENGraD: Time-Efficient Natural Gradient Descent with Exact Fisher-Block Inversion,"3 March, 2022"
511,Maryam Mehri Dehnavi,sparse fusion is a compiletime loop transformation and runtime scheduling implemented as a domainspecific code generator sparse fusion generates efficient parallel code for the combination of two sparse matrix kernels where at least one of the kernels has loopcarried dependencies available implementations optimize individual sparse kernels when optimized separately the irregular dependence patterns of sparse kernels create synchronization overheads and load imbalance and their irregular memory access patterns result in inefficient cache usage which reduces parallel efficiency sparse fusion uses a novel inspection strategy with code transformations to generate parallel fused code for sparse kernel combinations that is optimized for data locality and load balance code generated by sparse fusion outperforms the existing implementations parsy and mkl on average x and x respectively and outperforms the lbc and dagp coarsening strategies applied to a fused data dependence graph on average x and x respectively for various kernel combinations less,Composing Loop-carried Dependence with Other Loops,"23 November, 2021"
512,Maryam Mehri Dehnavi,this work proposes a distributed algorithm for solving empirical risk minimization problems called ldqn under the masterworker communication model ldqn is a distributed limitedmemory quasinewton method that supports asynchronous computations among the worker nodes our method is efficient both in terms of storage and communication costs ie in every iteration the master node and workers communicate vectors of size od where d is the dimension of the decision variable and the amount of memory required on each node is omd where m is an adjustable parameter to our knowledge this is the first distributed quasinewton method with provable global linear convergence guarantees in the asynchronous setting where delays between nodes are present numerical experiments are provided to illustrate the theory and the practical performance of our method less,L-DQN: An Asynchronous Limited-Memory Distributed Quasi-Newton Method,"4 September, 2021"
513,Maryam Mehri Dehnavi,"This work proposes a framework called FuSy that analyzes the data dependence graphs (DAGs) of two sparse kernels and creates an efficient schedule to execute the kernels in combination. Sparse kernels are frequently used in scientific codes and in machine learning algorithms and very often they are used in combination. Iterative linear system solvers are an example where kernels such as sparse triangular solver (SpTRSV) and sparse matrix-vector multiplication (SpMV) are called consecutively in each iteration of the solver. Prior approaches typically optimize these sparse kernels independently leading to high synchronization overheads and low locality. We propose an approach that analyzes the DAGs of two sparse kernels and then creates a new order of execution that enables running the two kernels efficiently in parallel. To investigate the efficiency of our approach, we compare it with the state-of-the-art",Optimizing sparse computations jointly,2022-04-02 00:00:00
514,Maryam Mehri Dehnavi,"In recent decades, parallel computing has been increasingly applied to address the computational challenges of calibrating watershed hydrologic models. The purpose of this paper is to review these parallelization studies to summarize their contributions, identify knowledge gaps, and propose future research directions. These studies parallelized models based on either random-sampling-based algorithms or optimization algorithms and demonstrated considerable parallel speedup gain and parallel efficiency. However, the speedup gain/efficiency decreases as the number of parallel processing units increases, particularly after a threshold. In future, various combinations of hydrologic models, optimization algorithms, parallelization strategies, parallelization architectures, and communication modes need to be implemented to systematically evaluate a suite of parallelization scenarios for improving speedup gain ",A review of parallel computing applications in calibrating watershed hydrologic models,2022-03-10 00:00:00
515,Maryam Mehri Dehnavi,"The effective resistance (ER) between a pair of nodes in a weighted undirected graph is defined as the potential difference induced when a unit current is injected at one node and extracted from the other, treating edge weights as the conductance values of edges. The ER is a key quantity of interest in many applications, e.g., solving linear systems, Markov chains, and continuous-time averaging networks. In this article, we consider ERs in the context of designing randomized gossiping methods for the consensus problem, where the aim is to compute the average of node values in a distributed manner through iteratively computing weighted averages among randomly chosen neighbors. For barbell graphs, we prove that choosing wake-up and communication probabilities proportional to ER weights improves the averaging time corresponding to the traditional choice of uniform weights. For -barbell graphs, we show",Randomized Gossiping with Effective Resistance Weights: Performance Guarantees and Applications,2022
516,Maryam Mehri Dehnavi,"This work proposes a distributed algorithm for solving empirical risk minimization problems, called L-DQN, under the master/worker communication model. L-DQN is a distributed limited-memory quasi-Newton method that supports asynchronous computations among the worker nodes. Our method is efficient both in terms of storage and communication costs, i.e., in every iteration, the master node and workers communicate vectors of size O(d), where d is the dimension of the decision variable, and the amount of memory required on each node is O(md), where m is an adjustable parameter. To our knowledge, this is the first distributed quasi-Newton method with provable global linear convergence guarantees in the asynchronous setting where delays between nodes are present. Numerical experiments are provided to illustrate the theory and the practical performance of our method.",L-DQN: An Asynchronous Limited-Memory Distributed Quasi-Newton Method,2021-12-14 00:00:00
517,Maryam Mehri Dehnavi,"Sparse fusion is a compile-time loop transformation and runtime scheduling implemented as a domain-specific code generator. Sparse fusion generates efficient parallel code for the combination of two sparse matrix kernels where at least one of the kernels has loop-carried dependencies. Available implementations optimize individual sparse kernels. When optimized separately, the irregular dependence patterns of sparse kernels create synchronization overheads and load imbalance, and their irregular memory access patterns result in inefficient cache usage, which reduces parallel efficiency. Sparse fusion uses a novel inspection strategy with code transformations to generate parallel fused code for sparse kernel combinations that is optimized for data locality and load balance. Code generated by Sparse fusion outperforms the existing implementations ParSy and MKL on average 1.6X and 5.1X respectively and outperforms the LBC and DAGP coarsening strategies applied to a fused data dependence graph on average 5.1X and 7.2X respectively for various kernel combinations.",Composing Loop-carried Dependence with Other Loops,2021-11-24 00:00:00
518,Maryam Mehri Dehnavi,"This work proposes a time-efficient Natural Gradient Descent method, called TENGraD, with linear convergence guarantees. Computing the inverse of the neural network's Fisher information matrix is expensive in NGD because the Fisher matrix is large. Approximate NGD methods such as KFAC attempt to improve NGD's running time and practical application by reducing the Fisher matrix inversion cost with approximation. However, the approximations do not reduce the overall time significantly and lead to less accurate parameter updates and loss of curvature information. TENGraD improves the time efficiency of NGD by computing Fisher block inverses with a computationally efficient covariance factorization and reuse method. It computes the inverse of each block exactly using the Woodbury matrix identity to preserve curvature information while admitting (linear) fast convergence rates. Our experiments on image classification tasks for state-of-the-art deep neural architecture on CIFAR-10, CIFAR-100, and Fashion-MNIST show that TENGraD significantly outperforms state-of-the-art NGD methods and often stochastic gradient descent in wall-clock time.",TENGraD: Time-Efficient Natural Gradient Descent with Exact Fisher-Block Inversion,2021-06-07 00:00:00
519,Maryam Mehri Dehnavi,"Quadratic programs (QP), minimizations of quadratic objectives subject to linear inequality and equality constraints, are at the heart of algorithms across scientific domains. Applications include fundamental tasks in geometry processing, simulation, engineering, animation and finance where the accurate, reliable, efficient, and scalable solution of QP problems is critical. However, available QP algorithms generally provide either accuracy or scalability - but not both. Some algorithms reliably solve QP problems to high accuracy but work only for smaller-scale QP problems due to their reliance on dense matrix methods. Alternately, many other QP solvers scale well via sparse, efficient algorithms but cannot reliably deliver solutions at requested accuracies. Towards addressing the need for accurate and efficient QP solvers at scale, we develop NASOQ, a new, full-space QP algorithm that provides accurate, efficient, and ",NASOQ: numerically accurate sparsity-oriented QP solver,2020-07-08 00:00:00
520,Maryam Mehri Dehnavi,"In this paper, we consider distributed algorithms for solving the empirical risk minimization problem under the master/worker communication model. We develop a distributed asynchronous quasi-Newton algorithm that can achieve superlinear convergence. To our knowledge, this is the first distributed asynchronous algorithm with superlinear convergence guarantees. Our algorithm is communication-efficient in the sense that at every iteration the master node and workers communicate vectors of size , where  is the dimension of the decision variable. The proposed method is based on a distributed asynchronous averaging scheme of decision vectors and gradients in a way to effectively capture the local Hessian information of the objective function. Our convergence theory supports asynchronous computations subject to both bounded delays and unbounded delays with a bounded time-average. Unlike in the majority of asynchronous optimization literature, we do not require choosing smaller stepsize when delays are huge. We provide numerical experiments that match our theoretical results and showcase significant improvement comparing to state-of-the-art distributed algorithms.",DAve-QN: A distributed averaged quasi-Newton method with local superlinear convergence rate,2020-06-03 00:00:00
521,Maryam Mehri Dehnavi,"Hierarchical matrix approximations have gained significant traction in the machine learning and scientific community as they exploit available low-rank structures in kernel methods to compress the kernel matrix. The resulting compressed matrix, HMatrix, is used to reduce the computational complexity of operations such as HMatrix-matrix multiplications with tuneable accuracy in an evaluation phase. Existing implementations of HMatrix evaluations do not preserve locality and often lead to unbalanced parallel execution with high synchronization. Also, current solutions require the compression phase to re-execute if the kernel method or the required accuracy change. MatRox is a framework that uses novel structure analysis strategies with code specialization and a storage format to improve locality and create load-balanced parallel tasks for HMatrix-matrix multiplications. Modularization of the matrix compression",MatRox: Modular approach for improving data locality in Hierarchical (Mat)rix App(Rox)imation,2020-02-19 00:00:00
522,Maryam Mehri Dehnavi,"ASYNC is a framework that supports the implementation of asynchrony and history for optimization methods on distributed computing platforms. The popularity of asynchronous optimization methods has increased in distributed machine learning. However, their applicability and practical experimentation on distributed systems are limited because current bulk-processing cloud engines do not provide a robust support for asynchrony and history. With introducing three main modules and bookkeeping system-specific and application parameters, ASYNC provides practitioners with a framework to implement asynchronous machine learning methods. To demonstrate ease-of-implementation in ASYNC, the synchronous and asynchronous variants of two well-known optimization methods, stochastic gradient descent and SAGA, are demonstrated in ASYNC.",ASYNC: A Cloud Engine with Asynchrony and History for Distributed Machine Learning,2020
523,Maryam Mehri Dehnavi,"This paper presents a combined compile-time and runtime loop-carried dependence analysis of sparse matrix codes and evaluates its performance in the context of wavefront parallellism. Sparse computations incorporate indirect memory accesses such as x [col [j]] whose memory locations cannot be determined until runtime. The key contributions of this paper are two compile-time techniques for significantly reducing the overhead of runtime dependence testing:(1) identifying new equality constraints that result in more efficient runtime inspectors, and (2) identifying subset relations between dependence constraints such that one dependence test subsumes another one that is therefore eliminated. New equality constraints discovery is enabled by taking advantage of domain-specific knowledge about index arrays, such as col [j]. These simplifications lead to automatically-generated inspectors that make it practical to ",Sparse computation data dependence simplification for efficient compiler-generated inspectors,2019-06-08 00:00:00
524,Maryam Mehri Dehnavi,"We present MatRox, a novel model-based algorithm and implementation of Hierarchically Semi-Separable (HSS) matrix computations on parallel architectures. MatRox uses a novel storage format to improve data locality and scalability of HSS matrix-matrix multiplications on shared memory multicore processors. We build a performance model for HSS matrixmatrix multiplications. Based on the performance model, a mixed-rank heuristic is introduced to find an optimal HSS-tree depth for a faster HSS matrix evaluation. Uniform sampling is used to improve the performance of HSS compression. MatRox outperforms state-of-the-art HSS matrix multiplication codes, GOFMM and STRUMPACK, with average speedups of 2.8× and 6.1× respectively on target multicore processors.",A Model-Based Algorithm with an Efficient Storage Format for Parallel HSS-Structured Matrix Approximations,2018-12-18 00:00:00
525,Maryam Mehri Dehnavi,"In this work, we describe ParSy, a framework that uses a novel inspection strategy along with a simple code transformation to optimize parallel sparse algorithms for shared memory processors. Unlike existing approaches that can suffer from load imbalance and excessive synchronization, ParSy uses a novel task coarsening strategy to create well-balanced tasks that can execute in parallel, while maintaining locality of memory accesses. Code using the ParSy inspector and transformation outperforms existing highly-optimized sparse matrix algorithms such as Cholesky factorization on multi-core processors with speedups of 2.8× and 3.1× over the MKL Pardiso and PaStiX libraries respectively.",ParSy: Inspection and transformation of sparse matrix computations for parallelism,2018-11-11 00:00:00
526,Maryam Mehri Dehnavi,"Automatic parallelization is an approach where a compiler analyzes serial code and identifies computations that can be rewritten to leverage parallelism. Many data dependence analysis techniques have been developed to determine which loops in a code can be parallelized. With code that includes indirect array accesses through what are commonly called index arrays, such data dependence analysis is restricted in the conclusions that can be drawn at compile time. Various approaches that use index array properties such as monotonicity have been shown to more effectively find parallel loops. In this paper, we extend the kinds of properties about index arrays that can be expressed, show how to convert loop-carried data dependence relations and relevant index-array properties to constraints that can be provided to the Z3 SMT solver, and evaluate the impact of using such index-array properties on identifying",Extending index-array properties for data dependence analysis,2018-10-09 00:00:00
527,Maryam Mehri Dehnavi,sparse tensors appear in many largescale applications with multidimensional and sparse data while multidimensional sparse data often need to be processed on manycore processors attempts to develop highlyoptimized gpubased implementations of sparse tensor operations are rare the irregular computation patterns and sparsity structures as well as the large memory footprints of sparse tensor operations make such implementations challenging we leverage the fact that sparse tensor operations share similar computation patterns to propose a unified tensor representation called fcoo combined with gpuspecific optimizations fcoo provides highlyoptimized implementations of sparse tensor computations on gpus the performance of the proposed unified approach is demonstrated for tensorbased kernels such as the sparse matricized tensor timeskhatrirao product spmttkrp and the sparse tensor timesmatrix multiply spttm and is used in tensor decomposition algorithms compared to stateoftheart work we improve the performance of spttm and spmttkrp up to and times respectively on nvidia titanx gpus we implement a candecompparafac cp decomposition and achieve up to times speedup using the unified method over stateoftheart libraries on nvidia titanx gpus less,A Unified Optimization Approach for Sparse Tensor Operations on GPUs,"28 May, 2017"
528,Michael Molloy,"The cochromatic number Z(G) of a graph G is the fewest number of colors needed to color the vertices of G so that each color class is a clique or an independent set. In a fractional cocoloring of G a non-negative weight is assigned to each clique and independent set so that for each vertex v, the sum of the weights of all cliques and independent sets containing v is at least one. The smallest total weight of such a fractional cocoloring of G is the fractional cochromatic number . In this paper we prove results for the fractional cochromatic number  that parallel results for Z(G) and the well studied fractional chromatic number . For example  when G is triangle-free, except when the only nontrivial component of G is a star. More generally, if G contains no k-clique, then , where R(k, k) is the minimum integer n such that every n-vertex graph has a k-clique or an independent set of size k",Fractional cocoloring of graphs,2019-06-13 00:00:00
529,Michael Molloy,"We study a random system of cn linear equations over n variables in GF(2), where each equation contains exactly r variables; this is equivalent to r‐XORSAT. Previous work has established a clustering threshold, for this model: if for any constant then with high probability all solutions form a well‐connected cluster; whereas if , then with high probability the solutions partition into well‐connected, well‐separated clusters (with probability tending to 1 as ). This is part of a general clustering phenomenon which is hypothesized to arise in most of the commonly studied models of random constraint satisfaction problems, via sophisticated but mostly nonrigorous techniques from statistical physics. We extend that study to the range , and prove that the connectivity parameters of the r‐XORSAT clusters undergo a smooth transition around the clustering threshold.",Inside the clustering window for random linear equations,2018/3
530,Michael Molloy,"We determine the exact value of the freezing threshold, rfk, for k-colourings of a random graph when k≥ 14. We prove that for random graphs with density above rfk, almost every colouring is such that a linear number of vertices are frozen, meaning that their colour cannot be changed by a sequence of alterations whereby we change the colours of o(n) vertices at a time, always obtaining another proper colouring. When the density is below rfk, then almost every colouring is such that every vertex can be changed by a sequence of alterations where we change O(log n) vertices at a time.",The Freezing Threshold for k-Colourings of a Random Graph,2018-02-16 00:00:00
531,Michael Molloy,"We study a random system of  linear equations over  variables in GF(2), where each equation contains exactly  variables; this is equivalent to -XORSAT. \cite{ikkm,amxor} determined the clustering threshold, : if $c=c^*_r+\e$ for any constant $\e>0$, then \aas the solutions partition into well-connected, well-separated {\em clusters} (with probability tending to 1 as ). This is part of a general clustering phenomenon which is hypothesized to arise in most of the commonly studied models of random constraint satisfaction problems, via sophisticated but mostly non-rigorous techniques from statistical physics. We extend that study to the range , showing that if $c=c^*_r+n^{-\d}, \d>0$, then the connectivity parameter of each -XORSAT cluster is $n^{\Theta(\d)}$, as compared to  when $c=c^*_r+\e$. This means that one can move between any two solutions in the same cluster via a sequence of solutions where consecutive solutions differ on at most $n^{\Theta(\d)}$ variables; this is tight up to the implicit constant. In contrast, moving to a solution in another cluster requires that some pair of consecutive solutions differ in at least $n^{1-O(\d)}$ variables. Along the way, we prove that in a random -uniform hypergraph with edge-density $n^{-\d}$ above the -core threshold, \aas every vertex not in the -core can be removed by a sequence of $n^{\Theta(\d)}$ vertex-deletions in which the deleted vertex has degree less than ; again, this is tight up to the implicit constant.",Inside the clustering threshold for random linear equations,2013-09-19 00:00:00
532,Michael Molloy,"We consider random instances of constraint satisfaction problems (CSPs) where each variable has domain size , each constraint is on  variables, and the constraints are chosen from a specified distribution. The number of constraints is , where  is a constant. We prove that for every possible distribution, either the resolution complexity is almost surely polylogarithmic for sufficiently large , or it is almost surely exponential for every . We characterize the distributions of each type. To do so, we introduce a closure operation on constraint sets which yields the set of all constraints that, in some sense, appear implicitly in the random CSP.",A Dichotomy Theorem for the Resolution Complexity of Random Constraint Satisfaction Problems,2013-01-10 00:00:00
533,Michael Molloy,"Viral spread on large graphs has many real-life applications such as malware propagation in computer networks and rumor (or misinformation) spread in Twitter-like online social networks. Although viral spread on large graphs has been intensively analyzed on classical models such as Susceptible–Infectious–Recovered, there still exits a deficit of effective methods in practice to contain epidemic spread once it passes a critical threshold. Against this backdrop, we explore methods of containing viral spread in large networks with the focus on sparse random networks. The viral containment strategy is to partition a large network into small components and then to ensure that all messages delivered across different components are free of infection. With such a defense mechanism in place, an epidemic spread starting from any node is limited to only those nodes belonging to the same component as the initial infection ","Containing Viral Spread on Sparse Random Graphs: Bounds, Algorithms, and Experiments",2013-01-01 00:00:00
534,Michael Molloy,"We determine the exact freezing threshold, rf, for a family of models of random boolean constraint satisfaction problems, including NAE-SAT and hypergraph 2-colouring, when the constraint size is sufficiently large. If the constraint-density of a random CSP, F, in our family is greater than rf then for almost every solution of F, a linear number of variables are frozen, meaning that their colours cannot be changed by a sequence of alterations in which we change o(n) variables at a time, always switching to another solution. If the constraint-density is less than rf, then almost every solution has o(n) frozen variables.",Frozen variables in random boolean constraint satisfaction problems,2012-09-21 00:00:00
535,Michael Molloy,"We determine the exact threshold of satisfiability for random instances of a particular NP-complete constraint satisfaction problem (CSP). This is the first random CSP model for which we have determined a precise linear satisfiability threshold, and for which random instances with density near that threshold appear to be computationally difficult. More formally, it is the first random CSP model for which the satisfiability threshold is known and which shares the following characteristics with random -SAT for : The problem is NP-complete, the satisfiability threshold occurs when there is a linear number of clauses, and a uniformly random instance with a linear number of clauses asymptotically almost surely has exponential resolution complexity.",The satisfiability threshold for a seemingly intractable random constraint satisfaction problem,2012-05-29 00:00:00
536,Michael Molloy,"We rigorously determine the exact freezing threshold, r k f, for k-colourings of a random graph. We prove that for random graphs with density above r k f, almost every colouring is such that a linear number of variables are frozen, meaning that their colours cannot be changed by a sequence of alterations whereby we change the colours of o (n) vertices at a time, always obtaining another proper colouring. When the density is below r k f, then almost every colouring has at most o (n) frozen variables. This confirms hypotheses made using the non-rigorous cavity method. It has been hypothesized that the freezing threshold is the cause of the"" algorithmic barrier"", the long observed phenomenon that when the edge-density of a random graph exceeds hf k ln k (1+ o k (1)), no algorithms are known to find k-colourings, despite the fact that this density is only half the k-colourability threshold.",The freezing threshold for k-colourings of a random graph,2012-05-19 00:00:00
537,Michael Molloy,the adaptable choosability of a multigraph g denoted mathrmchag is the smallest integer k such that any edge labelling of g and any assignment of lists of size k to the vertices of g permits a list colouring of g such that there is no edge e uv where e u v here we show that for a multigraph g with maximum degree and no cycles of length or mathrmchag leq sqrtosqrtln under natural restrictions we can show that the same bound holds for the conflict choosability of g which is a closely related parameter defined by dvok esperet kang and ozeki arxiv less,Adaptable and conflict colouring multigraphs with no cycles of length three or four,"9 July, 2021"
538,Michael Molloy,a key challenge in visual place recognition vpr is recognizing places despite drastic visual appearance changes due to factors such as time of day season weather or lighting conditions numerous approaches based on deeplearnt image descriptors sequence matching domain translation and probabilistic localization have had success in addressing this challenge but most rely on the availability of carefully curated representative reference images of the possible places in this paper we propose a novel approach dubbed bayesian selective fusion for actively selecting and fusing informative reference images to determine the best place match for a given query image the selective element of our approach avoids the counterproductive fusion of every reference image and enables the dynamic selection of informative reference images in environments with changing visual conditions such as indoors with flickering lights outdoors during sunshowers or over the daynight cycle the probabilistic element of our approach provides a means of fusing multiple reference images that accounts for their varying uncertainty via a novel trainingfree likelihood function for vpr on difficult query images from two benchmark datasets we demonstrate that our approach matches and exceeds the performance of several alternative fusion approaches along with stateoftheart techniques that are provided with prior unfair knowledge of the best reference images our approach is well suited for longterm robot autonomy where dynamic visual environments are commonplace since it is trainingfree descriptoragnostic and complements existing techniques such as sequence matching less,Intelligent Reference Curation for Visual Place Recognition via Bayesian Selective Fusion,"3 January, 2021"
539,Michael Molloy,we study a random system of cn linear equations over n variables in gf where each equation contains exactly r variables this is equivalent to rxorsat previous work has established a clustering threshold cr for this model if ccrfor any constant then with high probability all solutions form a wellconnected cluster whereas if ccr then with high probability the solutions partition into wellconnected wellseparated clusters with probability tending to as n goes to infinity this is part of a general clustering phenomenon which is hypothesized to arise in most of the commonly studied models of random constraint satisfaction problems via sophisticated but mostly nonrigorous techniques from statistical physics we extend that study to the range ccro and prove that the connectivity parameters of the rxorsat clusters undergo a smooth transition around the clustering threshold less,Inside the clustering window for random linear equations,"1 February, 2017"
540,Michael Molloy,viral spread on large graphs has many reallife applications such as malware propagation in computer networks and rumor or misinformation spread in twitterlike online social networks although viral spread on large graphs has been intensively analyzed on classical models such as susceptibleinfectiousrecovered there still exits a deficit of effective methods in practice to contain epidemic spread once it passes a critical threshold against this backdrop we explore methods of containing viral spread in large networks with the focus on sparse random networks the viral containment strategy is to partition a large network into small components and then to ensure the sanity of all messages delivered across different components with such a defense mechanism in place an epidemic spread starting from any node is limited to only those nodes belonging to the same component as the initial infection node we establish both lower and upper bounds on the costs of inspecting intercomponent messages we further propose heuristicbased approaches to partition large input graphs into small components finally we study the performance of our proposed algorithms under different network topologies and different edge weight models less,"Containing Viral Spread on Sparse Random Graphs: Bounds, Algorithms, and Experiments","7 October, 2013"
541,Aleksandar Nikolov,we prove a tight lower bound up to constant factors on the sample complexity of any noninteractive local differentially private protocol for optimizing a linear function over the simplex this lower bound also implies a tight lower bound again up to constant factors on the sample complexity of any noninteractive local differentially private protocol implementing the exponential mechanism these results reveal that any local protocol for these problems has exponentially worse dependence on the dimension than corresponding algorithms in the central model previously kasiviswanathan et al focs proved an exponential separation between local and central model algorithms for pac learning the class of parity functions in contrast our lower bound are quantitatively tight apply to a simple and natural class of linear optimization problems and our techniques are arguably simpler less,Tight Lower Bounds for Locally Differentially Private Selection,"14 May, 2021"
542,Aleksandar Nikolov,many problems in computer science and applied mathematics require rounding a vector mathbfw of fractional values lying in the interval to a binary vector mathbfx so that for a given matrix mathbfa mathbfamathbfx is as close to mathbfamathbfw as possible for example this problem arises in lp rounding algorithms used to approximate mathsfnphard optimization problems and in the design of uniformly distributed point sets for numerical integration for a given matrix mathbfa the worstcase error over all choices of mathbfw incurred by the best possible rounding is measured by the linear discrepancy of mathbfa a quantity studied in discrepancy theory and introduced by lovasz spencer and vesztergombi ejc we initiate the study of the computational complexity of linear discrepancy our investigation proceeds in two directions proving hardness results and finding both exact and approximate algorithms to evaluate the linear discrepancy of certain matrices for we show that linear discrepancy is mathsfnphard thus we do not expect to find an efficient exact algorithm for the general case restricting our attention to matrices with a constant number of rows we present a polytime exact algorithm for matrices consisting of a single row and matrices with a constant number of rows and entries of bounded magnitude we also present an exponentialtime approximation algorithm for general matrices and an algorithm that approximates linear discrepancy to within an exponential factor less,On the Computational Complexity of Linear Discrepancy,"31 July, 2020"
543,Aleksandar Nikolov,we study the problem of differentially private query release assisted by access to public data in this problem the goal is to answer a large class mathcalh of statistical queries with error no more than using a combination of public and private samples the algorithm is required to satisfy differential privacy only with respect to the private samples we study the limits of this task in terms of the private and public sample complexities first we show that we can solve the problem for any query class mathcalh of finite vcdimension using only d public samples and sqrtpd private samples where d and p are the vcdimension and dual vcdimension of mathcalh respectively in comparison with only private samples this problem cannot be solved even for simple query classes with vcdimension one and without any private samples a larger public sample of size d is needed next we give sample complexity lower bounds that exhibit tight dependence on p and for the class of decision stumps we give a lower bound of sqrtp on the private sample complexity whenever the public sample size is less than given our upper bounds this shows that the dependence on sqrtp is necessary in the private sample complexity we also give a lower bound of on the public sample complexity for a broad family of query classes which by our upper bound is tight in less,Private Query Release Assisted by Public Data,"22 April, 2020"
544,Aleksandar Nikolov,we give new characterizations of the sample complexity of answering linear queries statistical queries in the local and central models of differential privacy in the noninteractive local model we give the first approximate characterization of the sample complexity informally our bounds are tight to within polylogarithmic factors in the number of queries and desired accuracy our characterization extends to agnostic learning in the local model in the central model we give a characterization of the sample complexity in the highaccuracy regime that is analogous to that of nikolov talwar and zhang stoc but is both quantitatively tighter and has a dramatically simpler proof our lower bounds apply equally to the empirical and population estimation problems in both cases our characterizations show that a particular factorization mechanism is approximately optimal and the optimal sample complexity is bounded from above and below by well studied factorization norms of a matrix associated with the queries less,The Power of Factorization Mechanisms in Local and Central Differential Privacy,"19 November, 2019"
545,Aleksandar Nikolov,in the geometric transportation problem we are given a collection of points p in ddimensional euclidean space and each point is given a supply of p units of mass where p could be a positive or a negative integer and the total sum of the supplies is the goal is to find a flow called a transportation map that transports p units from any point p with p and transports p units into any point p with p moreover the flow should minimize the total distance traveled by the transported mass the optimal value is known as the transportation cost or the earth movers distance from the points with positive supply to those with negative supply this problem has been widely studied in many fields of computer science from theoretical work in computational geometry to applications in computer vision graphics and machine learning in this work we study approximation algorithms for the geometric transportation problem we give an algorithm which for any fixed dimension d finds a varepsilonapproximate transportation map in time nearlylinear in n and polynomial in varepsilon and in the logarithm of the total supply this is the first approximation scheme for the problem whose running time depends on n as ncdot mathrmpolylogn our techniques combine the generalized preconditioning framework of sherman which is grounded in continuous optimization with simple geometric arguments to first reduce the problem to a minimum cost flow problem on a sparse graph and then to design a good preconditioner for this latter problem less,Preconditioning for the Geometric Transportation Problem,"22 February, 2019"
546,Aleksandar Nikolov,we study efficient mechanisms for the query release problem in differential privacy given a workload of m statistical queries output approximate answers to the queries while satisfying the constraints of differential privacy in particular we are interested in mechanisms that optimally adapt to the given workload building on the projection mechanism of nikolov talwar and zhang and using the ideas behind dudleys chaining inequality we propose new efficient algorithms for the query release problem and prove that they achieve optimal sample complexity for the given workload up to constant factors in certain parameter regimes with respect to the class of mechanisms that satisfy concentrated differential privacy we also give variants of our algorithms that satisfy local differential privacy and prove that they also achieve optimal sample complexity among all local sequentially interactive private mechanisms less,Towards Instance-Optimal Private Query Release,"8 November, 2018"
547,Aleksandar Nikolov,we show that every symmetric normed space admits an efficient nearest neighbor search data structure with doublylogarithmic approximation specifically for every n d no and every ddimensional symmetric norm cdot there exists a data structure for mathrmpolylog log napproximate nearest neighbor search over cdot for npoint datasets achieving no query time and no space the main technical ingredient of the algorithm is a lowdistortion embedding of a symmetric norm into a lowdimensional iterated product of topk norms we also show that our techniques cannot be extended to general norms less,Approximate Near Neighbors for General Symmetric Norms,"24 July, 2017"
548,Aleksandar Nikolov,it is wellknown that for every n geq and d geq there exist point sets x dots xn in d whose discrepancy with respect to the lebesgue measure is of order at most log nd n in a more general setting the first author proved together with josef dick that for any normalized measure on d there exist points x dots xn whose discrepancy with respect to is of order at most log nd n the proof used methods from combinatorial mathematics and in particular a result of banaszczyk on balancings of vectors in the present note we use a version of the socalled transference principle together with recent results on the discrepancy of redblue colorings to show that for any there even exist points having discrepancy of order at most log ndfrac n which is almost as good as the discrepancy bound in the case of the lebesgue measure less,"Tusnády's problem, the transference principle, and non-uniform QMC sampling","17 March, 2017"
549,Aleksandar Nikolov,we study the optimal sample complexity of a given workload of linear queries under the constraints of differential privacy the sample complexity of a query answering mechanism under error parameter is the smallest n such that the mechanism answers the workload with error at most on any database of size n following a line of research started by hardt and talwar stoc we analyze sample complexity using the tools of asymptotic convex geometry we study the sensitivity polytope a natural convex body associated with a query workload that quantifies how query answers can change between neighboring databases this is the information that roughly speaking is protected by a differentially private algorithm and for this reason we expect that a bigger sensitivity polytope implies larger sample complexity our results identify the mean gaussian width as an appropriate measure of the size of the polytope and show sample complexity lower bounds in terms of this quantity our lower bounds completely characterize the workloads for which the gaussian noise mechanism is optimal up to constants as those having asymptotically maximal gaussian width our techniques also yield an alternative proof of pisiers volume number theorem which also suggests an approach to improving the parameters of the theorem less,Lower Bounds for Differential Privacy from Gaussian Width,"8 December, 2016"
550,Aleksandar Nikolov,the maximum volume jsimplex problem asks to compute the jdimensional simplex of maximum volume inside the convex hull of a given set of n points in mathbbqd we give a deterministic approximation algorithm for this problem which achieves an approximation ratio of ej oj the problem is known to be mathrmnphard to approximate within a factor of cj for some constant c our algorithm also gives a factor ej oj approximation for the problem of finding the principal jtimes j submatrix of a rank d positive semidefinite matrix with the largest determinant we achieve our approximation by rounding solutions to a generalization of the doptimal design problem or equivalently the dual of an appropriate smallest enclosing ellipsoid problem our arguments give a short and simple proof of a restricted invertibility principle for determinants less,Randomized Rounding for the Largest Simplex Problem,"14 April, 2015"
551,Aleksandar Nikolov,the norm of a real mtimes n matrix a is the minimum number t such that the column vectors of a are contained in a centered ellipsoid esubseteqmathbbrm which in turn is contained in the hypercube t tm we prove that this classical quantity approximates the emphhereditary discrepancy mathrmherdisc a as follows a olog mcdot mathrmherdisc a and mathrmherdisc a osqrtlog mcdota since is polynomialtime computable this gives a polynomialtime approximation algorithm for hereditary discrepancy both inequalities are shown to be asymptotically tight we then demonstrate on several examples the power of the norm as a tool for proving lower and upper bounds in discrepancy theory most notably we prove a new lower bound of logd n for the emphddimensional tusndy problem asking for the combinatorial discrepancy of an npoint set in mathbbrd with respect to axisparallel boxes for d this improves the previous best lower bound which was of order approximately logdn and it comes close to the best known upper bound of ologdn for which we also obtain a new very simple proof less,Factorization Norms and Hereditary Discrepancy,"8 April, 2015"
552,Aleksandar Nikolov,we give algorithms for geometric graph problems in the modern parallel models inspired by mapreduce for example for the minimum spanning tree mst problem over a set of points in the twodimensional space our algorithm computes a approximate mst our algorithms work in a constant number of rounds of communication while using total space and communication proportional to the size of the data linear space and near linear time algorithms in contrast for general graphs achieving the same result for mst or even connectivity remains a challenging open problem despite drawing significant attention in recent years we develop a general algorithmic framework that besides mst also applies to earthmover distance emd and the transportation cost problem our algorithmic framework has implications beyond the mapreduce model for example it yields a new algorithm for computing emd cost in the plane in nearlinear time no we note that while recently sharathkumar and agarwal developed a nearlinear time algorithm for approximating emd our algorithm is fundamentally different and for example also solves the transportation cost problem raised as an open question in their work furthermore our algorithm immediately gives a approximation algorithm with n space in the streamingwithsorting model with o passes as such it is tempting to conjecture that the parallel models may also constitute a concrete playground in the quest for efficient algorithms for emd and other similar problems in the vanilla streaming model a wellknown open problem less,Parallel Algorithms for Geometric Graph Problems,"4 January, 2014"
553,Aleksandar Nikolov,the komlos conjecture in discrepancy theory states that for some constant k and for any m by n matrix a whose columns lie in the unit ball there exists a vector x such that the infinity norm of ax is bounded above by k this conjecture also implies the beckfiala conjecture on the discrepancy of bounded degree hypergraphs here we prove a natural relaxation of the komlos conjecture if the columns of a are assigned unit real vectors rather than then the komlos conjecture holds with k our result rules out the possibility of a counterexample to the conjecture based on semidefinite programming it also opens the way to proving tighter efficient polynomialtime computable upper bounds for the conjecture using semidefinite programming techniques less,The Komlos Conjecture Holds for Vector Colorings,"1 August, 2013"
554,Aleksandar Nikolov,consider updates arriving online in which the tth input is itdt where its are thought of as ids of users informally a randomized function f is em differentially private with respect to the ids if the probability distribution induced by f is not much different from that induced by it on an input in which occurrences of an id j are replaced with some other id k recently this notion was extended to em panprivacy where the computation of f retains differential privacy even if the internal memory of the algorithm is exposed to the adversary say by a malicious breakin or by fiat by the government this is a strong notion of privacy and surprisingly for basic counting tasks such as distinct counts heavy hitters and others dwork et alcitedworkpan present panprivate algorithms with reasonable accuracy the panprivate algorithms are nontrivial and rely on sampling we reexamine these basic counting tasks and show improved bounds in particular we estimate the distinct count dt to within pm epsdt pm opolylog m where m is the number of elements in the universe this uses suitably noisy statistics on sketches known in the streaming literature we also present the first known lower bounds for panprivacy with respect to a single intrusion our lower bounds show that even if allowed to work with unbounded memory panprivate algorithms for distinct counts can not be significantly more accurate than our algorithms our lower bound uses noisy decoding for heavy hitter counts we present a pan private streaming algorithm that is accurate to within ok in worst case previously known bound for this problem is arbitrarily worse an interesting aspect of our panprivate algorithms is that they deliberately use very small polylogarithmic space and tend to be streaming algorithms even though using more space is not forbidden less,Pan-private Algorithms: When Memory Does Not Help,"8 September, 2010"
555,Gennady Pekhimenko,training deep neural networks on large datasets can often be accelerated by using multiple compute nodes this approach known as distributed training can utilize hundreds of computers via specialized messagepassing protocols such as ring allreduce however running these protocols at scale requires reliable highspeed networking that is only available in dedicated clusters in contrast many realworld applications such as federated learning and cloudbased distributed training operate on unreliable devices with unstable network bandwidth as a result these applications are restricted to using parameter servers or gossipbased averaging protocols in this work we lift that restriction by proposing moshpit allreduce an iterative averaging protocol that exponentially converges to the global average we demonstrate the efficiency of our protocol for distributed optimization with strong theoretical guarantees the experiments show x speedup for resnet training on imagenet compared to competitive gossipbased strategies and x speedup when training albertlarge from scratch using preemptible compute nodes less,Moshpit SGD: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices,"11 January, 2022"
556,Gennady Pekhimenko,modern deep learning applications require increasingly more compute to train stateoftheart models to address this demand large corporations and institutions use dedicated highperformance computing clusters whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations as a result some research directions become the exclusive domain of a few large industrial and even fewer academic actors to alleviate this disparity smaller groups may pool their computational resources and run collaborative experiments that benefit all participants this paradigm known as grid or volunteer computing has seen successful applications in numerous scientific areas however using this approach for machine learning is difficult due to high latency asymmetric bandwidth and several challenges unique to volunteer computing in this work we carefully analyze these constraints and propose a novel algorithmic framework designed specifically for collaborative training we demonstrate the effectiveness of our approach for swav and albert pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost finally we provide a detailed report of successful collaborative language model pretraining with participants less,Distributed Deep Learning in Open Collaborations,"8 November, 2021"
557,Gennady Pekhimenko,driven by the tremendous effort in researching novel deep learning dl algorithms the training cost of developing new models increases staggeringly in recent years we analyze gpu cluster usage statistics from a top research institute for more insights into the hardware efficiency achieved by typical dl training jobs our study reveals that singleaccelerator training jobs can dominate the clusterwide resource consumption when launched repetitively eg for hyperparameter tuning while severely underutilizing the hardware fortunately we observe that such workloads have the following unique characteristics i the models among jobs often have the same types of operators with the same shapes and ii the intermodel horizontal fusion of such operators is mathematically equivalent to other already welloptimized operators thus to help dl researchers and practitioners effectively improve the hardware utilization of their novel dl training workloads we propose horizontally fused training array hfta hfta is a new dl framework extension library that horizontally fuses the models from different repetitive jobs deeply down to operators and then trains them simultaneously on a shared accelerator to show the generality of our solution we apply hfta to six dl models training on stateoftheart accelerators gpus and tpus our results indicate that hfta is highly effective in improving hardware utilization and achieves up to times higher training throughput vs the standard practice of running each job on a separate accelerator less,Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models,"29 March, 2021"
558,Gennady Pekhimenko,deep reinforcement learning rl has made groundbreaking advancements in robotics data center management and other applications unfortunately systemlevel bottlenecks in rl workloads are poorly understood we observe fundamental structural differences in rl workloads that make them inherently less gpubound than supervised learning sl to explain where training time is spent in rl workloads we propose rlscope a crossstack profiler that scopes lowlevel cpugpu resource usage to highlevel algorithmic operations and provides accurate insights by correcting for profiling overhead using rlscope we survey rl workloads across its major dimensions including ml backend rl algorithm and simulator for ml backends we explain a times difference in runtime between equivalent pytorch and tensorflow algorithm implementations and identify a bottleneck rooted in overly abstracted algorithm implementations for rl algorithms and simulators we show that onpolicy algorithms are at least times more simulationbound than offpolicy algorithms finally we profile a scaleup workload and demonstrate that gpu utilization metrics reported by commonly used tools dramatically inflate gpu usage whereas rlscope reports true gpubound time rlscope is an opensource tool available at httpsgithubcomuoftecosystemrlscope less,RL-Scope: Cross-Stack Profiling for Deep Reinforcement Learning Workloads,"4 March, 2021"
559,Gennady Pekhimenko,we present fpraker a processing element for composing training accelerators fpraker processes several floatingpoint multiplyaccumulation operations concurrently and accumulates their result into a higher precision accumulator fpraker boosts performance and energy efficiency during training by taking advantage of the values that naturally appear during training specifically it processes the significand of the operands of each multiplyaccumulate as a series of signed powers of two the conversion to this form is done onthefly this exposes ineffectual work that can be skipped values when encoded have few terms and some of them can be discarded as they would fall outside the range of the accumulator given the limited precision of floatingpoint we demonstrate that fpraker can be used to compose an accelerator for training and that it can improve performance and energy efficiency compared to using conventional floatingpoint units under isocompute area constraints we also demonstrate that fpraker delivers additional benefits when training incorporates pruning and quantization finally we show that fpraker naturally amplifies performance with training methods that use a different precision per layer less,FPRaker: A Processing Element For Accelerating Neural Network Training,"15 October, 2020"
560,Gennady Pekhimenko,training a stateoftheart deep neural network dnn is a computationallyexpensive and timeconsuming process which incentivizes deep learning developers to debug their dnns for computational performance however effectively performing this debugging requires intimate knowledge about the underlying software and hardware systemssomething that the typical deep learning developer may not have to help bridge this gap we present skyline a new interactive tool for dnn training that supports ineditor computational performance profiling visualization and debugging skylines key contribution is that it leverages special computational properties of dnn training to provide i interactive performance predictions and visualizations and ii directly manipulatable visualizations that when dragged mutate the batch size in the code as an ineditor tool skyline allows users to leverage these diagnostic features to debug the performance of their dnns during development an exploratory qualitative user study of skyline produced promising results all the participants found skyline to be useful and easy to use less,Skyline: Interactive In-Editor Computational Performance Profiling for Deep Neural Network Training,"20 August, 2020"
561,Gennady Pekhimenko,we present automatic horizontal fusion a novel optimization technique that complements the standard kernel fusion techniques for gpu programs unlike the standard fusion whose goal is to eliminate intermediate data round trips our horizontal fusion technique aims to increase the threadlevel parallelism to hide instruction latencies we also present hfuse a new source to source cuda compiler that implements automatic horizontal fusion our experimental results show that horizontal fusion can speed up the running time by our results reveal that the horizontal fusion is especially beneficial for fusing kernels with instructions that require different kinds of gpu resources eg a memoryintensive kernel and a computeintensive kernel less,Automatic Horizontal Fusion for GPU Kernels,"2 July, 2020"
562,Gennady Pekhimenko,machinelearning ml hardware and software system demand is burgeoning driven by ml applications the number of different ml inference systems has exploded over organizations are building ml inference chips and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance they range from embedded devices to datacenter solutions fueling the hardware are a dozen or more software frameworks and libraries the myriad combinations of ml hardware and ml software make assessing mlsystem performance in an architectureneutral representative and reproducible manner challenging there is a clear need for industrywide standard ml benchmarking and evaluation criteria mlperf inference answers that call in this paper we present our benchmarking method for evaluating ml inference systems driven by more than organizations as well as more than ml engineers and practitioners mlperf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures the first call for submissions garnered more than reproducible inferenceperformance measurements from organizations representing over systems that showcase a wide range of capabilities the submissions attest to the benchmarks flexibility and adaptability less,MLPerf Inference Benchmark,"9 May, 2020"
563,Gennady Pekhimenko,machine learning ml needs industrystandard performance benchmarks to support design and competitive evaluation of the many emerging software and hardware solutions for ml but ml training presents three unique benchmarking challenges absent from other domains optimizations that improve training throughput can increase the time to solution training is stochastic and time to solution exhibits high variance and software and hardware systems are so diverse that fair benchmarking with the same binary code and even hyperparameters is difficult we therefore present mlperf an ml benchmark that overcomes these challenges our analysis quantitatively evaluates mlperfs efficacy at driving performance and scalability improvements across two rounds of results from multiple vendors less,MLPerf Training Benchmark,"2 March, 2020"
564,Gennady Pekhimenko,the longshorttermmemory recurrent neural networks lstm rnns are a popular class of machine learning models for analyzing sequential data their training on modern gpus however is limited by the gpu memory capacity our profiling results of the lstm rnnbased neural machine translation nmt model reveal that feature maps of the attention and rnn layers form the memory bottleneck and runtime is unevenly distributed across different layers when training on gpus based on these two observations we propose to recompute the feature maps rather than stashing them persistently in the gpu memory while the idea of feature map recomputation has been considered before existing solutions fail to deliver satisfactory footprint reduction as they do not address two key challenges for each feature map recomputation to be effective and efficient its effect on the total memory footprint and the total execution time has to be carefully estimated to this end we propose echo a new compilerbased optimization scheme that addresses the first challenge with a practical mechanism that estimates the memory benefits of recomputation over the entire computation graph and the second challenge by nonconservatively estimating the recomputation overhead leveraging layer specifics echo reduces the gpu memory footprint automatically and transparently without any changes required to the training source code and is effective for models beyond lstm rnns we evaluate echo on numerous stateoftheart machine learning workloads on real systems with modern gpus and observe footprint reduction ratios of x on average and x maximum such reduction can be converted into faster training with a larger batch size savings in gpu energy consumption eg training with one gpu as fast as with four andor an increase in the maximum number of layers under the same gpu memory budget less,Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training,"28 November, 2019"
565,Gennady Pekhimenko,stream analytics have an insatiable demand for memory and performance emerging hybrid memories combine commodity ddr dram with dstacked high bandwidth memory hbm dram to meet such demands however achieving this promise is challenging because hbm is capacitylimited and hbm boosts performance best for sequential access and high parallelism workloads at first glance stream analytics appear a particularly poor match for hbm because they have high capacity demands and data grouping operations their most demanding computations use random access this paper presents the design and implementation of streamboxhbm a stream analytics engine that exploits hybrid memories to achieve scalable high performance streamboxhbm performs data grouping with sequential access sorting algorithms in hbm in contrast to random access hashing algorithms commonly used in dram streamboxhbm solely uses hbm to store key pointer array kpa data structures that contain only partial records keys and pointers to full records for grouping operations it dynamically creates and manages prodigious data and pipeline parallelism choosing when to allocate kpas in hbm it dynamically optimizes for both the high bandwidth and limited capacity of hbm and the limited bandwidth and high capacity of standard dram streamboxhbm achieves million records per second and gbs memory bandwidth while effectively utilizing all cores of intels knights landing a commercial server with hybrid memory it outperforms stream engines with sequential access algorithms without kpas by x and stream engines with random access algorithms by an order of magnitude in throughput to the best of our knowledge streamboxhbm is the first stream engine optimized for hybrid memories less,StreamBox-HBM: Stream Analytics on High Bandwidth Hybrid Memory,"28 January, 2019"
566,Gennady Pekhimenko,this paper summarizes the softmc dram characterization infrastructure which was published in hpca and examines the works significance and future potential softmc soft memory controller is the first publiclyavailable dram testing infrastructure that can flexibly and efficiently test dram chips in a manner accessible to both software and hardware developers softmc is an fpgabased testing platform that can control and test memory modules designed for the commonlyused ddr double data rate interface softmc has two key properties i it provides flexibility to thoroughly control memory behavior or to implement a wide range of mechanisms using ddr commands and ii it is easy to use as it provides a simple and intuitive highlevel programming interface for users completely hiding the lowlevel details of the fpga we demonstrate the capability flexibility and programming ease of softmc with two example use cases first we implement a test that characterizes the retention time of dram cells second we show that the expected latency reduction of two recentlyproposed mechanisms which rely on accessing recentlyrefreshed or recentlyaccessed dram cells faster than other dram cells is not observable in existing dram chips various versions of the softmc platform have enabled many of our other dram characterization studies we discuss several other use cases of softmc including the ability to characterize emerging nonvolatile memory modules that obey the ddr standard we hope that our opensource release of softmc fills a gap in the space of publiclyavailable experimental memory testing infrastructures and inspires new studies ideas and methodologies in memory system design less,SoftMC: Practical DRAM Characterization Using an FPGA-Based Infrastructure,"8 May, 2018"
567,Gennady Pekhimenko,in existing systems to perform any bulk data movement operation copy or initialization the data has to first be read into the onchip processor all the way into the l cache and the result of the operation must be written back to main memory this is despite the fact that these operations do not involve any actual computation rowclone exploits the organization and operation of commodity dram to perform these operations completely inside dram using two mechanisms the first mechanism fast parallel mode copies data between two rows inside the same dram subarray by issuing backtoback activate commands to the source and the destination row the second mechanism pipelined serial mode transfers cache lines between two banks using the shared internal bus rowclone significantly reduces the raw latency and energy consumption of bulk data copy and initialization this reduction directly translates to improvement in performance and energy efficiency of systems running copy or initializationintensive workloads less,RowClone: Accelerating Data Movement and Initialization Using DRAM,"7 May, 2018"
568,Gennady Pekhimenko,the application resource specificationa static specification of several parameters such as the number of threads and the scratchpad memory usage per thread blockforms a critical component of modern gpu programming models this specification determines the parallelism and hence performance of the application during execution because the corresponding onchip hardware resources are allocated and managed based on this specification this tightcoupling between the softwareprovided resource specification and resource management in hardware leads to significant challenges in programming ease portability and performance zorua is a new resource virtualization framework that decouples the programmerspecified resource usage of a gpu application from the actual allocation in the onchip hardware resources zorua enables this decoupling by virtualizing each resource transparently to the programmer we demonstrate that by providing the illusion of more resources than physically available via controlled and coordinated virtualization zorua offers several important benefits i programming ease zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available onchip resources ii portability zorua alleviates the necessity of retuning an applications resource usage when porting the application across gpu generations iii performance by dynamically allocating resources and carefully oversubscribing them when necessary zorua improves or retains the performance of applications that are already highly tuned to best utilize the resources less,"Decoupling GPU Programming Models from Resource Management for Enhanced Programming Ease, Portability, and Performance","2 May, 2018"
569,Gennady Pekhimenko,the application resource specificationa static specification of several parameters such as the number of threads and the scratchpad memory usage per thread blockforms a critical component of the existing gpu programming models this specification determines the performance of the application during execution because the corresponding onchip hardware resources are allocated and managed purely based on this specification this tight coupling between the softwareprovided resource specification and resource management in hardware leads to significant challenges in programming ease portability and performance as we demonstrate in this work our goal in this work is to reduce the dependence of performance on the softwareprovided resource specification to simultaneously alleviate the above challenges to this end we introduce zorua a new resource virtualization framework that decouples the programmerspecified resource usage of a gpu application from the actual allocation in the onchip hardware resources zorua enables this decoupling by virtualizing each resource transparently to the programmer we demonstrate that by providing the illusion of more resources than physically available zorua offers several important benefits i programming ease zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available onchip resources ii portability zorua alleviates the necessity of retuning an applications resource usage when porting the application across gpu generations iii performance by dynamically allocating resources and carefully oversubscribing them when necessary zorua improves or retains the performance of applications that are already highly tuned to best utilize the resources the holistic virtualization provided by zorua has many other potential uses which we describe in this paper less,"Zorua: Enhancing Programming Ease, Portability, and Performance in GPUs by Decoupling Programming Models from Resource Management","7 February, 2018"
570,Gennady Pekhimenko,in this thesis we describe a new practical approach to integrating hardwarebased data compression within the memory hierarchy including onchip caches main memory and both onchip and offchip interconnects this new approach is fast simple and effective in saving storage space a key insight in our approach is that access time including decompression latency is critical in modern memory hierarchies by combining inexpensive hardware support with modest os support our holistic approach to compression achieves substantial improvements in performance and energy efficiency across the memory hierarchy using this new approach we make several major contributions in this thesis first we propose a new compression algorithm basedeltaimmediate compression bdi that achieves high compression ratio with very low compressiondecompression latency bdi exploits the existing low dynamic range of values present in many cache lines to compress them to smaller sizes using basedelta encoding second we observe that the compressed size of a cache block can be indicative of its reuse we use this observation to develop a new cache insertion policy for compressed caches the sizebased insertion policy sip which uses the size of a compressed block as one of the metrics to predict its potential future reuse third we propose a new main memory compression framework linearly compressed pages lcp that significantly reduces the complexity and power cost of supporting main memory compression we demonstrate that any compression algorithm can be adapted to fit the requirements of lcp and that lcp can be efficiently integrated with the existing cache compression designs avoiding extra compressiondecompression less,Practical Data Compression for Modern Memory Hierarchies,"7 September, 2016"
571,Gennady Pekhimenko,modern graphics processing units gpus are well provisioned to support the concurrent execution of thousands of threads unfortunately different bottlenecks during execution and heterogeneous application requirements create imbalances in utilization of resources in the cores for example when a gpu is bottlenecked by the available offchip memory bandwidth its computational resources are often overwhelmingly idle waiting for data from memory to arrive this work describes the coreassisted bottleneck acceleration caba framework that employs idle onchip resources to alleviate different bottlenecks in gpu execution caba provides flexible mechanisms to automatically generate assist warps that execute on gpu cores to perform specific tasks that can improve gpu performance and efficiency caba enables the use of idle computational units and pipelines to alleviate the memory bandwidth bottleneck eg by using assist warps to perform data compression to transfer less data from memory conversely the same framework can be employed to handle cases where the gpu is bottlenecked by the available computational units in which case the memory pipelines are idle and can be used by caba to speed up computation eg by performing memoization using assist warps we provide a comprehensive design and evaluation of caba to perform effective and flexible data compression in the gpu memory hierarchy to alleviate the memory bandwidth bottleneck our extensive evaluations show that caba when used to implement data compression provides an average performance improvement of as high as x across a variety of memorybandwidthsensitive gpgpu applications less,A Framework for Accelerating Bottlenecks in GPU Execution with Assist Warps,"3 February, 2016"
572,Gennady Pekhimenko,limited memory bandwidth is a critical bottleneck in modern systems dstacked dram enables higher bandwidth by leveraging wider throughsiliconvia tsv channels but todays systems cannot fully exploit them due to the limited internal bandwidth of dram dram reads a whole row simultaneously from the cell array to a row buffer but can transfer only a fraction of the data from the row buffer to peripheral io circuit through a limited and expensive set of wires referred to as global bitlines in presence of wider memory channels the major bottleneck becomes the limited data transfer capacity through these global bitlines our goal in this work is to enable higher bandwidth in dstacked dram without the increased cost of adding more global bitlines we instead exploit otherwiseidle resources such as global bitlines already existing within the multiple dram layers by accessing the layers simultaneously our architecture simultaneous multi layer access smla provides higher bandwidth by aggregating the internal bandwidth of multiple layers and transferring the available data at a higher io frequency to implement smla simultaneous data transfer from multiple layers through the same io tsvs requires coordination between layers to avoid channel conflict we first study coordination by static partitioning which we call dedicatedio that assigns groups of tsvs to each layer we then provide a simple yet sophisticated mechanism called cascadedio which enables simultaneous access to each layer by timemultiplexing the ios by operating at a frequency proportional to the number of layers smla provides a higher bandwidth x for a fourlayer stacked dram our evaluations show that smla provides significant performance improvement and energy reduction on average for multiprogrammed workloads respectively over a baseline dstacked dram with very low area overhead less,Simultaneous Multi Layer Access: A High Bandwidth and Low Cost 3D-Stacked Memory Interface,"10 June, 2015"
573,Gerald Penn,"In this paper, we present the first statistical parser for Lambek categorial grammar (LCG), a grammatical formalism for which the graphical proof method known as* proof nets* is applicable. Our parser incorporates proof net structure and constraints into a system based on self-attention networks via novel model elements. Our experiments on an English LCG corpus show that incorporating term graph structure is helpful to the model, improving both parsing accuracy and coverage. Moreover, we derive novel loss functions by expressing proof net constraints as differentiable functions of our model output, enabling us to train our parser without ground-truth derivations.",Proof net structure for neural Lambek categorial parsing,2021/8
574,Gerald Penn,"In the midst of a global pandemic, understanding the public’s opinion of their government’s policy-level, non-pharmaceutical interventions (NPIs) is a crucial component of the health-policy-making process. Prior work on CoViD-19 NPI sentiment analysis by the epidemiological community has proceeded without a method for properly attributing sentiment changes to events, an ability to distinguish the influence of various events across time, a coherent model for predicting the public’s opinion of future events of the same sort, nor even a means of conducting significance tests. We argue here that this urgently needed evaluation method does already exist. In the financial sector, event studies of the fluctuations in a publicly traded company’s stock price are commonplace for determining the effects of earnings announcements, product placements, etc. The same method is suitable for analysing temporal sentiment variation in the light of policy-level NPIs. We provide a case study of Twitter sentiment towards policy-level NPIs in Canada. Our results confirm a generally positive connection between the announcements of NPIs and Twitter sentiment, and we document a promising correlation between the results of this study and a public-health survey of popular compliance with",Statistically Evaluating Social Media Sentiment Trends towards COVID-19 Non-Pharmaceutical Interventions with Event Studies,2021/6
575,Gerald Penn,"In this paper, we define an abstract task called structural realization that generates words given a prefix of words and a partial representation of a parse tree. We also present a method for solving instances of this task using a Gated Graph Neural Network (GGNN). We evaluate it with standard accuracy measures, as well as with respect to perplexity, in which its comparison to previous work on language modelling serves to quantify the information added to a lexical selection task by the presence of syntactic knowledge. That the addition of parse-tree-internal nodes to this neural model should improve the model, with respect both to accuracy and to more conventional measures such as perplexity, may seem unsurprising, but previous attempts have not met with nearly as much success. We have also learned that transverse links through the parse tree compromise the model’s accuracy at generating adjectival and nominal parts of speech.",Structural Realization with GGNNs,2021/6
576,Gerald Penn,"HCI research has for long been dedicated to better and more naturally facilitating information transfer between humans and machines. Unfortunately, humans' most natural form of communication, speech, is also one of the most difficult modalities to be understood by machines–despite, and perhaps, because it is the highest-bandwidth communication channel we possess. While significant research efforts, from engineering, to linguistic, and to cognitive sciences, have been spent on improving machines' ability to understand speech, the CHI community (and the HCI field at large) has only recently started embracing this modality as a central focus of research. This can be attributed in part to the unexpected variations in error rates when processing speech, in contrast with often-unfounded claims of success from industry, but also to the intrinsic difficulty of designing and especially evaluating speech and natural",Conversational Voice User Interfaces: Connecting Engineering Fundamentals to Design Considerations,2021-05-08 00:00:00
577,Gerald Penn,"When working with problems in natural language processing, we can find ourselves in situations where the traditional measurements of descriptive complexity are ineffective at describing the behaviour of our algorithms. It is easy to see why—the models we use are often general frameworks into which difficult-to-define tasks can be embedded. These frameworks can have more power than we typically use, and so complexity measures such as worst-case running time can drastically overestimate the cost of running our algorithms. In particular, they can make an apparently tractable problem seem NP-complete. Using empirical studies to evaluate performance is a necessary but incomplete method of dealing with this mismatch, since these studies no longer act as a guarantee of good performance. In this paper we use statistical measures such as entropy to give an updated analysis of the complexity of the NP-complete Most Probable Sentence problem for pCFGs, which can then be applied to word sense disambiguation and inference tasks. We can bound both the running time and the error in a simple search algorithm, allowing for a much faster search than the NP-completeness of this problem would suggest.",Reanalyzing the Most Probable Sentence Problem: A Case Study in Explicating the Role of Entropy in Algorithmic Complexity,2021/4
578,Gerald Penn,"The growing availability of powerful mobile devices and other edge devices, together with increasing regulatory and security concerns about the exchange of personal information across networks of these devices has challenged the Computational Linguistics community to develop methods that are at once fast, space-efficient, accurate and amenable to secure encoding schemes such as homomorphic encryption. Inspired by recent work that restricts floating point precision to speed up neural network training in hardware-based SIMD, we have developed a method for compressing word vector embeddings into integers using the Chinese Reminder Theorem that speeds up addition by up to 48.27% and at the same time compresses GloVe word embedding libraries by up to 25.86%. We explore the practicality of this simple approach by investigating the trade-off between precision and performance in two NLP tasks: compositional semantic relatedness and opinion target sentiment classification. We find that in both tasks, lowering floating point number precision results in negligible changes to performance.","The Chinese remainder theorem for compact, task-precise, efficient and secure word embeddings",2021/4
579,Gerald Penn,"Ever since Pereira (2000) provided evidence against Chomsky’s (1957) conjecture that statistical language modelling is incommensurable with the aims of grammaticality prediction as a research enterprise, a new area of research has emerged that regards statistical language models as “psycholinguistic subjects” and probes their ability to acquire syntactic knowledge. The advent of The Corpus of Linguistic Acceptability (CoLA)(Warstadt et al., 2019) has earned a spot on the leaderboard for acceptability judgements, and the polemic between Lau et al.(2017) and Sprouse et al.(2018) has raised fundamental questions about the nature of grammaticality and how acceptability judgements should be elicited. All the while, we are told that neural language models continue to improve. That is not an easy claim to test at present, however, because there is almost no agreement on how to measure their improvement when it comes to grammaticality and acceptability judgements. The GLUE leaderboard bundles CoLA together with a Matthews correlation coefficient (MCC), although probably because CoLA’s seminal publication was using it to compute inter-rater reliabilities. Researchers working in this area have used other accuracy and correlation scores, often driven by a need to reconcile and compare various discrete and continuous variables with each other. The score that we will advocate for in this paper, the point biserial correlation, in fact compares a discrete variable (for us, acceptability judgements) to a continuous variable (for us, neural language model probabilities). The only previous work in this area to choose the PBC that ",Grammaticality and language modelling,2020/11
580,Gerald Penn,"In CCG and other highly lexicalized grammars, supertagging a sentence’s words with their lexical categories is a critical step for efficient parsing. Because of the high degree of lexicalization in these grammars, the lexical categories can be very complex. Existing approaches to supervised CCG supertagging treat the categories as atomic units, even when the categories are not simple; when they encounter words with categories unseen during training, their guesses are accordingly unsophisticated. In this paper, we make use of the primitives and operators that constitute the lexical categories of categorial grammars. Instead of opaque labels, we treat lexical categories themselves as linear sequences. We present an LSTM-based model that replaces standard word-level classification with prediction of a sequence of primitives, similarly to LSTM decoders. Our model obtains state-of-the-art word accuracy for single-task English CCG supertagging, increases parser coverage and F1, and is able to produce novel categories. Analysis shows a synergistic effect between this decomposed view and incorporation of prediction history.",Supertagging with CCG primitives,2020/7
581,Gerald Penn,"We frame the problem of de-identifying unstructured text within the greater landscape of privacy-enhancing technologies. We then cover what sort of background knowledge can be gained from only stylistic information about a written document and how we can use research on authorship attribution and author profiling to improve our understanding about the sorts of inferences that can be made from an otherwise de-identified text. Finally, we provide a risk score for determining the likelihood that a message will be attributed to a particular author within a dataset using only author profiling tools.",Reasoning about unstructured data de-identification,2020-06-01 00:00:00
582,Gerald Penn,"We present a new temporal annotation standard, THEE-TimeML, and a corpus TheeBank enabling precise temporal information extraction (TIE) for event-based surveillance (EBS) systems in the public health domain. Current EBS must estimate the occurrence time of each event based on coarse document metadata such as document publication time. Because of the complicated language and narration style of news articles, estimated case outbreak times are often inaccurate or even erroneous. Thus, it is necessary to create annotation standards and corpora to facilitate the development of TIE systems in the public health domain to address this problem. We will discuss the adaptations that have proved necessary for this domain as we present THEE-TimeML and TheeBank. Finally, we document the corpus annotation process, and demonstrate the immediate benefit to public health applications brought by the annotations.",Temporal histories of epidemic events (THEE): a case study in temporal annotation for public health,2020/5
583,Gerald Penn,"We introduce the French Absolute Beginner (FAB) speech corpus. The corpus is intended for the development and study of Computer-Assisted Pronunciation Training (CAPT) tools for absolute beginner learners. Data were recorded during two experiments focusing on using a CAPT system in paired role-play tasks. The setting grants FAB three distinguishing features from other non-native corpora: the experimental setting is ecologically valid, closing the gap between training and deployment; it features a label set based on teacher feedback, allowing for context-sensitive CAPT; and data have been primarily collected from absolute beginners, a group often ignored. Participants did not read prompts, but instead recalled and modified dialogues that were modelled in videos. Unable to distinguish modelled words solely from viewing videos, speakers often uttered unintelligible or out-of-L2 words. The corpus is split into three partitions: one from an experiment with minimal feedback; another with explicit, word-level feedback; and a third with supplementary read-and-record data. A subset of words in the first partition has been labelled as more or less native, with inter-annotator agreement reported. In the explicit feedback partition, labels are derived from the experiment’s online feedback. The FAB corpus is scheduled to be made freely available by the end of 2020.",FAB: The French Absolute Beginner Corpus for Pronunciation Training,2020/5
584,Gerald Penn,"Many AI applications need to process huge amounts of sensitive information for model training, evaluation, and real-world integration. These tasks include facial recognition, speaker recognition, text processing, and genomic data analysis. Unfortunately, one of the following two scenarios occur when training models to perform the aforementioned tasks: either models end up being trained on sensitive user information, making them vulnerable to malicious actors, or their evaluations are not representative of their abilities since the scope of the test set is limited. In some cases, the models never get created in the first place. There are a number of approaches that can be integrated into AI algorithms in order to maintain various levels of privacy. Namely, differential privacy, secure multi-party computation, homomorphic encryption, federated learning, secure enclaves, and automatic data de-identification. We will briefly explain each of these methods and describe the scenarios in which they would be most appropriate. Recently, several of these methods have been applied to machine learning models. We will cover some of the most interesting examples of privacy-preserving ML, including the integration of differential privacy with neural networks to avoid unwanted inferences from being made of a network’s training data. We will also discuss the work we have done on privacy-preserving language modeling and on training neural networks on obfuscated data. ",Perfectly Privacy-Preserving AI What is it and how do we achieve it?,2020
585,Gerald Penn,triangular overlapping melscaled filters fbanks are the current standard input for acoustic models that exploit their inputs timefrequency geometry because they provide a psychoacoustically motivated timefrequency geometry for a speech signal fbank coefficients are provably robust to small deformations in the scale in this paper we explore two ways in which filter banks can be adjusted for the purposes of speech recognition first triangular filters can be replaced with gabor filters a compactly supported filter that better localizes events in time or gammatone filters a psychoacousticallymotivated filter second by rearranging the order of operations in computing filter bank features features can be integrated over smaller time scales while simultaneously providing better frequency resolution we make all feature implementations available online through opensource repositories initial experimentation with a modern endtoend cnn phone recognizer yielded no significant improvements to phone error rate due to either modification the result and its ramifications with respect to learned filter banks is discussed less,Exploring spectro-temporal features in end-to-end convolutional neural networks,"31 December, 2018"
586,Gerald Penn,a perturbative qcd treatment of the pion wave function is applied to computing the scattering amplitude for coherent high relative momentum dijet production from a nucleon less,Perturbative Pion Wave function in Coherent Pion-Nucleon Di-Jet Production,"1 July, 1999"
587,Toniann Pitassi,we develop a new semialgebraic proof system called stabbing planes which formalizes modern branchandcut algorithms for integer programming and is in the style of dpllbased modern sat solvers as with dpll there is only a single rule the current polytope can be subdivided by branching on an inequality and its integer negation that is we can nondeterministically choose a hyperplane ax geq b with integer coefficients which partitions the polytope into three pieces the points in the polytope satisfying ax geq b the points satisfying ax leq b and the middle slab b ax b since the middle slab contains no integer points it can be safely discarded and the algorithm proceeds recursively on the other two branches each path terminates when the current polytope is empty which is polynomialtime checkable among our results we show that stabbing planes can efficiently simulate the cutting planes proof system and is equivalent to a treelike variant of the rcp system of krajicek as well we show that it possesses short proofs of the canonical family of systems of mathbbflinear equations known as the tseitin formulas finally we prove linear lower bounds on the rank of stabbing planes refutations by adapting lower bounds in communication complexity and use these bounds in order to show that stabbing planes proofs cannot be balanced less,Stabbing Planes,"18 May, 2022"
588,Toniann Pitassi,we introduce the notion of a reproducible algorithm in the context of learning a reproducible learning algorithm is resilient to variations in its samples with high probability it returns the exact same output when run on two samples from the same underlying distribution we begin by unpacking the definition clarifying how randomness is instrumental in balancing accuracy and reproducibility we initiate a theory of reproducible algorithms showing how reproducibility implies desirable properties such as data reuse and efficient testability despite the exceedingly strong demand of reproducibility there are efficient reproducible algorithms for several fundamental problems in statistics and learning first we show that any statistical query algorithm can be made reproducible with a modest increase in sample complexity and we use this to construct reproducible algorithms for finding approximate heavyhitters and medians using these ideas we give the first reproducible algorithm for learning halfspaces via a reproducible weak learner and a reproducible boosting algorithm finally we initiate the study of lower bounds and inherent tradeoffs for reproducible algorithms giving nearly tight sample complexity upper and lower bounds for reproducible versus nonreproducible sq algorithms less,Reproducibility in Learning,"20 January, 2022"
589,Toniann Pitassi,when studying the expressive power of neural networks a main challenge is to understand how the size and depth of the network affect its ability to approximate real functions however not all functions are interesting from a practical viewpoint functions of interest usually have a polynomiallybounded lipschitz constant and can be computed efficiently we call functions that satisfy these conditions benign and explore the benefits of size and depth for approximation of benign functions with relu networks as we show this problem is more challenging than the corresponding problem for nonbenign functions we give barriers to showing depthlowerbounds proving existence of a benign function that cannot be approximated by polynomialsize networks of depth would settle longstanding open problems in computational complexity it implies that beyond depth there is a barrier to showing depthseparation for benign functions even between networks of constant depth and networks of nonconstant depth we also study sizeseparation namely whether there are benign functions that can be approximated with networks of size osd but not with networks of size osd we show a complexitytheoretic barrier to proving such results beyond size odlogd but also show an explicit benign function that can be approximated with networks of size od and not with networks of size odlog d for approximation in linfty we achieve such separation already between size od and size od moreover we show superpolynomial size lower bounds and barriers to such lower bounds depending on the assumptions on the function our sizeseparation results rely on an analysis of size lower bounds for boolean functions which is of independent interest we show linear size lower bounds for computing explicit boolean functions with neural networks and threshold circuits less,Size and Depth Separation in Approximating Benign Functions with Neural Networks,"28 June, 2021"
590,Toniann Pitassi,one of the major open problems in complexity theory is proving superlogarithmic lower bounds on the depth of circuits ie mathbfpnotsubseteqmathbfnc karchmer raz and wigderson computational complexity suggested to approach this problem by proving that depth complexity behaves as expected with respect to the composition of functions fdiamond g they showed that the validity of this conjecture would imply that mathbfpnotsubseteqmathbfnc several works have made progress toward resolving this conjecture by proving special cases in particular these works proved the krw conjecture for every outer function f but only for few inner functions g thus it is an important challenge to prove the krw conjecture for a wider range of inner functions in this work we extend significantly the range of inner functions that can be handled first we consider the textitmonotone version of the krw conjecture we prove it for every monotone inner function g whose depth complexity can be lower bounded via a querytocommunication lifting theorem this allows us to handle several new and wellstudied functions such as the stextbftconnectivity clique and generation functions in order to carry this progress back to the textitnonmonotone setting we introduce a new notion of textitsemimonotone composition which combines the nonmonotone complexity of the outer function f with the monotone complexity of the inner function g in this setting we prove the krw conjecture for a similar selection of inner functions g but only for a specific choice of the outer function f less,KRW Composition Theorems via Lifting,"27 January, 2021"
591,Toniann Pitassi,in many application areaslending education and online recommenders for examplefairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and longterm effects for individuals and demographic groups we discuss causal directed acyclic graphs dags as a unifying framework for the recent literature on fairness in such dynamical systems we show that this formulation affords several new directions of inquiry to the modeler where causal assumptions can be expressed and manipulated we emphasize the importance of computing interventional quantities in the dynamical fairness setting and show how causal assumptions enable simulation when environment dynamics are known and offpolicy estimation when dynamics are unknown of intervention on short and longterm outcomes at both the group and individual levels less,Causal Modeling for Fairness in Dynamical Systems,"6 July, 2020"
592,Toniann Pitassi,we show that cutting planes cp proofs are hard to find given an unsatisfiable formula f it is nphard to find a cp refutation of f in time polynomial in the length of the shortest such refutation and unless gaphittingset admits a nontrivial algorithm one cannot find a treelike cp refutation of f in time polynomial in the length of the shortest such refutation the first result extends the recent breakthrough of atserias and mller focs that established an analogous result for resolution our proofs rely on two new lifting theorems daglike lifting for gadgets with many output bits treelike lifting that simulates an rround protocol with gadgets of query complexity olog r independent of input length less,Automating Cutting Planes is NP-Hard},"16 April, 2020"
593,Toniann Pitassi,we consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes taking inspiration from the disentangled representation learning literature we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction but are also emphflexibly fair meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions we show empirically that the resulting encoderwhich does not require the sensitive attributes for inferenceenables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions less,Flexibly Fair Representation Learning by Disentanglement,"6 June, 2019"
594,Toniann Pitassi,in many machine learning applications there are multiple decisionmakers involved both automated and human the interaction between these agents often goes unaddressed in algorithmic development in this work we explore a simple version of this interaction with a twostage framework containing an automated model and an external decisionmaker the model can choose to say pass and pass the decision downstream as explored in rejection learning we extend this concept by proposing learning to defer which generalizes rejection learning by considering the effect of other agents in the decisionmaking process we propose a learning algorithm which accounts for potential biases held by external decisionmakers in a system experiments demonstrate that learning to defer can make systems not only more accurate but also less biased even when working with inconsistent or biased users we show that deferring models still greatly improve the accuracy andor fairness of the entire system less,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"6 September, 2018"
595,Toniann Pitassi,the random ksat model is the most important and wellstudied distribution over ksat instances it is closely connected to statistical physics it is used as a testbench for satisfiability algorithms and averagecase hardness over this distribution has also been linked to hardness of approximation via feiges hypothesis we prove that any cutting planes refutation for random ksat requires exponential size for k that is logarithmic in the number of variables in the interesting regime where the number of clauses guarantees that the formula is unsatisfiable with high probability less,Random CNFs are Hard for Cutting Planes,"7 March, 2017"
596,Toniann Pitassi,a great deal of effort has been devoted to reducing the risk of spurious scientific discoveries from the use of sophisticated validation techniques to deep statistical methods for controlling the false discovery rate in multiple hypothesis testing however there is a fundamental disconnect between the theoretical results and the practice of data analysis the theory of statistical inference assumes a fixed collection of hypotheses to be tested or learning algorithms to be applied selected nonadaptively before the data are gathered whereas in practice data is shared and reused with hypotheses and new analyses being generated on the basis of data exploration and the outcomes of previous analyses in this work we initiate a principled study of how to guarantee the validity of statistical inference in adaptive data analysis as an instance of this problem we propose and investigate the question of estimating the expectations of m adaptively chosen functions on an unknown distribution given n random samples we show that surprisingly there is a way to estimate an exponential in n number of expectations accurately even if the functions are chosen adaptively this gives an exponential improvement over standard empirical estimators that are limited to a linear number of estimates our result follows from a general technique that counterintuitively involves actively perturbing and coordinating the estimates using techniques developed for privacy preservation we give additional applications of this technique to our question less,Preserving Statistical Validity in Adaptive Data Analysis,"2 March, 2016"
597,Toniann Pitassi,we introduce a new algebraic proof system which has tight connections to algebraic circuit complexity in particular we show that any superpolynomial lower bound on any boolean tautology in our proof system implies that the permanent does not have polynomialsize algebraic circuits vnp is not equal to vp as a corollary to the proof we also show that superpolynomial lower bounds on the number of lines in polynomial calculus proofs as opposed to the usual measure of number of monomials imply the permanent versus determinant conjecture note that prior to our work there was no proof system for which lower bounds on an arbitrary tautology implied any computational lower bound our proof system helps clarify the relationships between previous algebraic proof systems and begins to shed light on why proof complexity lower bounds for various proof systems have been so much harder than lower bounds on the corresponding circuit classes in doing so we highlight the importance of polynomial identity testing pit for understanding proof complexity more specifically we introduce certain propositional axioms satisfied by any boolean circuit computing pit we use these pit axioms to shed light on acpfrege lower bounds which have been open for nearly years with no satisfactory explanation as to their apparent difficulty we show that either a proving superpolynomial lower bounds on acpfrege implies vnp does not have polynomialsize circuits of depth d a notoriously open question for d at least thus explaining the difficulty of lower bounds on acpfrege or b acpfrege cannot efficiently prove the depth d pit axioms and hence we have a lower bound on acpfrege using the algebraic structure of our proof system we propose a novel way to extend techniques from algebraic circuit complexity to prove lower bounds in proof complexity less,"Circuit complexity, proof complexity, and polynomial identity testing","15 April, 2014"
598,Toniann Pitassi,fractional pebbling is a generalization of blackwhite pebbling introduced recently in this reasearch paper we solve an open problem by proving a tight lower bound on the pebble weight required to fractionally pebble a balanced dary tree of height h this bound has close ties with branching programs and the separation of p from nl less,Fractional Pebbling Game Lower Bounds,"28 May, 2013"
599,Toniann Pitassi,backtracking search is a powerful algorithmic paradigm that can be used to solve many problems it is in a certain sense the dual of variable elimination but on many problems eg sat it is vastly superior to variable elimination in practice motivated by this we investigate the application of backtracking search to the problem of bayesian inference bayes we show that natural generalizations of known techniques allow backtracking search to achieve performance guarantees similar to standard algorithms for bayes and that there exist problems on which backtracking can in fact do much better we also demonstrate that these ideas can be applied to implement a bayesian inference engine whose performance is competitive with standard algorithms since backtracking search can very naturally take advantage of context specific structure the potential exists for performance superior to standard algorithms on many problems less,Value Elimination: Bayesian Inference via Backtracking Search,"19 October, 2012"
600,Toniann Pitassi,we study the approximability of a number of graph problems treewidth and pathwidth of graphs oneshot black and blackwhite pebbling costs of directed acyclic graphs and a variety of different graph layout problems such as minimum cut linear arrangement and interval graph completion we show that assuming the recently introduced small set expansion conjecture all of these problems are hard to approximate within any constant factor less,"Inapproximability of Treewidth, One-Shot Pebbling, and Related Layout Problems","22 September, 2011"
601,Toniann Pitassi,we provide a nonexplicit separation of the numberonforehead communication complexity classes rp and np when the number of players is up to logn for any recent lower bounds on setdisjointness lsca provide an explicit separation between these classes when the number of players is only up to ologlogn less,Separating NOF communication complexity classes RP and NP,"26 February, 2008"
602,Frank Rudzicz,in this work we evaluate various existing dialogue relevance metrics find strong dependency on the dataset often with poor correlation with human scores of relevance and propose modifications to reduce data requirements and domain sensitivity while improving correlation our proposed metric achieves stateoftheart performance on the humod dataset while reducing measured sensitivity to dataset by we achieve this without finetuning a pretrained language model and using only unannotated human dialogues and a single negative example despite these limitations we demonstrate competitive performance on four datasets from different domains our code including our metric and experiments is open sourced less,"Relevance in Dialogue: Is Less More? An Empirical Comparison of Existing Metrics, and a Novel Simple Metric","3 June, 2022"
603,Frank Rudzicz,existing studies have investigated the tendency of autoregressive language models to generate contexts that exhibit undesired biases and toxicity various debiasing approaches have been proposed which are primarily categorized into databased and decodingbased in our study we investigate the ensemble of the two debiasing paradigms proposing to use toxic corpus as an additional resource to reduce the toxicity our result shows that toxic corpus can indeed help to reduce the toxicity of the language generation process substantially complementing the existing debiasing methods less,Detoxifying Language Models with a Toxic Corpus,"30 April, 2022"
604,Frank Rudzicz,we introduce doctor xavier a bertbased diagnostic system that extracts relevant clinical data from transcribed patientdoctor dialogues and explains predictions using feature attribution methods we present a novel performance plot and evaluation metric for feature attribution methods feature attribution dropping fad curve and its normalized area under the curve nauc fad curve analysis shows that integrated gradients outperforms shapley values in explaining diagnosis classification doctor xavier outperforms the baseline with fscore in named entity recognition and symptom pertinence classification and fscore in diagnosis classification less,Doctor XAvIer: Explainable Diagnosis on Physician-Patient Dialogues and XAI Evaluation,"25 April, 2022"
605,Frank Rudzicz,as large and powerful neural language models are developed researchers have been increasingly interested in developing diagnostic tools to probe them there are many papers with conclusions of the form observation x is found in model y using their own datasets with varying sizes larger probing datasets bring more reliability but are also expensive to collect there is yet to be a quantitative method for estimating reasonable probing dataset sizes we tackle this omission in the context of comparing two probing configurations after we have collected a small dataset from a pilot study how many additional data samples are sufficient to distinguish two different configurations we present a novel method to estimate the required number of data samples in such experiments and across several case studies we verify that our estimations have sufficient statistical power our framework helps to systematically construct probing datasets to diagnose neural nlp models less,On the data requirements of probing,"25 February, 2022"
606,Frank Rudzicz,we scale perceived distances of the coreset algorithm by a factor of uncertainty and search for lowconfidence configurations finding significant improvements in sample efficiency across cifar and svhn image classification especially in larger acquisition sizes we show the necessity of our modifications and explain how the improvement is due to a probabilistic quadratic speedup in the convergence of coreset loss under assumptions about the relationship of model uncertainty and misclassification less,Improving greedy core-set configurations for active learning with uncertainty-scaled distances,"8 February, 2022"
607,Frank Rudzicz,we consider language modelling lm as a multilabel structured prediction task by reframing training from solely predicting a single groundtruth word to ranking a set of words which could continue a given context to avoid annotating topk ranks we generate them using pretrained lms gpt bert and bornagain models this leads to a rankbased form of knowledge distillation kd we also develop a method using ngrams to create a nonprobabilistic teacher which generates the ranks without the need of a pretrained lm we confirm the hypotheses that we can treat lming as a ranking task and that we can do so without the use of a pretrained lm we show that rankbased kd generally improves perplexity ppl often with statistical significance when compared to kullbackleiblerbased kd surprisingly given the simplicity of the method ngrams act as competitive teachers and achieve similar performance as using either bert or a bornagain model teachers gpt always acts as the best teacher though and using it and a transformerxl student on wiki rankbased kd reduces a crossentropy baseline from to and against a klbased kd of less,Language Modelling via Learning to Rank,"10 December, 2021"
608,Frank Rudzicz,morality plays an important role in social wellbeing but peoples moral perception is not stable and changes over time recent advances in natural language processing have shown that text is an effective medium for informing moral change but no attempt has been made to quantify the origins of these changes we present a novel unsupervised framework for tracing textual sources of moral change toward entities through time we characterize moral change with probabilistic topical distributions and infer the source text that exerts prominent influence on the moral time course we evaluate our framework on a diverse set of data ranging from social media to news articles we show that our framework not only captures finegrained human moral judgments but also identifies coherent source topics of moral change triggered by historical events we apply our methodology to analyze the news in the covid pandemic and demonstrate its utility in identifying sources of moral change in highimpact and realtime social events less,An unsupervised framework for tracing textual sources of moral change,"1 September, 2021"
609,Frank Rudzicz,clinical machine learning is increasingly multimodal collected in both structured tabular formats and unstructured forms such as freetext we propose a novel task of exploring fairness on a multimodal clinical dataset adopting equalized odds for the downstream medical prediction tasks to this end we investigate a modalityagnostic fairness algorithm equalized odds post processing and compare it to a textspecific fairness algorithm debiased clinical word embeddings despite the fact that debiased word embeddings do not explicitly address equalized odds of protected groups we show that a textspecific approach to fairness may simultaneously achieve a good balance of performance and classical notions of fairness we hope that our paper inspires future contributions at the critical intersection of clinical nlp and fairness the full source code is available here httpsgithubcomjohntigermultimodalfairness less,Exploring Text Specific and Blackbox Fairness Algorithms in Multimodal Clinical NLP,"10 June, 2021"
610,Frank Rudzicz,eye movement data during reading is a useful source of information for understanding language comprehension processes in this paper we describe our submission to the cmcl shared task on predicting human reading patterns our model uses roberta with a regression layer to predict eyetracking features we train the model in two stages we first finetune on the provo corpus another eyetracking dataset then finetune on the task data we compare different transformer models and apply ensembling methods to improve the performance our final submission achieves a mae score of ranking rd place out of teams that participated in this shared task less,TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction,"15 April, 2021"
611,Frank Rudzicz,many healthcare decisions involve navigating through a multitude of treatment options in a sequential and iterative manner to find an optimal treatment pathway with the goal of an optimal patient outcome such optimization problems may be amenable to reinforcement learning a reinforcement learning agent could be trained to provide treatment recommendations for physicians acting as a decision support tool however a number of difficulties arise when using rl beyond benchmark environments such as specifying the reward function choosing an appropriate state representation and evaluating the learned policy less,Challenges for Reinforcement Learning in Healthcare,"9 March, 2021"
612,Frank Rudzicz,deep neural networks dnns used for braincomputerinterface bci classification are commonly expected to learn general features when trained across a variety of contexts such that these features could be finetuned to specific contexts while some success is found in such an approach we suggest that this interpretation is limited and an alternative would better leverage the newly publicly available massive eeg datasets we consider how to adapt techniques and architectures used for language modelling lm that appear capable of ingesting awesome amounts of data towards the development of encephalography modelling em with dnns in the same vein we specifically adapt an approach effectively used for automatic speech recognition which similarly to lms uses a selfsupervised training objective to learn compressed representations of raw data signals after adaptation to eeg we find that a single pretrained model is capable of modelling completely novel raw eeg sequences recorded with differing hardware and different subjects performing different tasks furthermore both the internal representations of this model and the entire architecture can be finetuned to a variety of downstream bci and eeg classification tasks outperforming prior work in more taskspecific sleep stage classification selfsupervision less,BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data,"28 January, 2021"
613,Frank Rudzicz,recently neural language models lms have demonstrated impressive abilities in generating highquality discourse while many recent papers have analyzed the syntactic aspects encoded in lms there has been no analysis to date of the intersentential rhetorical knowledge in this paper we propose a method that quantitatively evaluates the rhetorical capacities of neural lms we examine the capacities of neural lms understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from rhetorical structure theory rst our experiments show that bertbased lms outperform other transformer lms revealing the richer discourse knowledge in their intermediate layer representations in addition gpt and xlnet apparently encode less rhetorical knowledge and we suggest an explanation drawing from linguistic philosophy our method shows an avenue towards quantifying the rhetorical capacities of neural lms less,Examining the rhetorical capacities of neural language models,"4 October, 2020"
614,Frank Rudzicz,word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories extensive work in linguistic typology has sought to characterize word class flexibility across languages but quantifying this phenomenon accurately and at scale has been fraught with difficulties we propose a principled methodology to explore regularity in word class flexibility our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes eg nountoverb verbtonoun and we apply this method to languages we find that contextualized embeddings not only capture human judgment of class variation within words in english but also uncover shared tendencies in class flexibility across languages specifically we find greater semantic variation when flexible lemmas are used in their dominant word class supporting the view that word class flexibility is a directional process our work highlights the utility of deep contextualized models in linguistic typology less,Word class flexibility: A deep contextualized approach,"19 September, 2020"
615,Frank Rudzicz,here we discuss the four key principles of biomedical ethics from surgical context we elaborate on the definition of fairness and its implications in ai system design with taxonomy of algorithmic biases in ai we discuss the shifts in ethical paradigms as the degree of autonomy in ai systems continue to evolve we also emphasize the need for continuous revisions of ethics in ai due to evolution and dynamic nature of ai systems and technologies less,Ethics of Artificial Intelligence in Surgery,"28 July, 2020"
616,Frank Rudzicz,the act of explaining across two parties is a feedback loop where one provides information on what needs to be explained and the other provides an explanation relevant to this information we apply a reinforcement learning framework which emulates this format by providing explanations based on the explainees current mental model we conduct novel online human experiments where explanations generated by various explanation methods are selected and presented to participants using policies which observe participants mental models in order to optimize an interpretability proxy our results suggest that mental modelbased policies anchored in our proposed state representation may increase interpretability over multiple sequential explanations when compared to a random selection baseline this work provides insight into how to select explanations which increase relevant information for users and into conducting humangrounded experimentation to understand interpretability less,Sequential Explanations with Mental Model-Based Policies,"17 July, 2020"
617,Frank Rudzicz,tone is a prosodic feature used to distinguish words in many languages some of which are endangered and scarcely documented in this work we use unsupervised representation learning to identify probable clusters of syllables that share the same phonemic tone our method extracts the pitch for each syllable then trains a convolutional autoencoder to learn a low dimensional representation for each contour we then apply the mean shift algorithm to cluster tones in highdensity regions of the latent space furthermore by feeding the centers of each cluster into the decoder we produce a prototypical contour that represents each cluster we apply this method to spoken multisyllable words in mandarin chinese and cantonese and evaluate how closely our clusters match the ground truth tone categories finally we discuss some difficulties with our approach including contextual tone variation and allophony effects less,Representation Learning for Discovering Phonemic Tone Contours,"14 May, 2020"
618,Frank Rudzicz,perez et als study using the apple watch to identify atrial fibrillation af is a watershed moment in largescale machine learning for wearable computing identifying relevant patients will be tremendously important to research in healthcare for a condition like af this could reduce stroke risk by two thirds in the study by perez et al only out of individuals had ground truth data their study excluded participants using the irregular pulse notification this design decision means their study was only able to report positive predictive value ppv and unable to explore sensitivity or specificity in this editorial we explore the difficulty of obtaining ground truth data and its implications for study design less,The Ground Truth Trade-Off in Wearable Sensing Studies,"6 January, 2020"
619,Frank Rudzicz,one of the most prevalent symptoms among the elderly population dementia can be detected by classifiers trained on linguistic features extracted from narrative transcripts however these linguistic features are impacted in a similar but different fashion by the normal aging process aging is therefore a confounding factor whose effects have been hard for machine learning classifiers especially deep neural network based models to ignore we show dnn models are capable of estimating ages based on linguistic features predicting dementia based on this aging bias could lead to potentially nongeneralizable accuracies on clinical datasets if not properly deconfounded in this paper we propose to address this deconfounding problem with fair representation learning we build neural network classifiers that learn lowdimensional representations reflecting the impacts of dementia yet discarding the effects of age to evaluate these classifiers we specify a modelagnostic score eon measuring how classifier results are deconfounded from age our best models compromise accuracy by only and on two clinical datasets compared to dnns and their eo scores are better than statistical residulization and inverse probability weight adjustments less,Deconfounding age effects with fair representation learning when assessing dementia,"7 September, 2019"
620,Frank Rudzicz,machine learning has shown promise for automatic detection of alzheimers disease ad through speech however efforts are hampered by a scarcity of data especially in languages other than english we propose a method to learn a correspondence between independently engineered lexicosyntactic features in two languages using a large parallel corpus of outofdomain movie dialogue data we apply it to dementia detection in mandarin chinese and demonstrate that our method outperforms both unilingual and machine translationbased baselines this appears to be the first study that transfers feature domains in detecting cognitive decline less,Detecting dementia in Mandarin Chinese using transfer learning from a parallel corpus,"1 June, 2019"
621,Frank Rudzicz,we consider a classifier whose test set is exposed to various perturbations that are not present in the training set these test samples still contain enough features to map them to the same class as their unperturbed counterpart current architectures exhibit rapid degradation of accuracy when trained on standard datasets but then used to classify perturbed samples of that data to address this we present a novel architecture named deepconsensus that significantly improves generalization to these testtime perturbations our key insight is that deep neural networks should directly consider summaries of low and high level features when making classifications existing convolutional neural networks can be augmented with deepconsensus leading to improved resistance against large and small perturbations on mnist emnist fashionmnist cifar and svhn datasets less,DeepConsensus: using the consensus of features from multiple layers to attain robust image classification,"2 December, 2018"
622,Frank Rudzicz,speech datasets for identifying alzheimers disease ad are generally restricted to participants performing a single task eg describing an image shown to them as a result models trained on linguistic features derived from such datasets may not be generalizable across tasks building on prior work demonstrating that sametask data of healthy participants helps improve ad detection on a singletask dataset of pathological speech we augment an adspecific dataset consisting of subjects describing a picture with multitask healthy data we demonstrate that normative data from multiple speechbased tasks helps improve ad detection by up to visualization of decision boundaries reveals that models trained on a combination of structured picture descriptions and unstructured conversational speech have the least outoftask error and show the most potential to generalize to multiple tasks we analyze the impact of age of the added samples and if they affect fairness in classification we also provide explanations for a possible inductive bias effect across tasks using modelagnostic feature anchors this work highlights the need for heterogeneous datasets for encoding changes in multiple facets of cognition and for developing a taskindependent ad detection model less,The Effect of Heterogeneous Data for Alzheimer's Disease Detection from Speech,"29 November, 2018"
623,Frank Rudzicz,deep learning has demonstrated abilities to learn complex structures but they can be restricted by available data recently consensus networks cns were proposed to alleviate data sparsity by utilizing features from multiple modalities but they too have been limited by the size of labeled data in this paper we extend cn to transductive consensus networks tcns suitable for semisupervised learning in tcns different modalities of input are compressed into latent representations which we encourage to become indistinguishable during iterative adversarial training to understand tcns two mechanisms consensus and classification we put forward its three variants in ablation studies on these mechanisms to further investigate tcn models we treat the latent representations as probability distributions and measure their similarities as the negative relative jensenshannon divergences we show that a consensus state beneficial for classification desires a stable but imperfect similarity between the representations overall tcns outperform or align with the best benchmark algorithms given to labeled samples on the bank marketing and the dementiabank datasets less,Semi-supervised classification by reaching consensus among modalities,"19 November, 2018"
624,Frank Rudzicz,we replicate a variation of the image captioning architecture by vinyals et al then introduce dropout during inference mode to simulate the effects of neurodegenerative diseases like alzheimers disease ad and wernickes aphasia wa we evaluate the effects of dropout on language production by measuring the kldivergence of word frequency distributions and other linguistic metrics as dropout is added we find that the generated sentences most closely approximate the word frequency distribution of the training corpus when using a moderate dropout of during inference less,Dropout during inference as a model for neurological degeneration in an image captioning network,"10 August, 2018"
625,Frank Rudzicz,compared with automatic speech recognition asr the human auditory system is more adept at handling noiseadverse situations including environmental noise and channel distortion to mimic this adeptness auditory models have been widely incorporated in asr systems to improve their robustness this paper proposes a novel auditory model which incorporates psychoacoustics and otoacoustic emissions oaes into asr in particular we successfully implement the frequencydependent property of psychoacoustic models and effectively improve resulting system performance we also present a novel doubletransform spectrumanalysis technique which can qualitatively predict asr performance for different noise types detailed theoretical analysis is provided to show the effectiveness of the proposed algorithm experiments are carried out on the aurora database and show that the word recognition rate using our proposed feature extraction method is significantly increased over the baseline given models trained with clean speech our proposed method achieves up to word recognition accuracy on noisy data less,An Adaptive Psychoacoustic Model for Automatic Speech Recognition,"14 September, 2016"
626,Sushant Sachdeva,we study a variant of a recently introduced minmax optimization framework where the maxplayer is constrained to update its parameters in a greedy manner until it reaches a firstorder stationary point our equilibrium definition for this framework depends on a proposal distribution which the minplayer uses to choose directions in which to update its parameters we show that given a smooth and bounded nonconvexnonconcave objective function access to any proposal distribution for the minplayers updates and stochastic gradient oracle for the maxplayer our algorithm converges to the aforementioned approximate local equilibrium in a number of iterations that does not depend on the dimension the equilibrium point found by our algorithm depends on the proposal distribution and when applying our algorithm to train gans we choose the proposal distribution to be a distribution of stochastic gradients we empirically evaluate our algorithm on challenging nonconvexnonconcave testfunctions and loss functions arising in gan training our algorithm converges on these test functions and when used to train gans trains stably on synthetic and realworld datasets and avoids mode collapse less,A Convergent and Dimension-Independent Min-Max Optimization Algorithm,"30 June, 2022"
627,Sushant Sachdeva,our understanding of learning inputoutput relationships with neural nets has improved rapidly in recent years but little is known about the convergence of the underlying representations even in the simple case of linear autoencoders laes we show that when trained with proper regularization laes can directly learn the optimal representation ordered axisaligned principal components we analyze two such regularization schemes nonuniform ell regularization and a deterministic variant of nested dropout rippel et al icml though both regularization schemes converge to the optimal representation we show that this convergence is slow due to illconditioning that worsens with increasing latent dimension we show that the inefficiency of learning the optimal representation is not inevitable we present a simple modification to the gradient descent update that greatly speeds up convergence empirically less,"Regularized linear autoencoders recover the principal components, eventually","1 October, 2021"
628,Sushant Sachdeva,we give almostlineartime algorithms for constructing sparsifiers with n polylog n edges that approximately preserve weighted ell ellpp flow or voltage objectives on graphs for flow objectives this is the first sparsifier construction for such mixed objectives beyond unit ellp weights and is based on expander decompositions for voltage objectives we give the first sparsifier construction for these objectives which we build using graph spanners and leverage score sampling together with the iterative refinement framework of adil et al soda and a new multiplicativeweights based constantapproximation algorithm for mixedobjective flows or voltages we show how to find textpolylog n approximations for weighted ellpnorm minimizing flows or voltages in pmo n o time for p which is almostlinear for graphs that are slightly dense m ge n o less,Almost-linear-time Weighted $\ell_p$-norm Solvers in Slightly Dense Graphs via Sparsification,"13 February, 2021"
629,Sushant Sachdeva,linear regression in ellpnorm is a canonical optimization problem that arises in several applications including sparse recovery semisupervised learning and signal processing generic convex optimization algorithms for solving ellpregression are slow in practice iteratively reweighted least squares irls is an easy to implement family of algorithms for solving these problems that has been studied for over years however these algorithms often diverge for p and since the work of osborne it has been an open problem whether there is an irls algorithm that is guaranteed to converge rapidly for p we propose pirls the first irls algorithm that provably converges geometrically for any p in infty our algorithm is simple to implement and is guaranteed to find a varepsilonapproximate solution in op mfracpp log fracmvarepsilon le opsqrtm log fracmvarepsilon iterations our experiments demonstrate that it performs even better than our theoretical bounds beats the standard matlabcvx implementation for solving these problems by x and is the fastest among available implementations in the highaccuracy regime less,"Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression","10 January, 2020"
630,Sushant Sachdeva,we give improved algorithms for the ellpregression problem minx xp such that a xb for all p in cup infty our algorithms obtain a high accuracy solution in tildeopmfracpp p le tildeopmfrac iterations where each iteration requires solving an m times m linear system m being the dimension of the ambient space by maintaining an approximate inverse of the linear systems that we solve in each iteration we give algorithms for solving ellpregression to textpolyn accuracy that run in time tildeopmmax where is the matrix multiplication constant for the current best value of we can thus solve ellp regression as fast as ell regression for all constant p bounded away from our algorithms can be combined with fast graph laplacian linear equation solvers to give minimum ellpnorm flow voltage solutions to textpolyn accuracy on an undirected graph with m edges in tildeopm fracpp p le tildeopmfrac time for sparse graphs and for matrices with similar dimensions our iteration counts and running times improve on the pnorm regression algorithm by bubeckcohenleeli stoc and generalpurpose convex optimization algorithms at the core of our algorithms is an iterative refinement scheme for ellpnorms using the smoothed ellpnorms introduced in the work of bubeck et al given an initial solution we construct a problem that seeks to minimize a quadraticallysmoothed ellp norm over a subspace such that a crude solution to this problem allows us to improve the initial solution by a constant factor leading to algorithms with fast convergence less,Iterative Refinement for $\ell_p$-norm Regression,"20 January, 2019"
631,Sushant Sachdeva,we study whether a depth two neural network can learn another depth two network using gradient descent assuming a linear output node we show that the question of whether gradient descent converges to the target function is equivalent to the following question in electrodynamics given k fixed protons in mathbbrd and k electrons each moving due to the attractive force from the protons and repulsive force from the remaining electrons whether at equilibrium all the electrons will be matched up with the protons up to a permutation under the standard electrical force this follows from the classic earnshaws theorem in our setting the force is determined by the activation function and the input distribution building on this equivalence we prove the existence of an activation function such that gradient descent learns at least one of the hidden nodes in the target network iterating we show that gradient descent can be used to learn the entire network one node at a time less,Convergence Results for Neural Networks via Electrodynamics,"4 December, 2018"
632,Sushant Sachdeva,in the simultaneous maxcut problem we are given k weighted graphs on the same set of n vertices and the goal is to find a cut of the vertex set so that the minimum over the k graphs of the cut value is as large as possible previous work bks gave a polynomial time algorithm which achieved an approximation factor of o for this problem and an approximation factor of k in the unweighted case where k rightarrow as k rightarrow infty in this work we give a polynomial time approximation algorithm for simultaneous maxcut with an approximation factor of for all constant k the natural sdp formulation for simultaneous maxcut was shown to have an integrality gap of k in bks in achieving the better approximation guarantee we use a stronger sumofsquares hierarchy sdp relaxation and a rounding algorithm based on raghavendratan rt in addition to techniques from bks less,Near-optimal approximation algorithm for simultaneous Max-Cut,"13 January, 2018"
633,Sushant Sachdeva,a spectral sparsifier of a graph g is a sparser graph h that approximately preserves the quadratic form of g ie for all vectors x xt lg x approx xt lh x where lg and lh denote the respective graph laplacians spectral sparsifiers generalize cut sparsifiers and have found many applications in designing graph algorithms in recent years there has been interest in computing spectral sparsifiers in semistreaming and dynamic settings natural algorithms in these settings often involve repeated sparsification of a graph and accumulation of errors across these steps we present a framework for analyzing algorithms that perform repeated sparsifications that only incur error corresponding to a single sparsification step leading to better results for many resparsificationbased algorithms as an application we show how to maintain a spectral sparsifier in the semistreaming setting we present a simple algorithm that for a graph g on n vertices and m edges computes a spectral sparsifier of g with on log n edges in a single pass over g using only on log n space and om log n total time this improves on previous best semistreaming algorithms for both spectral and cut sparsifiers by a factor of logn in both space and runtime the algorithm extends to semistreaming row sampling for general psd matrices we also use our framework to combine a spectral sparsification algorithm by koutis with improved spanner constructions to give a parallel algorithm for constructing onlognloglogn sized spectral sparsifiers in omlognloglogn time this is the best known combinatorial graph sparsification algorithmthe size of the sparsifiers is only a factor lognloglogn more than ones produced by numerical routines less,A Framework for Analyzing Resparsification Algorithms,"21 November, 2016"
634,Sushant Sachdeva,we study the mixing time of the dikin walk in a polytope a random walk based on the logbarrier from the interior point method literature this walk and a close variant were studied by narayanan and kannannarayanan bounds on its mixing time are important for algorithms for sampling and optimization over polytopes here we provide a simple proof of their result that this random walk mixes in time omn for an ndimensional polytope described using m inequalities less,The Mixing Time of the Dikin Walk in a Polytope - A Simple Proof,"8 August, 2016"
635,Sushant Sachdeva,we give an arithmetic version of the recent proof of the triangle removal lemma by fox fox for the group mathbbfn a triangle in mathbbfn is a triple xyz such that xyz the triangle removal lemma for mathbbfn states that for every there is a such that if a subset a of mathbbfn requires the removal of at least cdot n elements to make it trianglefree then it must contain at least cdot n triangles this problem was first studied by green gre who proved a lower bound on using an arithmetic regularity lemma regularity based lower bounds for triangle removal in graphs were recently improved by fox and we give a direct proof of an analogous improvement for triangle removal in mathbbfn the improved lower bound was already known to follow for triangleremoval in all groups using foxs removal lemma for directed cycles and a reduction by krl serra and vena ksv see foxcf the purpose of this note is to provide a direct fourieranalytic proof for the group mathbbfn less,An Arithmetic Analogue of Fox's Triangle Removal Argument,"1 February, 2016"
636,Sushant Sachdeva,given a directed acyclic graph g and a set of values y on the vertices the isotonic regression of y is a vector x that respects the partial order described by g and minimizes xy for a specified norm this paper gives improved algorithms for computing the isotonic regression for all weighted ellpnorms with rigorous performance guarantees our algorithms are quite practical and their variants can be implemented to run fast in practice less,"Fast, Provable Algorithms for Isotonic Regression in all $\ell_{p}$-norms","11 November, 2015"
637,Sushant Sachdeva,given k collections of sat clauses on the same set of variables v can we find one assignment that satisfies a large fraction of clauses from each collection we consider such simultaneous constraint satisfaction problems and design the first nontrivial approximation algorithms in this context our main result is that for every csp f for k tildeolog n there is a polynomial time constant factor pareto approximation algorithm for k simultaneous maxfcsp instances our methods are quite general and we also use them to give an improved approximation factor for simultaneous maxwsat for k tildeolog n in contrast for k log n no nonzero approximation factor for k simultaneous maxfcsp instances can be achieved in polynomial time assuming the exponential time hypothesis these problems are a natural meeting point for the theory of constraint satisfaction problems and multiobjective optimization we also suggest a number of interesting directions for future research less,Simultaneous Approximation of Constraint Satisfaction Problems,"29 July, 2014"
638,Sushant Sachdeva,we survey key techniques and results from approximation theory in the context of uniform approximations to real functions such as ex x and xk we then present a selection of results demonstrating how such approximations can be used to speed up primitives crucial for the design of fast algorithms for problems such as simulating random walks graph partitioning solving linear system of equations computing eigenvalues and combinatorial approaches to solve semidefinite programs less,Approximation Theory and the Design of Fast Algorithms,"19 September, 2013"
639,Sushant Sachdeva,suppose we are given an oracle that claims to approximate the permanent for most matrices x where x is chosen from the gaussian ensemble the matrix entries are iid univariate complex gaussians can we test that the oracle satisfies this claim this paper gives a polynomialtime algorithm for the task the oracletesting problem is of interest because a recent paper of aaronson and arkhipov showed that if there is a polynomialtime algorithm for simulating bosonboson interactions in quantum mechanics then an approximation oracle for the permanent of the type described above exists in bppnp since computing the permanent of even matrices is pcomplete this seems to demonstrate more computational power in quantum mechanics than shors factoring algorithm does however unlike factoring which is in np it was unclear previously how to test the correctness of an approximation oracle for the permanent and this is the contribution of the paper the technical difficulty overcome here is that univariate polynomial selfcorrection which underlies similar oracletesting algorithms for permanent over finite fields and whose discovery led to a revolution in complexity theory does not seem to generalize to complex or even real numbers we believe that this tester will motivate further progress on understanding the permanent of gaussian matrices less,Testing Permanent Oracles -- Revisited,"19 July, 2012"
640,Sushant Sachdeva,we give a novel spectral approximation algorithm for the balanced separator problem that given a graph g a constant balance b in and a parameter either finds an bbalanced cut of conductance osqrt in g or outputs a certificate that all bbalanced cuts in g have conductance at least and runs in time tildeom this settles the question of designing asymptotically optimal spectral algorithms for balanced separator our algorithm relies on a variant of the heat kernel random walk and requires as a subroutine an algorithm to compute explv where l is the laplacian of a graph related to g and v is a vector algorithms for computing the matrixexponentialvector product efficiently comprise our next set of results our main result here is a new algorithm which computes a good approximation to expav for a class of psd matrices a and a given vector u in time roughly tildeoma where ma is the number of nonzero entries of a this uses in a nontrivial way the result of spielman and teng on inverting sdd matrices in tildeoma time finally we prove ex can be uniformly approximated up to a small additive error in a nonnegative interval ab with a polynomial of degree roughly sqrtba while this result is of independent interest in approximation theory we show that via the lanczos method from numerical analysis it yields a simple algorithm to compute expav for psd matrices that runs in time roughly ota sqrta where ta is the time required for computation of the vector aw for given vector w as an application we obtain a simple and practical algorithm with output conductance osqrt for balanced separator that runs in time tildeomsqrt this latter algorithm matches the running time but improves on the approximation guarantee of the algorithm by andersen and peres less,"Approximating the Exponential, the Lanczos Method and an \tilde{O}(m)-Time Spectral Algorithm for Balanced Separator","7 November, 2011"
641,Sushant Sachdeva,in a wellknown paperarv arora rao and vazirani obtained an osqrtlog n approximation to the balanced separator problem and uniform sparsest cut at the heart of their result is a geometric statement about sets of points that satisfy triangle inequalities which also underlies subsequent work on approximation algorithms and geometric embeddings in this note we give an equivalent formulation of the structure theorem in arv in terms of the expansion of large sets in geometric graphs on sets of points satisfying triangle inequalities less,A Reformulation of the Arora-Rao-Vazirani Structure Theorem,"7 February, 2011"
642,Bianca Schroeder,datacenter designers rely on conservative estimates of it equipment power draw to provision resources this leaves resources underutilized and requires more datacenters to be built prior work has used power capping to shave the rare power peaks and add more servers to the datacenter thereby oversubscribing its resources and lowering capital costs this works well when the workloads and their server placements are known unfortunately these factors are unknown in public clouds forcing providers to limit the oversubscription so that performance is never impacted in this paper we argue that providers can use predictions of workload performance criticality and virtual machine vm resource utilization to increase oversubscription this poses many challenges such as identifying the performancecritical workloads from blackbox vms creating support for criticalityaware power management and increasing oversubscription while limiting the impact of capping we address these challenges for the hardware and software infrastructures of microsoft azure the results show that we enable a x increase in oversubscription with minimum impact to critical workloads less,Prediction-Based Power Oversubscription in Cloud Platforms,"29 October, 2020"
643,Bianca Schroeder,"High density Solid State Drives, such as QLC drives, offer increased storage capacity, but a magnitude lower Program and Erase (P/E) cycles, limiting their endurance and hence usability. We present the design and implementation of non-binary, Voltage-Based Write-Once-Memory (WOM-v) Codes to improve the lifetime of QLC drives. First, we develop a FEMU based simulator test-bed to evaluate the gains of WOM-v codes on real world workloads. Second, we propose and implement two optimizations, an efficient garbage collection mechanism and an encoding optimization to drastically improve WOM-v code endurance without compromising performance. A careful evaluation, including microbenchmarks and trace-driven evaluation, demonstrates that WOM-v codes can reduce the Erase cycles for QLC drives by 4.4 x-11.1 x for real world workloads with minimal performance overheads resulting in improved QLC SSD lifetime.",Improving the Reliability of Next Generation {SSDs} using {WOM-v} Codes,2022
644,Bianca Schroeder,"As we increasingly rely on SSDs for our storage needs, it is important to understand their operational characteristics in the field, in particular since they vary from HDDs. This includes operational aspects, such as the level of write amplification experienced by SSDs in production systems and how it is affected by various factors; the effectiveness of wear leveling; or the rate at which drives in the field use up their program-erase (PE) cycle limit and what that means for the transition to future generations of flash with lower endurance. This paper presents the first large-scale field study of key operational characteristics of SSDs in production use based on a large population of enterprise storage systems covering almost 2 million SSDs of a major storage vendor (NetApp).",Operational Characteristics of {SSDs} in Enterprise Storage Systems: A {Large-Scale} Field Study,2022
645,Bianca Schroeder,"This article presents the first large-scale field study of NAND-based SSDs in enterprise storage systems (in contrast to drives in distributed data center storage systems). The study is based on a very comprehensive set of field data, covering 1.6 million SSDs of a major storage vendor (NetApp). The drives comprise three different manufacturers, 18 different models, 12 different capacities, and all major flash technologies (SLC, cMLC, eMLC, 3D-TLC). The data allows us to study a large number of factors that were not studied in prior works, including the effect of firmware versions, the reliability of TLC NAND, and the correlations between drives within a RAID system. This article presents our analysis, along with a number of practical implications derived from it.",Reliability of SSDs in Enterprise Storage Systems: A Large-Scale Field Study,2021-01-13 00:00:00
646,Bianca Schroeder,"Storage systems are designed and optimized relying on wisdom derived from analysis studies of file-system and block-level workloads. However, while SSDs are becoming a dominant building block in many storage systems, their design continues to build on knowledge derived from analysis targeted at hard disk optimization. Though still valuable, it does not cover important aspects relevant for SSD performance. In a sense, we are “searching under the streetlight,” possibly missing important opportunities for optimizing storage system design.",SSD-based workload characteristics and their performance implications,2021-01-08 00:00:00
647,Bianca Schroeder,"Prior work has used power capping to shave rare power peaks and add more servers to a datacenter, thereby oversubscribing its resources and lowering capital costs. This works well when the workloads and their server placements are known. Unfortunately, these factors are unknown in public clouds, forcing providers to limit the oversubscription and thus the potential performance loss from power capping. In this paper, we argue that providers can use predictions of workload performance criticality and virtual machine (VM) resource utilization to increase oversubscription. This poses many challenges, such as identifying the performance-critical workloads from opaque VMs, creating support for criticality-aware power management, and increasing oversubscription while limiting the impact of capping. We address these challenges for the hardware and software of Microsoft Azure. The results show that we enable a 2x increase in oversubscription with minimum impact to critical workloads. We describe lessons from deploying our work in production.",{Prediction-Based} Power Oversubscription in Cloud Platforms,2021
648,Bianca Schroeder,"As solid state drives (SSDs) are increasingly replacing hard disk drives, the reliability of storage systems depends on the failure modes of SSDs and the ability of the file system layered on top to handle these failure modes. While the classical paper on IRON File Systems provides a thorough study of the failure policies of three file systems common at the time, we argue that 13 years later it is time to revisit file system reliability with SSDs and their reliability characteristics in mind, based on modern file systems that incorporate journaling, copy-on-write, and log-structured approaches and are optimized for flash. This article presents a detailed study, spanning ext4, Btrfs, and F2FS, and covering a number of different SSD error modes. We develop our own fault injection framework and explore over 1,000 error cases. Our results indicate that 16% of these cases result in a file system that cannot be mounted or even ",The reliability of modern file systems in the face of SSD errors,2020-03-16 00:00:00
649,Bianca Schroeder,"New generations of Solid State Drives (SSDs) offer increased storage density with higher bits per cell, but an order of magnitude lower Program and Erase (P/E) cycles. This decreases the number of times one can rewrite on the SSD, and hence, the overall lifetime of the drive. One way of improving drive lifetime is by applying Write-Once Memory (WOM) codes which can rewrite on top of pre-existing data without erasing previous data. This increases the total logical data that can be written on the physical medium before an erase operation is required. Traditional WOM codes are not scalable and only offer up to 50% increase in total writable logical data between any two erase operations. In this paper we present a novel, simple and highly efficient family of WOM codes. Although our design is generic and could be applied to any N-Level cell drive, we focus on QLC drives to demonstrate and evaluate the proposed scheme and show that it can increase the total logical writable data before an erase in a range of 50-375% the physical medium capacity with varying storage overheads. Next, we argue that it is possible to further increase the total logical writable data between two erase operations by up to 500% with the help of a carefully chosen internal error-correcting code (ECC) already present in SSDs.",Rethinking {WOM} Codes to Enhance the Lifetime in New {SSD} Generations,2020
650,Bianca Schroeder,"This paper presents the first large-scale field study of NAND-based SSDs in enterprise storage systems (in contrast to drives in distributed data center storage systems). The study is based on a very comprehensive set of field data, covering 1.4 million SSDs of a major storage vendor (NetApp). The drives comprise three different manufacturers, 18 different models, 12 different capacities, and all major flash technologies (SLC, cMLC, eMLC, 3D-TLC). The data allows us to study a large number of factors that were not studied in previous works, including the effect of firmware versions, the reliability of TLC NAND, and correlations between drives within a RAID system. This paper presents our analysis, along with a number of practical implications derived from it.",A Study of {SSD} Reliability in Large Scale Enterprise Storage Deployments,2020
651,Bianca Schroeder,"As solid state drives (SSDs) are increasingly replacing hard disk drives, the reliability of storage systems depends on the failure modes of SSDs and the ability of the file system layered on top to handle these failure modes. While the classical paper on IRON File Systems provides a thorough study of the failure policies of three file systems common at the time, we argue that 13 years later it is time to revisit file system reliability with SSDs and their reliability characteristics in mind, based on modern file systems that incorporate journaling, copy-on-write and log-structured approaches, and are optimized for flash. This paper presents a detailed study, spanning ext4, Btrfs and F2FS, and covering a number of different SSD error modes. We develop our own fault injection framework and explore over a thousand error cases. Our results indicate that 16\% of these cases result in a file system that cannot be mounted or even repaired by its system checker. We also identify the key file system metadata structures that can cause such failures and finally, we recommend some design guidelines for file systems that are deployed on top of SSDs.",Evaluating file system reliability on solid state drives,2019
652,Bianca Schroeder,"The recent popularity of deep neural networks (DNNs) has generated considerable research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference - i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for DNN training, called TBD 1 , which comprises a representative set of eight DNN models and covers six major machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single",Benchmarking and analyzing deep neural network training,2018-09-30 00:00:00
653,Bianca Schroeder,"Frameworks for large-scale distributed data processing, such as the Hadoop ecosystem, are at the core of the big data revolution we have experienced over the last decade. In this paper, we conduct an extensive study of the Hadoop Distributed File System (HDFS)'s code evolution. Our study is based on the reports and patch files (patches) available from the official Apache issue tracker (JIRA) and our goal was to make complete use of the entire history of HDFS at the time and the richness of the available data. The purpose of our study is to assist developers in improving the design of similar systems and implementing more solid systems in general. In contrast to prior work, our study covers all reports that have been submitted over HDFS's lifetime, rather than a sampled subset. Additionally, we include all associated patch files that have been verified by the developers of the system and classify the root causes of ",The evolution of the hadoop distributed file system,2018-05-16 00:00:00
654,Bianca Schroeder,"The recent popularity of deep neural networks (DNNs) has generated a lot of research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference -- i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark for DNN training, called TBD (TBD is short for Training Benchmark for DNNs), that uses a representative set of DNN models that cover a wide range of machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) by performing an extensive performance analysis of training these different applications on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). TBD currently covers six major application domains and eight different state-of-the-art models. We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of new and existing metrics and methodologies to analyze the results, and utilization of domain specific characteristics of DNN training. We also build a new set of tools for memory profiling in all three major frameworks; much needed tools that can finally shed some light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN ",Tbd: Benchmarking and analyzing deep neural network training,2018-03-16 00:00:00
655,Bianca Schroeder,"Solid-state drives (SSDs) based on NAND flash are making deep inroads into data centers as well as the consumer market. In 2016, manufacturers shipped more than 130 million units totaling around 50 Exabytes of storage capacity. As the amount of data stored on solid state drives keeps increasing, it is important to understand the reliability characteristics of these devices. For a long time, our knowledge about flash reliability was derived from controlled experiments in lab environments under synthetic workloads, often using methods for accelerated testing. However, within the last two years, three large-scale field studies have been published that report on the failure behavior of flash devices in production environments subjected to real workloads and operating conditions. The goal of this paper is to provide an overview of what we have learned about flash reliability in production, and where appropriate ",Reliability of NAND-based SSDs: What field studies tell us,2017-08-18 00:00:00
656,Bianca Schroeder,"This paper proposes the use of machine learning techniques to make storage systems more reliable in the face of sector errors. Sector errors are partial drive failures, where individual sectors on a drive become unavailable, and occur at a high rate in both hard disk drives and solid state drives. The data in the affected sectors can only be recovered through redundancy in the system (eg another drive in the same RAID) and is lost if the error is encountered while the system operates in degraded mode, eg during RAID reconstruction. In this paper, we explore a range of different machine learning techniques and show that sector errors can be predicted ahead of time with high accuracy. Prediction is robust, even when only little training data or only training data for a different drive model is available. We also discuss a number of possible use cases for improving storage system reliability through the use of sector error predictors. We evaluate one such use case in detail: We show that the mean time to detecting errors (and hence the window of vulnerability to data loss) can be greatly reduced by adapting the speed of a scrubber based on error predictions.",Improving storage system reliability with proactive error prediction,2017-07-12 00:00:00
657,Bianca Schroeder,we measure the energy emitted by extensive air showers in the form of radio emission in the frequency range from to mhz exploiting the accurate energy scale of the pierre auger observatory we obtain a radiation energy of pm stat pm sys mev for cosmic rays with an energy of eev arriving perpendicularly to a geomagnetic field of g scaling quadratically with the cosmicray energy a comparison with predictions from stateoftheart firstprinciple calculations shows agreement with our measurement the radiation energy provides direct access to the calorimetric energy in the electromagnetic cascade of extensive air showers comparison with our result thus allows the direct calibration of any cosmicray radio detector against the wellestablished energy scale of the pierre auger observatory less,Measurement of the Radiation Energy in the Radio Signal of Extensive Air Showers as a Universal Estimator of Cosmic-Ray Energy,"21 June, 2016"
658,Nisarg Shah,"A voting rule decides on a probability distribution over a set of  alternatives, based on rankings of those alternatives provided by agents. We assume that agents have cardinal utility functions over the alternatives, but voting rules have access to only the rankings induced by these utilities. We evaluate how well voting rules do on measures of social welfare and of proportional fairness, computed based on the hidden utility functions. In particular, we study the distortion of voting rules, which is a worst-case measure. It is an approximation ratio comparing the utilitarian social welfare of the optimum outcome to the welfare of the outcome selected by the voting rule, in the worst case over possible input profiles and utility functions that are consistent with the input. The literature has studied distortion with unit-sum utility functions, and left a small asymptotic gap in the best possible distortion. Using tools from the theory of fair multi-winner elections, we propose the first voting rule which achieves the optimal distortion  for unit-sum utilities. ",Optimized Distortion and Proportional Fairness in Voting,2022-05-31 00:00:00
659,Nisarg Shah,"Gerrymandering is the process of creating electoral districts for partisan advantage, allowing a party to win more seats than what is reasonable for their vote. While research on gerrymandering has recently grown, many issues are still not fully understood such as what influences the degree to which a party can gerrymander and what techniques can be used to counter it. One commonly suggested (and, in some US states, mandated) requirement is that districts be “geographically compact”. However, there are many competing compactness definitions and the impact of compactness on the gerrymandering abilities of the parties is not well understood. Also not well understood is how the growing urban-rural divide between supporters of different parties impacts redistricting. We develop a modular, scalable, and efficient algorithm that can design districts for various criteria. ","Little House (Seat) on the Prairie: Compactness, Gerrymandering, and Population Distribution.",2022-05-09 00:00:00
660,Nisarg Shah,"In the classical version of online bipartite matching, there is a given set of offline vertices (aka agents) and another set of vertices (aka items) that arrive online. When each item arrives, its incident edges -- the agents who like the item -- are revealed and the algorithm must irrevocably match the item to such agents. We initiate the study of class fairness in this setting, where agents are partitioned into a set of classes and the matching is required to be fair with respect to the classes. We adopt popular fairness notions from the fair division literature such as envy-freeness (up to one item), proportionality, and maximin share fairness to our setting. Our class versions of these notions demand that all classes, regardless of their sizes, receive a fair treatment. We study deterministic and randomized algorithms for matching indivisible items (leading to integral matchings) and for matching divisible items (leading to fractional matchings). We design and analyze three novel algorithms. ",Class Fairness in Online Matching,2022-03-07 00:00:00
661,Nisarg Shah,"We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, n agents and m alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternative that approximately minimizes the total distance from the agents, and the worst-case approximation ratio is termed distortion. In the case of multiwinner voting, the goal is to select a committee of k alternatives that (approximately) minimizes the total cost to all agents. We consider the scenario where the cost of an agent for a committee is her distance from the q-th closest alternative in the committee. ",The metric distortion of multiwinner voting,2022-01-31 00:00:00
662,Nisarg Shah,"Motivated by fair division applications, we study a fair connected graph partitioning problem, in which an undirected graph with m nodes must be divided between n agents such that each agent receives a connected subgraph and the partition is fair. We study approximate versions of two fairness criteria: α-proportionality requires that each agent receives a subgraph with at least 1/α· m/n nodes, and α-balancedness requires that the ratio between the sizes of the largest and smallest subgraphs be at most α. Unfortunately, there exist simple examples in which no partition is reasonably proportional or balanced. To circumvent this, we introduce the idea of charity. We show that by “donating” just n− 1 nodes, we can guarantee the existence of 2-proportional and almost 2-balanced partitions (and find them in polynomial time), and that this result is almost tight. More generally, we chart the tradeoff between the size of charity and the approximation of proportionality or balancedness we can guarantee.",A Little Charity Guarantees Fair Connected Graph Partitioning,2022
663,Nisarg Shah,"A fundamental question in social choice and multiagent systems is aggregating ordinal preferences expressed by agents into a measurably prudent collective choice. A promising line of recent work views ordinal preferences as a proxy for underlying cardinal preferences. It aims to optimize distortion, the worst-case approximation ratio of the (utilitarian) social welfare. When agents rank the set of alternatives, prior work identifies near-optimal voting rules for selecting one or more alternatives. However, ranking all the alternatives is prohibitive when there are many alternatives. In this work, we consider the setting where each agent ranks only her t favorite alternatives and identify almost tight bounds on the best possible distortion when selecting a single alternative or a committee of alternatives of a given size k. Our results also extend to approximating higher moments of social welfare. Along the way, we close a gap left open in prior work by identifying asymptotically tight distortion bounds for committee selection given full rankings.",Distortion in voting with top-t preferences,2022
664,Nisarg Shah,"We propose a multi-agent variant of the classical multi-armed bandit problem, in which there are  agents and  arms, and pulling an arm generates a (possibly different) stochastic reward for each agent. Unlike the classical multi-armed bandit problem, the goal is not to learn the"" best arm""; indeed, each agent may perceive a different arm to be the best for her personally. Instead, we seek to learn a fair distribution over the arms. Drawing on a long line of research in economics and computer science, we use the Nash social welfare as our notion of fairness. We design multi-agent variants of three classic multi-armed bandit algorithms and show that they achieve sublinear regret, which is now measured in terms of the lost Nash social welfare. We also extend a classical lower bound, establishing the optimality of one of our algorithms.",Fair algorithms for multi-agent multi-armed bandits,2021-12-06 00:00:00
665,Nisarg Shah,"A major open question in fair allocation of indivisible items is whether there always exists an allocation of chores that is Pareto optimal (PO) and envy-free up to one item (EF1). We answer this question affirmatively for the natural class of bivalued utilities, where each agent partitions the chores into easy and difficult ones, and has cost  for chores that are difficult for her and cost  for chores that are easy for her. Such an allocation can be found in polynomial time using an algorithm based on the Fisher market. We also show that for a slightly broader class of utilities, where each agent  can have a potentially different integer , an allocation that is maximin share fair (MMS) always exists and one that is both PO and MMS can be computed in polynomial time, provided that each  is an integer. Our MMS arguments also hold when allocating goods instead of chores, and extend to another natural class of utilities, namely weakly lexicographic utilities.",How to fairly allocate easy and difficult chores,2021-10-21 00:00:00
666,Nisarg Shah,"We introduce a new model for two-sided matching which allows us to borrow popular fairness notions from the fair division literature such as envy-freeness up to one good and maximin share guarantee. In our model, each agent is matched to multiple agents on the other side over whom she has additive preferences. We demand fairness for each side separately, giving rise to notions such as double envy-freeness up to one match (DEF1) and double maximin share guarantee (DMMS). We show that (a slight strengthening of) DEF1 cannot always be achieved, but in the special case where both sides have identical preferences, the round-robin algorithm with a carefully designed agent ordering achieves it. In contrast, DMMS cannot be achieved even when both sides have identical preferences.",Two-sided matching meets fair division,2021-07-15 00:00:00
667,Nisarg Shah,"The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \emph{prediction} of other individuals' votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches.","Surprisingly Popular Voting Recovers Rankings, Surprisingly!",2021-05-19 00:00:00
668,Nisarg Shah,"We study the classical problem of matching n agents to n objects, where the agents have ranked preferences over the objects. We focus on two popular desiderata from the matching literature: Pareto optimality and rank-maximality. Instead of asking the agents to report their complete preferences, our goal is to learn a desirable matching from partial preferences, specifically a matching that is necessarily Pareto optimal (NPO) or necessarily rank-maximal (NRM) under any completion of the partial preferences. We focus on the top-k model in which agents reveal a prefix of their preference rankings. We design efficient algorithms to check if a given matching is NPO or NRM, and to check whether such a matching exists given top-k partial preferences. We also study online algorithms for eliciting partial preferences adaptively, and prove bounds on their competitive ratio.",Necessarily optimal one-sided matchings,2021-05-18 00:00:00
669,Nisarg Shah,"We consider approval-based committee elections, in which a size-k subset of available candidates must be selected given approval sets for each voter, indicating the candidates approved by the voter. A number of axioms capturing ideas of fairness and proportionality have been proposed for this framework. We argue that even the strongest of them, such as priceability and the core, only rule out certain undesirable committees, but fail to ensure that the selected committee is fair in all cases. We propose two new solution concepts, stable priceability and balanced stable priceability, and show that they select arguably fair committees. Our solution concepts come with a non-trivial-to-construct but easy-to-understand market-based explanation for why the chosen committee is fair. We show that stable priceability is closely related to the notion of Lindahl equilibrium from economics.",Market-based explanations of collective decisions,2021-05-18 00:00:00
670,Nisarg Shah,"We revisit the fundamental problem of predicting a binary ground truth based on independent binary judgments provided by experts. When the accuracy levels of the experts are known, the problem can be solved easily through maximum likelihood estimation. We consider, however, a setting in which we are given only a ranking of the experts by their accuracy. Motivated by the worst-case approach to handle the missing information, we consider three objective functions and design efficient algorithms for optimizing them. In particular, the recently popular distortion objective leads to an intuitive new rule. We show that our algorithms perform well empirically using real and synthetic data in collaborative filtering and political prediction domains.",Aggregating Binary Judgments Ranked By Accuracy,2021-05-18 00:00:00
671,Nisarg Shah,"The notion of distortion in social choice problems has been defined to measure the loss in efficiency -- typically measured by the utilitarian social welfare, the sum of utilities of the participating agents -- due to having access only to limited information about the preferences of the agents. We survey the most significant results of the literature on distortion from the past 15 years, and highlight important open problems and the most promising avenues of ongoing and future work.",Distortion in social choice problems: the first 15 years and beyond,2021-03-01 00:00:00
672,Florian Shkurti,selfsupervised methods have significantly closed the gap with endtoend supervised learning for image classification in the case of human action videos however where both appearance and motion are significant factors of variation this gap remains significant one of the key reasons for this is that sampling pairs of similar video clips a required step for many selfsupervised contrastive learning methods is currently done conservatively to avoid false positives a typical assumption is that similar clips only occur temporally close within a single video leading to insufficient examples of motion similarity to mitigate this we propose slic a clusteringbased selfsupervised contrastive learning method for human action videos our key contribution is that we improve upon the traditional intravideo positive sampling by using iterative clustering to group similar video instances this enables our method to leverage pseudolabels from the cluster assignments to sample harder positives and negatives slic outperforms stateoftheart video retrieval baselines by on top recall on ucf and by when directly transferred to hmdb with endtoend finetuning for action classification slic achieves top accuracy on ucf and on hmdb slic is also competitive with the stateoftheart in action classification after selfsupervised pretraining on kinetics less,SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos,"24 June, 2022"
673,Florian Shkurti,the robustness of visual navigation policies trained through imitation often hinges on the augmentation of the training imageaction pairs traditionally this has been done by collecting data from multiple cameras by using standard data augmentations from computer vision such as adding random noise to each image or by synthesizing training images in this paper we show that there is another practical alternative for data augmentation for visual navigation based on extrapolating viewpoint embeddings and actions nearby the ones observed in the training data our method makes use of the geometry of the visual navigation problem in d and d and relies on policies that are functions of equivariant embeddings as opposed to images given an imageaction pair from a training navigation dataset our neural network model predicts the latent representations of images at nearby viewpoints using the equivariance property and augments the dataset we then train a policy on the augmented dataset our simulation results indicate that policies trained in this way exhibit reduced crosstrack error and require fewer interventions compared to policies trained using standard augmentation methods we also show similar results in autonomous visual navigation by a real ground robot along a path of over m less,Augmenting Imitation Experience via Equivariant Representations,"14 October, 2021"
674,Florian Shkurti,human motion synthesis is an important problem with applications in graphics gaming and simulation environments for robotics existing methods require accurate motion capture data for training which is costly to obtain instead we propose a framework for training generative models of physically plausible human motion directly from monocular rgb videos which are much more widely available at the core of our method is a novel optimization formulation that corrects imperfect imagebased pose estimations by enforcing physics constraints and reasons about contacts in a differentiable way this optimization yields corrected d poses and motions as well as their corresponding contact forces results show that our physicallycorrected motions significantly outperform prior work on pose estimation we can then use these to train a generative model to synthesize future motion we demonstrate both qualitatively and quantitatively significantly improved motion estimation synthesis quality and physical plausibility achieved by our method on the large scale humanm dataset citehmpami as compared to prior kinematic and physicsbased methods by enabling learning of motion synthesis from video our method paves the way for largescale realistic and diverse motion synthesis less,Physics-based Human Motion Estimation and Synthesis from Videos,"20 September, 2021"
675,Florian Shkurti,selfsupervised goal proposal and reaching is a key component for exploration and efficient policy learning algorithms such a selfsupervised approach without access to any oracle goal sampling distribution requires deep exploration and commitment so that long horizon plans can be efficiently discovered in this paper we propose an exploration framework which learns a dynamicsaware manifold of reachable states for a goal our proposed method deterministically visits a state at the current frontier of reachable states commitmentreaching and then stochastically explores to reach the goal exploration this allocates exploration budget near the frontier of the reachable region instead of its interior we target the challenging problem of policy learning from initial and goal states specified as images and do not assume any access to the underlying groundtruth states of the robot and the environment to keep track of reachable latent states we propose a distanceconditioned reachability network that is trained to infer whether one state is reachable from another within the specified latent space distance given an initial state we obtain a frontier of reachable states from that state by incorporating a curriculum for sampling easier goals closer to the start state before more difficult goals we demonstrate that the proposed selfsupervised exploration algorithm superior performance compared to existing baselines on a set of challenging robotic environmentshttpssitesgooglecomviewleafexploration less,LEAF: Latent Exploration Along the Frontier,"26 April, 2021"
676,Florian Shkurti,we consider the problem of estimating an objects physical properties such as mass friction and elasticity directly from video sequences such a system identification problem is fundamentally illposed due to the loss of information during image formation current solutions require precise d labels which are laborintensive to gather and infeasible to create for many systems such as deformable solids or cloth we present gradsim a framework that overcomes the dependence on d supervision by leveraging differentiable multiphysics simulation and differentiable rendering to jointly model the evolution of scene dynamics and image formation this novel combination enables backpropagation from pixels in a video sequence through to the underlying physical attributes that generated them moreover our unified computation graph spanning from the dynamics and through the rendering process enables learning in challenging visuomotor control tasks without relying on statebased d supervision while obtaining performance competitive to or better than techniques that rely on precise d labels less,gradSim: Differentiable simulation for system identification and visuomotor control,"6 April, 2021"
677,Florian Shkurti,hairstyle transfer is challenging due to hair structure differences in the source and target hair therefore we propose latent optimization of hairstyles via orthogonalization loho an optimizationbased approach using gan inversion to infill missing hair structure details in latent space during hairstyle transfer our approach decomposes hair into three attributes perceptual structure appearance and style and includes tailored losses to model each of these attributes independently furthermore we propose twostage optimization and gradient orthogonalization to enable disentangled latent space optimization of our hair attributes using loho for latent space manipulation users can synthesize novel photorealistic images by manipulating hair attributes either individually or jointly transferring the desired attributes from reference hairstyles loho achieves a superior fid compared with the current stateoftheart sota for hairstyle transfer additionally loho preserves the subjects identity comparably well according to psnr and ssim when compared to sota image embedding pipelines code is available at httpsgithubcomdukebwloho less,LOHO: Latent Optimization of Hairstyles via Orthogonalization,"10 March, 2021"
678,Florian Shkurti,the potential benefits of modelfree reinforcement learning to real robotics systems are limited by its uninformed exploration that leads to slow convergence lack of dataefficiency and unnecessary interactions with the environment to address these drawbacks we propose a method that combines reinforcement and imitation learning by shaping the reward function with a stateandactiondependent potential that is trained from demonstration data using a generative model we show that this accelerates policy learning by specifying highvalue areas of the state and action space that are worth exploring first unlike the majority of existing methods that assume optimal demonstrations and incorporate the demonstration data as hard constraints on policy optimization we instead incorporate demonstration data as advice in the form of a reward shaping potential trained as a generative model of states and actions in particular we examine both normalizing flows and generative adversarial networks to represent these potentials we show that unlike many existing approaches that incorporate demonstrations as hard constraints our approach is unbiased even in the case of suboptimal and noisy demonstrations we present an extensive range of simulations as well as experiments on the franka emika dof arm to demonstrate the practicality of our method less,Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models,"2 November, 2020"
679,Florian Shkurti,we present navgoal a dataefficient and endtoend learning method for goalconditioned visual navigation our technique is used to train a navigation policy that enables a robot to navigate close to sparse geographic waypoints provided by a user without any prior map all while avoiding obstacles and choosing paths that cover userinformed regions of interest our approach is based on recent advances in conditional imitation learning generalpurpose safe and informative actions are demonstrated by a human expert the learned policy is subsequently extended to be goalconditioned by training with hindsight relabelling guided by the robots relative localization system which requires no additional manual annotation we deployed our method on an underwater vehicle in the open ocean to collect scientifically relevant data of coral reefs which allowed our robot to operate safely and autonomously even at very close proximity to the coral our field deployments have demonstrated over a kilometer of autonomous visual navigation where the robot reaches on the order of waypoints while collecting scientifically relevant data this is done while travelling within m altitude from sensitive corals and exhibiting significant learned agility to overcome turbulent ocean conditions and to actively avoid collisions less,Vision-Based Goal-Conditioned Policies for Underwater Navigation in the Presence of Obstacles,"29 June, 2020"
680,Florian Shkurti,mobile manipulators consist of a mobile platform equipped with one or more robot arms and are of interest for a wide array of challenging tasks because of their extended workspace and dexterity typically mobile manipulators are deployed in slowmotion collaborative robot scenarios in this paper we consider scenarios where accurate highspeed motions are required we introduce a framework for this regime of tasks including two main components i a bilevel motion optimization algorithm for realtime trajectory generation which relies on sequential quadratic programming sqp and quadratic programming qp respectively and ii a learningbased controller optimized for precise tracking of highspeed motions via a learned inverse dynamics model we evaluate our framework with a mobile manipulator platform through numerous highspeed ball catching experiments where we show a success rate of to the best of our knowledge this success rate exceeds the reported performance of existing related systems and sets a new state of the art less,Catch the Ball: Accurate High-Speed Motions for Mobile Manipulators via Inverse Dynamics Learning,"16 March, 2020"
681,Florian Shkurti,as demand drives systems to generalize to various domains and problems the study of multitask transfer and lifelong learning has become an increasingly important pursuit in discrete domains performance on the atari game suite has emerged as the de facto benchmark for assessing multitask learning however in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly in this work we describe a benchmark set of tasks that we have developed in an extendable framework based on openai gym we run a simple baseline using trust region policy optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask transfer and lifelong learning in continuous domains less,Benchmark Environments for Multitask Learning in Continuous Domains,"14 August, 2017"
682,Karan Singh,we consider the problem of controlling an invasive mechanical ventilator for pressurecontrolled ventilation a controller must let air in and out of a sedated patients lungs according to a trajectory of airway pressures specified by a clinician handtuned pid controllers and similar variants have comprised the industry standard for decades yet can behave poorly by over or undershooting their target or oscillating rapidly we consider a datadriven machine learning approach first we train a simulator based on data we collect from an artificial lung then we train deep neural network controllers on these simulatorswe show that our controllers are able to track target pressure waveforms significantly better than pid controllers we further show that a learned controller generalizes across lungs with varying characteristics much more readily than pid controllers do less,Machine Learning for Mechanical Ventilation Control,"18 January, 2022"
683,Karan Singh,we study efficient algorithms for reinforcement learning in markov decision processes whose complexity is independent of the number of states this formulation succinctly captures large scale problems but is also known to be computationally hard in its general form previous approaches attempt to circumvent the computational hardness by assuming structure in either transition function or the value function or by relaxing the solution guarantee to a local optimality condition we consider the methodology of boosting borrowed from supervised learning for converting weak learners into an accurate policy the notion of weak learning we study is that of sampledbased approximate optimization of linear functions over policies under this assumption of weak learnability we give an efficient algorithm that is capable of improving the accuracy of such weak learning methods till global optimality is reached we prove sample complexity and running time bounds on our method that are polynomial in the natural parameters of the problem approximation guarantee discount factor distribution mismatch and number of actions in particular our bound does not depend on the number of states a technical difficulty in applying previous boosting results is that the value function over policy space is not convex we show how to use a nonconvex variant of the frankwolfe method coupled with recent advances in gradient boosting that allow incorporating a weak learner with multiplicative approximation guarantee to overcome the nonconvexity and attain global convergence less,A Boosting Approach to Reinforcement Learning,"22 August, 2021"
684,Karan Singh,the recent developments in technology have rewarded us with amazing audio synthesis models like tacotron and wavenets on the other side it poses greater threats such as speech clones and deep fakes that may go undetected to tackle these alarming situations there is an urgent need to propose models that can help discriminate a synthesized speech from an actual human speech and also identify the source of such a synthesis here we propose a model based on convolutional neural network cnn and bidirectional recurrent neural network birnn that helps to achieve both the aforementioned objectives the temporal dependencies present in ai synthesized speech are exploited using bidirectional rnn and cnn the model outperforms the stateoftheart approaches by classifying the ai synthesized audio from real human speech with an error rate of and detecting the underlying architecture with an accuracy of less,Using Deep Learning Techniques and Inferential Speech Statistics for AI Synthesised Speech Recognition,"23 July, 2021"
685,Karan Singh,we present a new approach for modelling musculoskeletal anatomy unlike previous methods we do not model individual muscle shapes as geometric primitives polygonal meshes nurbs etc instead we adopt a volumetric segmentation approach where every point in our volume is assigned to a muscle fat or bone tissue we provide an interactive modelling tool where the user controls the segmentation via muscle curves and we visualize the muscle shapes using volumetric rendering muscle curves enable intuitive yet powerful control over the muscle shapes this representation allows us to automatically handle intersections between different tissues musclemuscle musclebone and muscleskin during the modelling and automates computation of muscle fiber fields we further introduce a novel algorithm for converting the volumetric muscle representation into tetrahedral or surface geometry for use in downstream tasks additionally we introduce an interactive skeleton authoring tool that allows the users to create skeletal anatomy starting from only a skin mesh using a library of bone parts less,Interactive Modelling of Volumetric Musculoskeletal Anatomy,"9 June, 2021"
686,Karan Singh,we consider the setting of iterative learning control or modelbased policy learning in the presence of uncertain timevarying dynamics in this setting we propose a new performance metric planning regret which replaces the standard stochastic uncertainty assumptions with worst case regret based on recent advances in nonstochastic control we design a new iterative algorithm for minimizing planning regret that is more robust to model mismatch and uncertainty we provide theoretical and empirical evidence that the proposed algorithm outperforms existing methods on several benchmarks less,A Regret Minimization Approach to Iterative Learning Control,"26 February, 2021"
687,Karan Singh,we consider the decisionmaking framework of online convex optimization with a very large number of experts this setting is ubiquitous in contextual and reinforcement learning problems where the size of the policy class renders enumeration and search within the policy class infeasible instead we consider generalizing the methodology of online boosting we define a weak learning algorithm as a mechanism that guarantees multiplicatively approximate regret against a base class of experts in this access model we give an efficient boosting algorithm that guarantees nearoptimal regret against the convex hull of the base class we consider both full and partial aka bandit information feedback models we also give an analogous efficient boosting algorithm for the iid statistical setting our results simultaneously generalize online boosting and gradient boosting guarantees to contextual learning model online convex optimization and bandit linear optimization settings less,Boosting for Online Convex Optimization,"18 February, 2021"
688,Karan Singh,studies connected with the investigations of nonfermi liquid nfl systems continue to attract interest in condensed matter physics community understanding the anomalous physical properties exhibited by such systems and its related electronic structures is one of the central research topics in this area in this context cebased and cesite diluted with nonmagnetic ions compounds provide a fertile playground here we present a detailed study of nonlinear dc susceptibility and combined density functional theory plus dynamical mean field theory dftdmft on celage theoretical investigation of f partial density of states local susceptibility and selfenergy demonstrates the presence of nfl behavior which is associated with fluctuating local moments nonlinear dc susceptibility studies on this compound reveal that the transition from nfl state to the new phase is due to development of the biquadratic exchange coupling and it obeys the nonlinear susceptibility scaling under the application of magnetic fields local moments interact spatially through conduction electrons resulting in magnetic fluctuations our studies point to the fact that the origin of the observed biquadratic exchange coupling is due to the spatial magnetic fluctuations less,Coexistance of non-Fermi liquid behavior and bi-quadratic exchange coupling in La-substituted CeGe: Non-linear susceptibility and DFT + DMFT study,"17 December, 2020"
689,Karan Singh,adaptive regularization methods premultiply a descent direction by a preconditioning matrix due to the large number of parameters of machine learning problems fullmatrix preconditioning methods are prohibitively expensive we show how to modify fullmatrix adaptive regularization in order to make it practical and effective we also provide a novel theoretical analysis for adaptive regularization in nonconvex optimization settings the core of our algorithm termed ggt consists of the efficient computation of the inverse square root of a lowrank matrix our preliminary experiments show improved iterationwise convergence rates across synthetic tasks and standard deep learning benchmarks and that the more carefullypreconditioned steps sometimes lead to a better solution less,Efficient Full-Matrix Adaptive Regularization,"17 November, 2020"
690,Karan Singh,we consider the problem of controlling a possibly unknown linear dynamical system with adversarial perturbations adversarially chosen convex loss functions and partially observed states known as nonstochastic control we introduce a controller parametrization based on the denoised observations and prove that applying online gradient descent to this parametrization yields a new controller which attains sublinear regret vs a large class of closedloop policies in the fullyadversarial setting our controller attains an optimal regret bound of sqrttwhen the system is known and when combined with an initial stage of leastsquares estimation t when the system is unknown both yield the first sublinear regret for the partially observed setting our bounds are the first in the nonstochastic control setting that compete with emphall stabilizing linear dynamical controllers not just state feedback moreover in the presence of semiadversarial noise containing both stochastic and adversarial components our controller attains the optimal regret bounds of mathrmpolylog t when the system is known and sqrtt when unknown to our knowledge this gives the first endtoend sqrtt regret for online linear quadratic gaussian controller and applies in a more general setting with adversarial losses and semiadversarial noise less,Improper Learning for Non-Stochastic Control,"24 June, 2020"
691,Karan Singh,in this work we report the results of dc susceptibility ac susceptibility and related technique resistivity transverse and longitudinal magnetoresistance and heat capacity on polycrystalline magnetic semimetal cealge this compound undergoes antiferromagnetic type ordering around k t under application of external magnetic fields parallel alignment of magnetic moments is favored above tesla at low field and temperature frequency and ac field amplitude response of ac susceptibility indicate to the presence of spinlattice relaxation phenomena the observation of spinlattice interaction suggests to the presence of rashbadresselhaus spinorbit interaction which is associated with inversion and time reversal symmetry breaking additionally presence of negative and asymmetric longitudinal magnetoresistance indicates anomalous velocity contribution to the magnetoresistance due to rashbadresselhaus spinorbit interaction which is further studied by heat capacity less,Spin-lattice relaxation phenomena in the magnetic state of a suggested Weyl semimetal CeAlGe,"1 June, 2020"
692,Karan Singh,to preserve anonymity and obfuscate their identity on online platforms users may morph their text and portray themselves as a different gender or demographic similarly a chatbot may need to customize its communication style to improve engagement with its audience this manner of changing the style of written text has gained significant attention in recent years yet these past research works largely cater to the transfer of single style attributes the disadvantage of focusing on a single style alone is that this often results in target text where other existing style attributes behave unpredictably or are unfairly dominated by the new style to counteract this behavior it would be nice to have a style transfer mechanism that can transfer or control multiple styles simultaneously and fairly through such an approach one could obtain obfuscated or written text incorporated with a desired degree of multiple soft styles such as femalequality politeness or formalness in this work we demonstrate that the transfer of multiple styles cannot be achieved by sequentially performing multiple singlestyle transfers this is because each single styletransfer step often reverses or dominates over the style incorporated by a previous transfer step we then propose a neural network architecture for fairly transferring multiple style attributes in a given text we test our architecture on the yelp data set to demonstrate our superior performance as compared to existing onestyle transfer steps performed in a sequence less,Fair Transfer of Multiple Style Attributes in Text,"18 January, 2020"
693,Karan Singh,we study optimal regret bounds for control in linear dynamical systems under adversarially changing strongly convex cost functions given the knowledge of transition dynamics this includes several well studied and fundamental frameworks such as the kalman filter and the linear quadratic regulator state of the art methods achieve regret which scales as osqrtt where t is the time horizon we show that the optimal regret in this setting can be significantly smaller scaling as otextpolylog t this regret bound is achieved by two different efficient iterative methods online gradient descent and online natural gradient less,Logarithmic Regret for Online Control,"11 September, 2019"
694,Karan Singh,computer animation in conjunction with d printing has the potential to positively impact traditional stopmotion animation as d printing every frame of a computer animation is prohibitively slow and expensive d printed stopmotion can only be viable if animations can be faithfully reproduced using a compact library of d printed and efficiently assemblable parts we thus present the first system for processing computer animation sequences typically faces to produce an optimal set of replacement parts for use in d printed stopmotion animation given an input animation sequence of topology invariant deforming meshes our problem is to output a library of replacement parts and peranimationframe assignment of the parts such that we maximally approximate the input animation while minimizing the amount of d printing and assembly inspired by current stopmotion workflows a user manually indicates which parts of the model are preferred for segmentation then we find curves with minimal deformation along which to segment the mesh we then present a novel algorithm to zero out deformations along the segment boundaries so that replacement sets for each part can be interchangeably and seamlessly assembled together the part boundaries are designed to ease d printing and instrumentation for assembly each part is then independently optimized using a graphcut technique to find a set of replacements whose size can be user defined or automatically computed to adhere to a printing budget or allowed deviation from the original animation our evaluation is threefold we show results on a variety of facial animations both digital and d printed critiqued by a professional animator we show the impact of various algorithmic parameters and compare our results to naive solutions our approach can reduce the printing time and cost significantly for stopmotion animated films less,A system for efficient 3D printed stop-motion face animation,"23 July, 2019"
695,Karan Singh,polycrystalline cege is investigated by means of dc and ac susceptibility nonlinear dc susceptibility electrical transport and heat capacity measurements in the low temperature regime this compound shows two peaks at low magnetic field approximately around and k due to antiferromagnetic ordering and subsequent spin rearrangement respectively investigation of nonlinear dc susceptibility reveals a presence of higher order magnetization which results in the development of a new order parameter around k this leads to a lowering of symmetry of the magnetic state the order parameter increases with decreasing temperature and stabilizes around k consequently the symmetry of the magnetic state is preserved below this transition heat capacity and resistivity results indicate the presence of a gap opening around k on portion of fermi surface due to evolution of the fermi surface magnetoresistance behavior and violation of kohlers rule suggest that the evolution of fermi surface changes the symmetry of magnetic state the observation of new order parameter which is of second order is also confirmed from the landau free energy theory less,Possibility of a new order parameter driven by multipolar moment and Fermi surface evolution in CeGe,"28 March, 2019"
696,Karan Singh,the first observational run of the advanced ligo detectors from september to january saw the first detections of gravitational waves from binary black hole mergers in this paper we present full results from a search for binary black hole merger signals with total masses up to modot and detailed implications from our observations of these systems our search based on generalrelativistic models of gravitational wave signals from binary black hole systems unambiguously identified two signals gw and gw with a significance of greater than over the observing period it also identified a third possible signal lvt with substantially lower significance and with an probability of being of astrophysical origin we provide detailed estimates of the parameters of the observed systems both gw and gw provide an unprecedented opportunity to study the twobody motion of a compactobject binary in the large velocity highly nonlinear regime we do not observe any deviations from general relativity and place improved empirical bounds on several highorder postnewtonian coefficients from our observations we infer stellarmass binary black hole merger rates lying in the range mathrmgpc mathrmyr these observations are beginning to inform astrophysical predictions of binary black hole formation rates and indicate that future observing runs of the advanced detector network will yield many more gravitational wave detections less,Binary Black Hole Mergers in the first Advanced LIGO Observing Run,"23 October, 2018"
697,Karan Singh,we present a novel deeplearning based approach to producing animatorcentric speech motion curves that drive a jali or standard facsbased production facerig directly from input audio our threestage long shortterm memory lstm network architecture is motivated by psycholinguistic insights segmenting speech audio into a stream of phoneticgroups is sufficient for viseme construction speech styles like mumbling or shouting are strongly corelated to the motion of facial landmarks and animator style is encoded in viseme motion curve profiles our contribution is an automatic realtime lipsynchronization from audio solution that integrates seamlessly into existing animation pipelines we evaluate our results by crossvalidation to groundtruth data animator critique and edits visual comparison to recent deeplearning lipsynchronization solutions and showing our approach to be resilient to diversity in speaker and language less,VisemeNet: Audio-Driven Animator-Centric Speech Animation,"23 May, 2018"
698,Karan Singh,we report the magnetic thermodynamic and transport properties of a heavy fermion compound cenige this compound undergoes two antiferromagnetic transitions around and k it is observed in heat capacity that as magnetic field is increased to t the two peak merge into a single peak around k however this peak is not suppressed under the application of magnetic field instead a new feature develops at k above t the magnetic field induced new feature is investigated through entropy evolution magnetic gruneisen parameter and resistivity studies these studies emphasis the fact that partial magnetic frustration due to field induced spin fluctuation is responsible for this observed feature this partially frustrated regime develops a new antiferromagnetically ordered phase at high fields in this compound magnetic field induced qcp is absent implying that the behavior of cenige is not in accordance to doniach model proposed for heavy fermions compounds less,Signature of partially frustrated moments and a new magnetic phase in CeNiGe2,"18 December, 2017"
699,Karan Singh,we consider regret minimization in repeated games with nonconvex loss functions minimizing the standard notion of regret is computationally intractable thus we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum we give gradientbased methods that achieve optimal regret which in turn guarantee convergence to equilibrium in this framework less,Efficient Regret Minimization in Non-Convex Games,"31 July, 2017"
700,Karan Singh,we present a fast direct algorithm for computing symmetric factorizations ie a wwt of symmetric positivedefinite hierarchical matrices with weakadmissibility conditions the computational cost for the symmetric factorization scales as mathcalon log n for hierarchically offdiagonal lowrank matrices once this factorization is obtained the cost for inversion application and determinant computation scales as mathcalon log n in particular this allows for the near optimal generation of correlated random variates in the case where a is a covariance matrix this symmetric factorization algorithm depends on two key ingredients first we present a novel symmetric factorization formula for lowrank updates to the identity of the form iukut this factorization can be computed in mathcalon time if the rank of the perturbation is sufficiently small second combining this formula with a recursive divideandconquer strategy near linear complexity symmetric factorizations for hierarchically structured matrices can be obtained we present numerical results for matrices relevant to problems in probability statistics gaussian processes interpolation radial basis functions and brownian dynamics calculations in fluid mechanics the rotneprageryamakawa tensor less,Fast symmetric factorization of hierarchical matrices with applications,"30 December, 2016"
701,Karan Singh,a transient gravitationalwave signal gw was identified in the twin advanced ligo detectors on september at utc to assess the implications of this discovery the detectors remained in operation with unchanged configurations over a period of d around the time of the signal at the detection statistic threshold corresponding to that observed for gw our search of the days of simultaneous twodetector observational data is estimated to have a false alarm rate far of times mathrmyr yielding a pvalue for gw of times parameter estimation followup on this trigger identifies its source as a binary black hole bbh merger with component masses m m leftright modot at redshift z median and credible range here we report on the constraints these observations place on the rate of bbh coalescences considering only gw assuming that all bbhs in the universe have the same masses and spins as this event imposing a search far threshold of per years and assuming that the bbh merger rate is constant in the comoving frame we infer a credible range of merger rates between mathrmgpc mathrmyr comoving frame incorporating all search triggers that pass a much lower threshold while accounting for the uncertainty in the astrophysical origin of each trigger we estimate a higher rate ranging from mathrmgpc mathrmyr depending on assumptions about the bbh mass distribution all together our various rate estimates fall in the conservative range mathrmgpc mathrmyr less,The Rate of Binary Black Hole Mergers Inferred from Advanced LIGO Observations Surrounding GW150914,"20 September, 2016"
702,Suzanne Stevenson,we adopt an evolutionary view on language change in which cognitive factors in addition to social ones affect the fitness of words and their success in the linguistic ecosystem specifically we propose a variety of psycholinguistic factors semantic distributional and phonological that we hypothesize are predictive of lexical decline in which words greatly decrease in frequency over time using historical data across three languages english french and german we find that most of our proposed factors show a significant difference in the expected direction between each curated set of declining words and their matched stable words moreover logistic regression analyses show that semantic and distributional factors are significant in predicting declining words further diachronic analysis reveals that declining words tend to decrease in the diversity of their lexical contexts over time gradually narrowing their ecological niches less,Quantifying Cognitive Factors in Lexical Decline,"12 October, 2021"
703,Suzanne Stevenson,a large body of research on genderlinked language has established foundations regarding crossgender differences in lexical emotional and topical preferences along with their sociological underpinnings we compile a novel large and diverse corpus of spontaneous linguistic productions annotated with speakers gender and perform a first largescale empirical study of distinctions in the usage of textitfigurative language between male and female authors our analyses suggest that idiomatic choices reflect genderspecific lexical and semantic preferences in general language mens and womens idiomatic usages express higher emotion than their literal language with detectable albeit more subtle differences between male and female authors along the dimension of dominance compared to similar distinctions in their literal utterances and contextual analysis of idiomatic expressions reveals considerable differences reflecting subtle divergences in usage environments shaped by crossgender communication styles and semantic biases less,Pick a Fight or Bite your Tongue: Investigation of Gender Differences in Idiomatic Language Usage,"31 October, 2020"
704,Suzanne Stevenson,lexical semantic typology has identified important crosslinguistic generalizations about the variation and commonalities in polysemy patternshow languages package up meanings into words recent computational research has enabled investigation of lexical semantics at a much larger scale but little work has explored lexical typology across semantic domains nor the factors that influence crosslinguistic similarities we present a novel computational framework that quantifies semantic affinity the crosslinguistic similarity of lexical semantics for a concept our approach defines a common multilingual semantic space that enables a direct comparison of the lexical expression of concepts across languages we validate our framework against empirical findings on lexical semantic typology at both the concept and domain levels our results reveal an intricate interaction between semantic domains and extralinguistic factors beyond language phylogeny that coshape the typology of polysemy across languages less,The Typology of Polysemy: A Multilingual Distributional Framework,"2 June, 2020"
705,Suzanne Stevenson,in contrast to many decades of research on oral codeswitching the study of written multilingual productions has only recently enjoyed a surge of interest many open questions remain regarding the sociolinguistic underpinnings of written codeswitching and progress has been limited by a lack of suitable resources we introduce a novel large and diverse dataset of written codeswitched productions curated from topical threads of multiple bilingual communities on the reddit discussion platform and explore questions that were mainly addressed in the context of spoken language thus far we investigate whether findings in oral codeswitching concerning content and style as well as speaker proficiency are carried over into written codeswitching in discussion forums the released dataset can further facilitate a range of research and practical activities less,CodeSwitch-Reddit: Exploration of Written Multilingual Discourse in Online Discussion Forums,"30 August, 2019"
706,Suzanne Stevenson,children can use the statistical regularities of their environment to learn word meanings a mechanism known as crosssituational learning we take a computational approach to investigate how the information present during each observation in a crosssituational framework can affect the overall acquisition of word meanings we do so by formulating various inthemoment learning mechanisms that are sensitive to different statistics of the environment such as counts and conditional probabilities each mechanism introduces a unique source of competition or mutual exclusivity bias to the model the mechanism that maximally uses the models knowledge of word meanings performs the best moreover the gap between this mechanism and others is amplified in more challenging learning scenarios such as learning from few examples less,Calculating Probabilities Simplifies Word Learning,"21 February, 2017"
707,Suzanne Stevenson,"Gender associations have been a long‐standing research topic in psychological and social sciences. Although it is known that children learn aspects of gender associations at a young age, it is not well understood how they might emerge through the course of development. We investigate whether gender associations, such as the association of dresses with women and bulldozers with men, are reflected in the linguistic communication of young children from ages 1–5. Drawing on recent methods from machine learning, we use word embeddings derived from large text corpora including news articles and web pages as a proxy for gender associations in society, and we compare those with the gender associations of words uttered by caretakers and children in children's linguistic environment. We quantify gender associations in childhood language through gender probability, which measures the extent to which",The Emergence of Gender Associations in Child Language Development,2022/6
708,Suzanne Stevenson,"To process language in a way that is compatible with human expectations in a communicative interaction, we need computational representations of lexical properties that form the basis of human knowledge of words. In this article, we concentrate on word-level semantics. We discuss key concepts and issues that underlie the scientific understanding of the human lexicon: its richly structured semantic representations, their ready and continual adaptability, and their grounding in crosslinguistically valid conceptualization. We assess the state of the art in natural language processing (NLP) in achieving these identified properties, and suggest ways in which the language sciences can inspire new approaches to their computational instantiation.",Beyond the Benchmarks: Toward Human-Like Lexical Representations,2022
709,Suzanne Stevenson,"We adopt an evolutionary view on language change in which cognitive factors (in addition to social ones) affect the fitness of words and their success in the linguistic ecosystem. Specifically, we propose a variety of psycholinguistic factors—semantic, distributional, and phonological—that we hypothesize are predictive of lexical decline, in which words greatly decrease in frequency over time. Using historical data across three languages (English, French, and German), we find that most of our proposed factors show a significant difference in the expected direction between each curated set of declining words and their matched stable words. Moreover, logistic regression analyses show that semantic and distributional factors are significant in predicting declining words. Further diachronic analysis reveals that declining words tend to decrease in the diversity of their lexical contexts over time, gradually narrowing",Quantifying Cognitive Factors in Lexical Decline,2021-12-30 00:00:00
710,Suzanne Stevenson,"While distributional semantic models (DSMs) can successfully capture the similarity structure within a semantic domain, less is known about their ability to represent abstract semantic properties that hold across domains. Such properties can form the basis for abstract semantic classes that are a crucial aspect of human semantic knowledge. For example, the abstract class of extreme adjectives (such as brilliant and freezing) spans a wide range of domains (here, INTELLIGENCE and TEMPERATURE). Using a model that compares query items to an aggregate DSM representation of a set of extreme adjectives, we show that novel adjectives can be classified accurately, supporting the insight that a cross-domain property like extremeness can be captured in a word’s DSM representation. We then use the extremeness classifier to model the emergence of intensifier meaning in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property.",A formidable ability: detecting adjectival extremeness with DSMs,2021/8
711,Suzanne Stevenson,"Lexical ambiguity—the phenomenon of a single word having multiple, distinguishable senses—is pervasive in language. Both the degree of ambiguity of a word (roughly, its number of senses) and the relatedness of those senses have been found to have widespread effects on language acquisition and processing. Recently, distributional approaches to semantics, in which a word's meaning is determined by its contexts, have led to successful research quantifying the degree of ambiguity, but these measures have not distinguished between the ambiguity of words with multiple related senses versus multiple unrelated meanings. In this work, we present the first assessment of whether distributional meaning representations can capture the ambiguity structure of a word, including both the number and relatedness of senses. On a very large sample of English words, we find that some, but not all, distributional semantic",Probing lexical ambiguity: word vectors encode number and relatedness of senses,2021/5
712,Suzanne Stevenson,"When bilingual speakers name stimuli such as colors or objects, their naming patterns can differ from those of monolingual speakers. Three accounts have been proposed to explain these differences–conceptual change, online lexical coactivation, and L1 footprint–yet these have not been empirically evaluated against each other. In this study, we propose a novel computational cognitive model which operationalizes each of these proposals as a mechanism of crosslinguistic influence, such that we can study their individual and combined effects on the model’s behavior. We focus on the domain of color in which we model existing experimental data collected from Navajo and English monolinguals and Navajo–English bilinguals. Our color learning model extends a statistical learning procedure for mixture models to the acquisition of labelled categories, and achieves bilingual learning by maintaining two sets of color categories and associated color words, which are connected in varying ways according to the three crosslinguistic mechanisms. We test the combinations of mechanisms in a color naming task, and analyze the match between the naming patterns of the model and the differences between bilingual and monolingual human speakers. Our results suggest that gradual conceptual change following crosslinguistic transfer at the initial learning stage can best capture the observed differences in human color naming patterns. While lexical coactivation combined with initial transfer can account for some of the empirical data, this mechanism consistently performs less well than that of conceptual change.",Modeling color naming in bilinguals: Computational mechanisms of crosslinguistic influence,2021-01-26 00:00:00
713,Suzanne Stevenson,"Children learn word meanings by making use of commonalities across the usages of a word in different situations. However, early word learning experiences have a high level of uncertainty. For a word in an utterance, there are many possible meanings in the environment (referential uncertainty). Similarly, for a meaning, there are multiple possible words in the utterance (linguistic uncertainty). We propose a general framework to investigate the role of mutual exclusivity bias (asserting one-to-one mappings between words and their meanings) in early word learning. Through a set of computational studies, we show that to successfully learn word meanings under uncertainty, a model needs to implement two types of competition: words competing for the association to a meaning reduces linguistic uncertainty, and meanings competing for a word limits referential uncertainty. Our work highlights the importance of an algorithmic-level analysis to shed light on different mechanisms that implement a computational-level theory.",Mutual Exclusivity as Competition in Cross-situational Word Learning,2021
714,Suzanne Stevenson,"Conversational interaction involves integrating the perspectives of multiple interlocutors with varying knowledge and beliefs. An issue that has received little attention in cognitive modeling of pragmatics is how speakers deal with the choice of words like come that are inherently perspectival. How do such lexical perspectival items fit into a speaker's overall integration of conversational perspective? We present new experimental results on production of perspectival words, in which speakers have varying degrees of certainty about their addressee's perspective. We show that the Multiple Perspectives Model closely fits the empirical data, lending support to the hypothesis that use of perspectival words can be naturally accommodated as a type of conversational perspective taking.",Come Together: Integrating Perspective Taking and Perspectival Expressions,2021
715,Suzanne Stevenson,"Language is inherently flexible: people continually generalize over observed data to produce creative linguistic expressions. This process is constrained by a wide range of factors, whose interaction is not fully understood. We present a novel study of the creative use of verb constructions ``in the wild'', in a very large social media corpus. Our first experiment confirms on this large-scale data the important interaction of category variability and item similarity within creative extensions in actual language use. Our second experiment confirms the novel hypothesis that low-frequency exemplars may play a role in generalization by signaling the area of semantic space where creative coinages occur.",Coin it up: Generalization of creative constructions in the wild,2021
716,Suzanne Stevenson,recent empirical and modeling research has focused on the semantic fluency task because it is informative about semantic memory an interesting interplay arises between the richness of representations in semantic memory and the complexity of algorithms required to process it it has remained an open question whether representations of words and their relations learned from language use can enable a simple search algorithm to mimic the observed behavior in the fluency task here we show that it is plausible to learn rich representations from naturalistic data for which a very simple search algorithm a random walk can replicate the human patterns we suggest that explicitly structuring knowledge about words into a semantic network plays a crucial role in modeling human behavior in memory search and retrieval moreover this is the case across a range of semantic information sources less,Simple Search Algorithms on Semantic Networks Learned from Language Use,"10 February, 2016"
717,Babak Taati,druginduced parkinsonism affects many older adults with dementia often causing gait disturbances new advances in visionbased human poseestimation have opened possibilities for frequent and unobtrusive analysis of gait in residential settings this work leverages novel spatialtemporal graph convolutional network stgcn architectures and training procedures to predict clinical scores of parkinsonism in gait from video of individuals with dementia we propose a twostage training approach consisting of a selfsupervised pretraining stage that encourages the stgcn model to learn about gait patterns before predicting clinical scores in the finetuning stage the proposed stgcn models are evaluated on joint trajectories extracted from video and are compared against traditional ordinal linear random forest regression models and temporal convolutional network baselines three d human poseestimation libraries openpose detectron alphapose and the microsoft kinect d and d are used to extract joint trajectories of natural walking bouts from older adults with dementia a subset of walks from participants is annotated with scores of parkinsonism severity on the gait criteria of the unified parkinsons disease rating scale updrs and the simpsonangus scale sas our results demonstrate that stgcn models operating on d joint trajectories extracted from the kinect consistently outperform all other models and feature sets prediction of parkinsonism scores in natural walking bouts of unseen participants remains a challenging task with the best models achieving macroaveraged fscores of and for updrsgait and sasgait respectively pretrained model and demo code for this work is available httpsgithubcomtaatiteamstgcnparkinsonismprediction less,Estimating Parkinsonism Severity in Natural Gait Videos of Older Adults with Dementia,"1 October, 2021"
718,Babak Taati,orofacial deficits are common in people with parkinsons disease pd and their evolution might represent an important biomarker of disease progression we are developing an automated system for assessment of orofacial function in pd that can be used inhome or inclinic and can provide useful and objective clinical information that informs disease management our current approach relies on color and depth cameras for the estimation of d facial movements however depth cameras are not commonly available might be expensive and require specialized software for control and data processing the objective of this paper was to evaluate if depth cameras are needed to differentiate between healthy controls and pd patients based on features extracted from orofacial kinematics results indicate that d features extracted from color cameras only are as informative as d features extracted from color and depth cameras differentiating healthy controls from pd patients these results pave the way for the development of a universal system for automatic and objective assessment of orofacial function in pd less,Estimation of Orofacial Kinematics in Parkinson's Disease: Comparison of 2D and 3D Markerless Systems for Motion Tracking,"18 March, 2020"
719,Babak Taati,accurate facial expression analysis is an essential step in various clinical applications that involve physical and mental health assessments of older adults eg diagnosis of pain or depression although remarkable progress has been achieved toward developing robust facial landmark detection methods stateoftheart methods still face many challenges when encountering uncontrolled environments different ranges of facial expressions and different demographics of the population a recent study has revealed that the health status of individuals can also affect the performance of facial landmark detection methods on front views of faces in this work we investigate this matter in a much greater context using seven facial landmark detection methods we perform our evaluation not only on frontal faces but also on profile faces and in various regions of the face our results shed light on limitations of the existing methods and challenges of applying these methods in clinical settings by indicating a significant difference between the performance of stateoftheart when tested on the profile or frontal faces of individuals with vs without dementia insights on the existing bias for all regions of the face and the presence of this bias despite retrainingfinetuning with various configurations of six datasets less,Limitations and Biases in Facial Landmark Detection -- An Empirical Study on Older Adults with Dementia,"17 May, 2019"
720,Babak Taati,medical imaging machine learning algorithms are usually evaluated on a single dataset although training and testing are performed on different subsets of the dataset models built on one study show limited capability to generalize to other studies while database bias has been recognized as a serious problem in the computer vision community it has remained largely unnoticed in medical imaging research transfer learning thus remains confined to the reuse of feature representations requiring retraining on the new dataset as a result machine learning models do not generalize even when trained on imaging datasets that were captured to study the same variable of interest the ability to transfer knowledge gleaned from one study to another without the need for retraining if possible would provide reassurance that the models are learning knowledge fundamental to the problem under study instead of latching onto the idiosyncracies of a dataset in this paper we situate the problem of dataset bias in the context of medical imaging studies we show empirical evidence that such a problem exists in medical datasets we then present a framework to unlearn study membership as a means to handle the problem of database bias our main idea is to take the data from the original feature space to an intermediate space where the data points are indistinguishable in terms of which study they come from while maintaining the recognition capability with respect to the variable of interest this will promote models which learn the more general properties of the etiology under study instead of aligning to datasetspecific peculiarities essentially our proposed model learns to unlearn the dataset bias less,Learning to Unlearn: Building Immunity to Dataset Bias in Medical Imaging Studies,"2 December, 2018"
721,Babak Taati,objective to apply deep learning pose estimation algorithms for visionbased assessment of parkinsonism and levodopainduced dyskinesia lid methods nine participants with parkinsons disease pd and lid completed a levodopa infusion protocol where symptoms were assessed at regular intervals using the unified dyskinesia rating scale udysrs and unified parkinsons disease rating scale updrs a stateoftheart deep learning pose estimation method was used to extract movement trajectories from videos of pd assessments features of the movement trajectories were used to detect and estimate the severity of parkinsonism and lid using random forest communication and drinking tasks were used to assess lid while leg agility and toe tapping tasks were used to assess parkinsonism feature sets from tasks were also combined to predict total udysrs and updrs part iii scores results for lid the communication task yielded the best results for dyskinesia severity estimation r detection auc for parkinsonism leg agility had better results for severity estimation r while toe tapping was better for detection auc udysrs and updrs part iii scores were predicted with r and respectively conclusion this paper presents the first application of deep learning for visionbased assessment of parkinsonism and lid and demonstrates promising performance for the future translation of deep learning to pd clinical practices significance the proposed system provides insight into the potential of computer vision and deep learning for clinical application in pd less,Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation,"1 August, 2017"
722,Babak Taati,because falls are funny youtube and other video sharing sites contain a large repository of reallife falls we propose extracting gait and balance information from these videos to help us better understand some of the factors that contribute to falls proofofconcept is explored in a single video containing multiple n fallsnonfalls in the presence of an unexpected obstacle the analysis explores computing spatiotemporal parameters of gait in a video captured from an arbitrary viewpoint the relationship between parameters of gait from the last few steps before the obstacle and falling vs not falling and the predictive capacity of a multivariate model in predicting a fall in the presence of an unexpected obstacle homography transformations correct the perspective projection distortion and allow for the consistent tracking of gait parameters as an individual walks in an arbitrary direction in the scene a synthetic top view allows for computing the average stride length and a synthetic side view allows for measuring up and down motions of the head in leaveoneout crossvalidation we were able to correctly predict whether a person would fall or not in out of the cases just by looking at the average stride length and the range of vertical head motion during the most recent steps prior to reaching the obstacle less,"Video Analysis of ""YouTube Funnies"" to Aid the Study of Human Gait and Falls - Preliminary Results and Proof of Concept","26 October, 2016"
723,Khai Truong,deaf and hardofhearing dhh audiences have long complained about caption qualities for many online videos created by individual content creators on videosharing platforms eg youtube however there lack explorations of practices challenges and perceptions of online video captions from the perspectives of both individual content creators and dhh audiences in this work we first explore dhh audiences feedback on and reactions to youtube video captions through interviews with dhh individuals and uncover dhh audiences experiences challenges and perceptions on watching videos created by individual content creators eg manually added caption tags could create additional confidence and trust in caption qualities for dhh audiences we then discover individual content creators practices challenges and perceptions on captioning their videos eg backcaptioning problems by conducting a youtube video analysis with captioningrelated youtube videos followed by a survey with individual content creators overall our findings provide an indepth understanding of captions generated by individual content creators and bridge the knowledge gap mutually between content creators and dhh audiences on captions less,An Exploration of Captioning Practices and Challenges of Individual Content Creators on YouTube for People with Hearing Impairments,"26 January, 2022"
724,Khai Truong,"AAC research has traditionally focused on input speed, leaving higher-level communication goals such as relational maintenance under-explored. Through semi-structured interviews with AAC users with motor and speech impairments and their primary family caregivers, we offer a nuanced understanding of AAC’s roles in maintaining close relationships. Our inductive analysis reveals emerging themes including how AAC users and their partners share the physical and mental workload to overcome communication barriers in complex situations. Our deductive application of the Relational Maintenance Strategies framework exposes the efforts made and the challenges encountered in managing social engagements, providing mutual support, and decoding implicit expressions. From these insights, we propose novel research directions for better supporting maintenance strategies and social purposes of ",Designing for Relational Maintenance: New Directions for AAC Research,2022-04-29 00:00:00
725,Khai Truong,"Deaf and Hard-of-Hearing (DHH) audiences have long complained about caption qualities for many online videos created by individual content creators on video-sharing platforms (e.g., YouTube). However, there lack explorations of practices, challenges, and perceptions of online video captions from the perspectives of both individual content creators and DHH audiences. In this work, we first explore DHH audiences' feedback on and reactions to YouTube video captions through interviews with 13 DHH individuals, and uncover DHH audiences' experiences, challenges, and perceptions on watching videos created by individual content creators (e.g., manually added caption tags could create additional confidence and trust in caption qualities for DHH audiences). We then discover individual content creators' practices, challenges, and perceptions on captioning their videos (e.g., back-captioning problems) by conducting a YouTube video analysis with 189 captioning-related YouTube videos, followed by a survey with 62 individual content creators. Overall, our findings provide an in-depth understanding of captions generated by individual content creators and bridge the knowledge gap mutually between content creators and DHH audiences on captions.",An Exploration of Captioning Practices and Challenges of Individual Content Creators on YouTube for People with Hearing Impairments,2022-01-26 00:00:00
726,Khai Truong,"People with chronic obstructive pulmonary disease (COPD) experience dyspnea and dyspnea-related distress and anxiety (DDA) upon physical exertion. When performing supervised exercise, measurement of physiological data, such as heart rate (HR) and blood oxygen saturation (O2sat), are commonly used for safety, but the impacts of such monitoring on their perceptions and behaviour have not previously been studied. This paper investigates the effect of presenting live physiological data to people with COPD during exercise with a focus on its impact on perceptions of dyspnea intensity (DI) and DDA. Informed by formative interviews with 15 people with COPD, we design VIDDE, an exercise companion tool visualizing live data from a pulse oximeter, and evaluate its effect on DI and DDA through case studies involving 3 participants with COPD exercising at their homes. We also conducted design probe",VIDDE: Visualizations for Helping People with COPD Interpret Dyspnea During Exercise,2021-10-17 00:00:00
727,Khai Truong,"Chronic pain is often an ongoing challenge for patients to track and collect data. Pain-O-Vision is a smartwatch enabled pain management system that uses computer vision to capture the details of painful events from the user. A natural reaction to pain is to clench ones fist. The embedded camera is used to capture different types of fist clenching, to represent different levels of pain. An initial prototype was built on an Android smartwatch that uses a cloud-based classification service to detect the fist clench gestures. Our results show that it is possible to map a fist clench to different levels of pain which allows the patient to record the intensity of a painful event without carrying a specialized pain management device.","Pain-o-vision, effortless pain management",2021-06-24 00:00:00
728,Khai Truong,"Recently, digital scribe systems have been gaining popularity as a possible work-around solution to the Electronic Medical Record (EMR) documentation burden that affects many physicians. The proposed system would automate the clinical summary physicians take by capturing and extracting the patient-physician conversation during the consultation. While promising in concept, how this system would apply to real-world use and its limitations are still not well understood. To examine these issues, we designed a digital scribe prototype to generate notes of different qualities ranging from the reality of current state-of-the-art technology to the potential of future implementations. We conducted a” Wizard of Oz” study with 24 primary care physicians using our digital scribe prototype in 4 simulated medical encounters followed by a semi-structured interview. This exploratory study provides an understanding of physicians",Automating Clinical Documentation with Digital Scribes: Understanding the Impact on Physicians,2021-05-06 00:00:00
729,Khai Truong,"Despite the potential benefits of assistive technologies (ATs) for people with various disabilities, only around 7% of Chinese with disabilities have had an opportunity to use ATs. Even for those who have used ATs, the abandonment rate was high. Although China has the world’s largest population with disabilities, prior research exploring how ATs are used and perceived, and why ATs are abandoned have been conducted primarily in North America and Europe. In this paper, we present an interview study conducted in China with 26 people with various disabilities to understand their practices, challenges, perceptions, and misperceptions of using ATs. From the study, we learned about factors that influence AT adoption practices (eg, misuse of accessible infrastructure, issues with replicating existing commercial ATs), challenges using ATs in social interactions (eg, Chinese stigma), and misperceptions about ATs ",“I Choose Assistive Devices That Save My Face” A Study on Perceptions of Accessibility and Assistive Technology Use Conducted in China,2021-05-06 00:00:00
730,Khai Truong,"Chronic pain is often an ongoing challenge for patients to track and collect data. Pain-O-Vision is a smartwatch enabled pain management system that uses computer vision to capture the details of painful events from the user. A natural reaction to pain is to clench ones fist. The embedded camera is used to capture different types of fist clenching, to represent different levels of pain. An initial prototype was built on an Android smartwatch that uses a cloud-based classification service to detect the fist clench gestures. Our results show that it is possible to map a fist clench to different levels of pain which allows the patient to record the intensity of a painful event without carrying a specialized pain management device.","Poster: Pain-O-Vision, Effortless Pain Management",2021
731,Khai Truong,"Chronic Obstructive Pulmonary Disease (COPD) is a terminal, progressive lung condition which mainly affects older adults. The onset of symptoms, obstacles, and impairments brought about by COPD often necessitates a grieving process for patients. Acceptance is the stage of the grieving process in which the patient has healthily integrated the condition into his or her lifestyle and identity. Because of the progressive nature of COPD, the process of acceptance is a perpetual journey in which patients must continuously shift their mindsets and lifestyles to adapt to the increasing severity of the condition. Using the health belief model as a theoretical foundation, we explore the usage of daily automated SMS messages as an engaging and accessible means of facilitating and maintaining a patient's acceptance of COPD. The results of our investigation show that SMS messages serve as an effective tool for improving ",Evaluating the Gradual Delivery of Knowledge-focused and Mindset-focused Messages for Facilitating the Acceptance of COPD,2020-12-17 00:00:00
732,Khai Truong,"Persons with dementia and their care partners have been found to adapt their own technological arrangements using commercially available information and communication technologies (ICTs). Yet, little is known about these processes of technology appropriation and how care practices are impacted. Adopting a relational perspective of care, we longitudinally examined how four family care networks appropriated a new commercial ICT service into their existing technological arrangements and care practices. Cross-case analysis interpreted collaborative appropriation to encompass two interrelated processes of creating and adapting technological practices and negotiating and augmenting care relationships. Four driving forces were also proposed: motivating meanings that actors ascribe to the technology and its use; the learnability of the technology and actors’ resourcefulness; the establishment of responsive ",Exploring how persons with dementia and care partners collaboratively appropriate information and communication technologies,2020-11-24 00:00:00
733,Khai Truong,"Although gaze has been widely studied for mobile interactions, eyelid-based gestures are relatively understudied and limited to few basic gestures (eg, blink). In this work, we propose a gesture grammar to construct both basic and compound eyelid gestures. We present an algorithm to detect nine eyelid gestures in real-time on mobile devices and evaluate its performance with 12 participants. Results show that our algorithm is able to recognize nine eyelid gestures with 83% and 78% average accuracy using user-dependent and user-independent models respectively. Further, we design a gesture mapping scheme to allow for navigating between and within mobile apps only using eyelid gestures. Moreover, we show how eyelid gestures can be used to enable cross-application and sensitive interactions. Finally, we highlight future research directions",iWink: Exploring eyelid gestures on mobile devices,2020-10-12 00:00:00
734,Khai Truong,"Think-aloud protocols are a highly valued usability testing method for identifying usability problems. Despite the value of conducting think-aloud usability test sessions, analyzing think-aloud sessions is often time-consuming and labor-intensive. Consequently, previous research has urged the community to develop techniques to support fast-paced analysis. In this work, we took the first step to design and evaluate machine learning (ML) models to automatically detect usability problem encounters based on users’ verbalization and speech features in think-aloud sessions. Inspired by recent research that shows subtle patterns in users’ verbalizations and speech features tend to occur when they encounter problems, we examined whether these patterns can be utilized to improve the automatic detection of usability problems. We first conducted and recorded think-aloud sessions and then examined the effect of different",Automatic detection of usability problem encounters in think-aloud sessions,2020-05-30 00:00:00
735,Khai Truong,"Think-aloud protocols are one of the classic methods often taught in universities for training UX designers and researchers. Although previous research reported how these protocols were used in industry, the findings were typically based on the practices of a small number of professionals in specific geographic regions or on studies conducted years ago. As UX practices continuously evolve to address new challenges emerging in industry, it is important to understand the challenges faced by current UX practitioners around the world when using think-aloud protocols. Such an understanding is beneficial for UX professionals to reflect on and learn from the UX community’s practices. It is also invaluable for academic researchers and educators to understand the challenges faced by professionals when carrying out the protocols in a wide range of practical contexts and to better explore methods to address these challenges. We conducted an international survey study with UX professionals in various sized companies around the world. We found that think-aloud protocols are widely and almost equally used in controlled lab studies and remote usability testing; concurrent protocols are more popular than retrospective protocols. Most UX practitioners probe participants during test sessions, explicitly request them to verbalize particular types of content, and do not administer practice sessions. The findings also offer insights on practices and challenges in analyzing think-aloud sessions. In sum, UX practitioners often deal with the tension between validity and efficiency in their analysis and demand better fast-paced and reliable analysis methods than ",Practices and Challenges of Using Think-Aloud Protocols in Industry: An International Survey.,2020-02-01 00:00:00
736,Khai Truong,"Older adults sometimes forget about whether or not they have completed routine actions and the states of objects that they have interacted with (e.g., the kitchen stove is on or off). In this work, we explore whether video clips captured from a body-worn camera every time objects of interest are found within its field of view can help older adults determine if they have completed certain actions with these objects and what their states are. We designed FMT (""Fiducial Marker Tracker"")---a real-time capture and access application that opportunistically captures video clips of objects the user interacts with. To do this, the user places fiducial markers close to objects which would be captured when the marker enters the user's body-worn camera's field of view. We examine and discuss what objects this system would be best suited to track, and the usefulness and usability of this approach. FMT successfully captured direct",FMT: A wearable camera-based object tracking memory aid for older adults,2019-09-09 00:00:00
737,Khai Truong,"Although there are many face recognition systems to help individuals with visual impairments (VIPs) recognize other people, almost all require a database with the pictures and names of the people who should be tracked. These solutions would not be able to help VIPs recognize people they might not know well. In this work, we investigate the requirements and challenges that must be addressed in the design of a face recognition system for helping VIPs recognize people with whom they have weak-ties. We first conducted a formative study with eight visually impaired people. Using insights learned from the formative study, we developed a research prototype that runs on a mobile phone worn around the user's neck. The developed prototype is a wearable face recognition system that opportunistically captures and stores undistorted face images and contextual information about the user's interaction with each ",Face recognition assistant for people with visual impairments,2019-09-09 00:00:00
738,Khai Truong,"The concurrent think-aloud protocol—in which participants verbalize their thoughts when performing tasks—is a widely employed approach in usability testing. Despite its value, analyzing think-aloud sessions can be onerous because it often entails assessing all of a user's verbalizations. This has motivated previous research on developing categories to segment verbalizations into manageable units of analysis. However, the way in which a category might relate to usability problems is currently unclear. In this research, we sought to address this gap in our understanding. We also studied how speech features might relate to usability problems. Through two studies, this research demonstrates that certain patterns of verbalizations are more telling of usability problems than others and that these patterns are robust to different types of test products (i.e., physical devices and digital systems), access to different types of",Concurrent think-aloud verbalizations and usability problems,2019-07-19 00:00:00
739,Khai Truong,in this study we aim to identify moments of rudeness between two individuals in particular we segment all occurrences of rudeness in conversations into three broad distinct categories and try to identify each we show how machine learning algorithms can be used to identify rudeness based on acoustic and semantic signals extracted from conversations furthermore we make note of our shortcomings in this task and highlight what makes this problem inherently difficult finally we provide next steps which are needed to ensure further success in identifying rudeness in conversations less,On the Challenges of Detecting Rude Conversational Behaviour,"28 December, 2017"
740,Raquel Urtasun,recovering the spatial layout of the cameras and the geometry of the scene from extremeview images is a longstanding challenge in computer vision prevailing d reconstruction algorithms often adopt the image matching paradigm and presume that a portion of the scene is covisible across images yielding poor performance when there is little overlap among inputs in contrast humans can associate visible parts in one image to the corresponding invisible components in another image via prior knowledge of the shapes inspired by this fact we present a novel concept called virtual correspondences vcs vcs are a pair of pixels from two images whose camera rays intersect in d similar to classic correspondences vcs conform with epipolar geometry unlike classic correspondences vcs do not need to be covisible across views therefore vcs can be established and exploited even if images do not overlap we introduce a method to find virtual correspondences based on humans in the scene we showcase how vcs can be seamlessly integrated with classic bundle adjustment to recover camera poses across extreme views experiments show that our method significantly outperforms stateoftheart camera pose estimation methods in challenging scenarios and is comparable in the traditional densely captured setup our approach also unleashes the potential of multiple downstream tasks such as scene reconstruction from multiview stereo and novel view synthesis in extremeview scenarios less,Virtual Correspondence: Humans as a Cue for Extreme-View Geometry,"16 June, 2022"
741,Raquel Urtasun,modern selfdriving perception systems have been shown to improve upon processing complementary inputs such as lidar with images in isolation d images have been found to be extremely vulnerable to adversarial attacks yet there have been limited studies on the adversarial robustness of multimodal models that fuse lidar features with image features furthermore existing works do not consider physically realizable perturbations that are consistent across the input modalities in this paper we showcase practical susceptibilities of multisensor detection by placing an adversarial object on top of a host vehicle we focus on physically realizable and inputagnostic attacks as they are feasible to execute in practice and show that a single universal adversary can hide different host vehicles from stateoftheart multimodal detectors our experiments demonstrate that successful attacks are primarily caused by easily corrupted image features furthermore we find that in modern sensor fusion methods which project image features into d adversarial attacks can exploit the projection process to generate false positives across distant regions in d towards more robust multimodal perception systems we show that adversarial training with feature denoising can boost robustness to such attacks significantly however we find that standard adversarial defenses still struggle to prevent false positives which are also caused by inaccurate associations between d lidar points and d pixels less,Exploring Adversarial Robustness of Multi-Sensor Perception Systems in Self Driving,"7 January, 2022"
742,Raquel Urtasun,growing at a fast pace modern autonomous systems will soon be deployed at scale opening up the possibility for cooperative multiagent systems sharing information and distributing workloads allow autonomous agents to better perform tasks and increase computation efficiency however shared information can be modified to execute adversarial attacks on deep learning models that are widely employed in modern systems thus we aim to study the robustness of such systems and focus on exploring adversarial attacks in a novel multiagent setting where communication is done through sharing learned intermediate representations of neural networks we observe that an indistinguishable adversarial message can severely degrade performance but becomes weaker as the number of benign agents increases furthermore we show that blackbox transfer attacks are more difficult in this setting when compared to directly perturbing the inputs as it is necessary to align the distribution of learned representations with domain adaptation our work studies robustness at the neural network level to contribute an additional layer of fault tolerance to modern security protocols for more secure multiagent systems less,Adversarial Attacks On Multi-Agent Communication,"12 October, 2021"
743,Raquel Urtasun,d object detection is a key component of many robotic applications such as selfdriving vehicles while many approaches rely on expensive d sensors such as lidar to produce accurate d estimates methods that exploit stereo cameras have recently shown promising results at a lower cost existing approaches tackle this problem in two steps first depth estimation from stereo images is performed to produce a pseudo lidar point cloud which is then used as input to a d object detector however this approach is suboptimal due to the representation mismatch as the two tasks are optimized in two different metric spaces in this paper we propose a model that unifies these two tasks and performs them in the same metric space specifically we directly construct a pseudo lidar feature volume plume in d space which is then used to solve both depth estimation and object detection tasks our approach achieves stateoftheart performance with much faster inference times when compared to existing methods on the challenging kitti benchmark less,PLUMENet: Efficient 3D Object Detection from Stereo Images,"31 July, 2021"
744,Raquel Urtasun,in this paper we present a nonparametric structured latent variable model for image generation called npdraw which sequentially draws on a latent canvas in a partbypart fashion and then decodes the image from the canvas our key contributions are as follows we propose a nonparametric prior distribution over the appearance of image parts so that the latent variable whattodraw per step becomes a categorical random variable this improves the expressiveness and greatly eases the learning compared to gaussians used in the literature we model the sequential dependency structure of parts via a transformer which is more powerful and easier to train compared to rnns used in the literature we propose an effective heuristic parsing algorithm to pretrain the prior experiments on mnist omniglot cifar and celeba show that our method significantly outperforms previous structured image models like draw and air and is competitive to other generic generative models moreover we show that our models inherent compositionality and interpretability bring significant benefits in the lowdata learning regime and latent space editing code is available at httpsgithubcomzengxhnpdraw less,NP-DRAW: A Non-Parametric Structured Latent Variable Model for Image Generation,"4 July, 2021"
745,Raquel Urtasun,in this paper we present lookout a novel autonomy system that perceives the environment predicts a diverse set of futures of how the scene might unroll and estimates the trajectory of the sdv by optimizing a set of contingency plans over these future realizations in particular we learn a diverse joint distribution over multiagent future trajectories in a traffic scene that covers a wide range of future modes with high sample efficiency while leveraging the expressive power of generative models unlike previous work in diverse motion forecasting our diversity objective explicitly rewards sampling future scenarios that require distinct reactions from the selfdriving vehicle for improved safety our contingency planner then finds comfortable and nonconservative trajectories that ensure safe reactions to a wide range of future scenarios through extensive evaluations we show that our model demonstrates significantly more diverse and sampleefficient motion forecasting in a largescale selfdriving dataset as well as safer and lessconservative motion plans in longterm closedloop simulations when compared to current stateoftheart models less,LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving,"7 May, 2021"
746,Raquel Urtasun,over the last few years we have witnessed tremendous progress on many subtasks of autonomous driving including perception motion forecasting and motion planning however these systems often assume that the car is accurately localized against a highdefinition map in this paper we question this assumption and investigate the issues that arise in stateoftheart autonomy stacks under localization error based on our observations we design a system that jointly performs perception prediction and localization our architecture is able to reuse computation between both tasks and is thus able to correct localization errors efficiently we show experiments on a largescale autonomy dataset demonstrating the efficiency and accuracy of our proposed approach less,"Deep Multi-Task Learning for Joint Localization, Perception, and Prediction","10 April, 2021"
747,Raquel Urtasun,selfdriving vehicles must perceive and predict the future positions of nearby actors in order to avoid collisions and drive safely a learned deep learning module is often responsible for this task requiring largescale highquality training datasets as data collection is often significantly cheaper than labeling in this domain the decision of which subset of examples to label can have a profound impact on model performance active learning techniques which leverage the state of the current model to iteratively select examples for labeling offer a promising solution to this problem however despite the appeal of this approach there has been little scientific analysis of active learning approaches for the perception and prediction pp problem in this work we study active learning techniques for pp and find that the traditional active learning formulation is illsuited for the pp setting we thus introduce generalizations that ensure that our approach is both costaware and allows for finegrained selection of examples through partially labeled scenes our experiments on a realworld largescale selfdriving dataset suggest that finegrained selection can improve the performance across perception prediction and downstream planning tasks less,Just Label What You Need: Fine-Grained Active Selection for Perception and Prediction through Partially Labeled Scenes,"8 April, 2021"
748,Raquel Urtasun,in the past few years we have seen great advances in object perception particularly in d spacetime dimensions thanks to deep learning methods however they typically rely on large amounts of highquality labels to achieve good performance which often require timeconsuming and expensive work by human annotators to address this we propose an automatic annotation pipeline that generates accurate object trajectories in d space ie d labels from lidar point clouds the key idea is to decompose the d object label into two parts the object size in d thats fixed through time for rigid objects and the motion path describing the evolution of the objects pose through time instead of generating a series of labels in one shot we adopt an iterative refinement process where online generated object detections are tracked through time as the initialization given the cheap but noisy input our model produces higher quality d labels by reestimating the object size and smoothing the motion path where the improvement is achieved by exploiting aggregated observations and motion cues over the entire trajectory we validate the proposed method on a largescale driving dataset and show a reduction of human annotation efforts we also showcase the benefits of our approach in the annotatorintheloop setting less,Auto4D: Learning to Label 4D Objects from Sequential Point Clouds,"11 March, 2021"
749,Raquel Urtasun,we present an efficient effective and generic approach towards solving inverse problems the key idea is to leverage the feedback signal provided by the forward process and learn an iterative update model specifically at each iteration the neural network takes the feedback as input and outputs an update on the current estimation our approach does not have any restrictions on the forward process it does not require any prior knowledge either through the feedback information our model not only can produce accurate estimations that are coherent to the input observation but also is capable of recovering from early incorrect predictions we verify the performance of our approach over a wide range of inverse problems including dof pose estimation illumination estimation as well as inverse kinematics comparing to traditional optimizationbased methods we can achieve comparable or better performance while being two to three orders of magnitude faster compared to deep learningbased approaches our model consistently improves the performance on all metrics please refer to the project page for videos animations supplementary materials etc less,Deep Feedback Inverse Problem Solver,"19 January, 2021"
750,Raquel Urtasun,highdefinition maps hd maps are a key component of most modern selfdriving systems due to their valuable semantic and geometric information unfortunately building hd maps has proven hard to scale due to their cost as well as the requirements they impose in the localization system that has to work everywhere with centimeterlevel accuracy being able to drive without an hd map would be very beneficial to scale selfdriving solutions as well as to increase the failure tolerance of existing ones eg if localization fails or the map is not uptodate towards this goal we propose mp an endtoend approach to mapless driving where the input is raw sensor data and a highlevel command eg turn left at the intersection mp predicts intermediate representations in the form of an online map and the current and future state of dynamic agents and exploits them in a novel neural motion planner to make interpretable decisions taking into account uncertainty we show that our approach is significantly safer more comfortable and can follow commands better than the baselines in challenging longterm closedloop simulations as well as when compared to an expert driver in a largescale realworld dataset less,"MP3: A Unified Model to Map, Perceive, Predict and Plan","17 January, 2021"
751,Raquel Urtasun,in this paper we propose a neural motion planner nmp for learning to drive autonomously in complex urban scenarios that include trafficlight handling yielding and interactions with multiple roadusers towards this goal we design a holistic model that takes as input raw lidar data and a hd map and produces interpretable intermediate representations in the form of d detections and their future trajectories as well as a cost volume defining the goodness of each position that the selfdriving car can take within the planning horizon we then sample a set of diverse physically possible trajectories and choose the one with the minimum learned cost importantly our cost volume is able to naturally capture multimodality we demonstrate the effectiveness of our approach in realworld driving data captured in several cities in north america our experiments show that the learned cost volume can generate safer planning than all the baselines less,End-to-end Interpretable Neural Motion Planner,"17 January, 2021"
752,Raquel Urtasun,network pruning can significantly reduce the computation and memory footprint of large neural networks to achieve a good tradeoff between model size and performance popular pruning techniques usually rely on handcrafted heuristics and require manually setting the compression ratio for each layer this process is typically timeconsuming and requires expert knowledge to achieve good results in this paper we propose nap a unified and automatic pruning framework for both finegrained and structured pruning it can find out unimportant components of a network and automatically decide appropriate compression ratios for different layers based on a theoretically sound criterion towards this goal nap uses an efficient approximation of the hessian for evaluating the importances of components based on a kroneckerfactored approximate curvature method despite its simpleness to use nap outperforms previous pruning methods by large margins for finegrained pruning nap can compress alexnet and vgg by x and resnet by x without loss in accuracy on imagenet for structured pruning eg channel pruning it can reduce flops of vgg by x and resnet by x with only accuracy drop more importantly this method is almost free from hyperparameter tuning and requires no expert knowledge you can start nap and then take a nap less,Network Automatic Pruning: Start NAP and Take a Nap,"17 January, 2021"
753,Raquel Urtasun,constructing and animating humans is an important component for building virtual worlds in a wide variety of applications such as virtual reality or robotics testing in simulation as there are exponentially many variations of humans with different shape pose and clothing it is critical to develop methods that can automatically reconstruct and animate humans at scale from real world data towards this goal we represent the pedestrians shape pose and skinning weights as neural implicit functions that are directly learned from data this representation enables us to handle a wide variety of different pedestrian shapes and poses without explicitly fitting a human parametric body model allowing us to handle a wider range of human geometries and topologies we demonstrate the effectiveness of our approach on various datasets and show that our reconstructions outperform existing stateoftheart methods furthermore our reanimation experiments show that we can generate d human animations at scale from a single rgb image andor an optional lidar sweep as input less,"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling","16 January, 2021"
754,Raquel Urtasun,modern selfdriving autonomy systems heavily rely on deep learning as a consequence their performance is influenced significantly by the quality and richness of the training data data collecting platforms can generate many hours of raw data in a daily basis however it is not feasible to label everything it is thus of key importance to have a mechanism to identify what to label active learning approaches identify examples to label but their interestingness is tied to a fixed model performing a particular task these assumptions are not valid in selfdriving where we have to solve a diverse set of tasks ie perception and motion forecasting and our models evolve over time frequently in this paper we introduce a novel approach and propose a new data selection method that exploits a diverse set of criteria that quantize interestingness of traffic scenes our experiments on a wide range of tasks and models show that the proposed curation pipeline is able to select datasets that lead to better generalization and higher performance less,Diverse Complexity Measures for Dataset Curation in Self-driving,"16 January, 2021"
755,Raquel Urtasun,we consider the problem of generating realistic traffic scenes automatically existing methods typically insert actors into the scene according to a set of handcrafted heuristics and are limited in their ability to model the true complexity and diversity of real traffic scenes thus inducing a content gap between synthesized traffic scenes versus real ones as a result existing simulators lack the fidelity necessary to train and test selfdriving vehicles to address this limitation we present scenegen a neural autoregressive model of traffic scenes that eschews the need for rules and heuristics in particular given the egovehicle state and a high definition map of surrounding area scenegen inserts actors of various classes into the scene and synthesizes their sizes orientations and velocities we demonstrate on two largescale datasets scenegens ability to faithfully model distributions of real traffic scenes moreover we show that scenegen coupled with sensor simulation can be used to train perception models that generalize to the real world less,SceneGen: Learning to Generate Realistic Traffic Scenes,"16 January, 2021"
756,Raquel Urtasun,in this paper we address the problem of detecting crosswalks from lidar and camera imagery towards this goal given multiple lidar sweeps and the corresponding imagery we project both inputs onto the ground surface to produce a top down view of the scene we then leverage convolutional neural networks to extract semantic cues about the location of the crosswalks these are then used in combination with road centerlines from freely available maps eg openstreetmaps to solve a structured optimization problem which draws the final crosswalk boundaries our experiments over crosswalks in a large city area show that automation can be achieved less,End-to-End Deep Structured Models for Drawing Crosswalks,"14 January, 2021"
757,Raquel Urtasun,we present a novel compression algorithm for reducing the storage of lidar sensor data streams our model exploits spatiotemporal relationships across multiple lidar sweeps to reduce the bitrate of both geometry and intensity values towards this goal we propose a novel conditional entropy model that models the probabilities of the octree symbols by considering both coarse level geometry and previous sweeps geometric and intensity information we then use the learned probability to encode the full data stream into a compact one our experiments demonstrate that our method significantly reduces the joint geometry and intensity bitrate over prior stateoftheart lidar compression methods with a reduction of and on the urbancity and semantickitti datasets respectively less,MuSCLE: Multi Sweep Compression of LiDAR using Deep Entropy Models,"8 January, 2021"
758,Raquel Urtasun,we are interested in understanding whether retrievalbased localization approaches are good enough in the context of selfdriving vehicles towards this goal we introduce pitm a new image and lidar dataset with over million frames which is to times larger than those used in previous work pitm is captured under diverse conditions ie season weather time of the day traffic and provides accurate localization ground truth we also automatically annotate our dataset with historical weather and astronomical data as well as with image and lidar semantic segmentation as a proxy measure for occlusion we benchmark multiple existing methods for image and lidar retrieval and in the process introduce a simple yet effective convolutional networkbased lidar retrieval method that is competitive with the state of the art our work provides for the first time a benchmark for submetre retrievalbased localization at city scale the dataset additional experimental results as well as more information about the sensors calibration and metadata are available on the project website httpsubercomatgdatasetspitm less,Pit30M: A Benchmark for Global Localization in the Age of Self-Driving Cars,"22 December, 2020"
759,Raquel Urtasun,in this paper we propose to exploit multiple related tasks for accurate multisensor d object detection towards this goal we present an endtoend learnable architecture that reasons about d and d object detection as well as ground estimation and depth completion our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels importantly our approach leads the kitti benchmark on d d and bev object detection while being real time less,Multi-Task Multi-Sensor Fusion for 3D Object Detection,"22 December, 2020"
760,Raquel Urtasun,one of the fundamental challenges to scale selfdriving is being able to create accurate high definition maps hd maps with low cost current attempts to automate this process typically focus on simple scenarios estimate independent maps per frame or do not have the level of precision required by modern self driving vehicles in contrast in this paper we focus on drawing the lane boundaries of complex highways with many lanes that contain topology changes due to forks and merges towards this goal we formulate the problem as inference in a directed acyclic graphical model dag where the nodes of the graph encode geometric and topological properties of the local regions of the lane boundaries since we do not know a priori the topology of the lanes we also infer the dag topology ie nodes and edges for each region we demonstrate the effectiveness of our approach on two major north american highways in two different states and show high precision and recall as well as correct topology less,DAGMapper: Learning to Map by Discovering Lane Topology,"22 December, 2020"
761,Raquel Urtasun,in this paper we show that highdefinition hd maps provide strong priors that can boost the performance and robustness of modern d object detectors towards this goal we design a single stage detector that extracts geometric and semantic features from the hd maps as maps might not be available everywhere we also propose a map prediction module that estimates the map on the fly from raw lidar data we conduct extensive experiments on kitti as well as a largescale d detection benchmark containing million frames and show that the proposed mapaware detector consistently outperforms the stateoftheart in both mapped and unmapped scenarios importantly the whole framework runs at frames per second less,HDNET: Exploiting HD Maps for 3D Object Detection,"21 December, 2020"
762,Raquel Urtasun,in this paper we propose a novel d object detector that can exploit both lidar as well as cameras to perform very accurate localization towards this goal we design an endtoend learnable architecture that exploits continuous convolutions to fuse image and lidar feature maps at different levels of resolution our proposed continuous fusion layer encode both discretestate image features as well as continuous geometric information this enables us to design a novel reliable and efficient endtoend learnable d object detector based on multiple sensors our experimental evaluation on both kitti as well as a large scale d object detection benchmark shows significant improvements over the state of the art less,Deep Continuous Fusion for Multi-Sensor 3D Object Detection,"20 December, 2020"
763,Raquel Urtasun,in this paper we propose a realtime calibrationagnostic and effective localization system for selfdriving cars our method learns to embed the online lidar sweeps and intensity map into a joint deep embedding space localization is then conducted through an efficient convolutional matching between the embeddings our full system can operate in realtime at hz while achieving centimeter level accuracy across different lidar sensors and environments our experiments illustrate the performance of the proposed approach over a largescale dataset consisting of over km of driving less,Learning to Localize Using a LiDAR Intensity Map,"20 December, 2020"
764,Raquel Urtasun,in this paper we derive generalization bounds for the two primary classes of graph neural networks gnns namely graph convolutional networks gcns and message passing gnns mpgnns via a pacbayesian approach our result reveals that the maximum node degree and spectral norm of the weights govern the generalization bounds of both models we also show that our bound for gcns is a natural generalization of the results developed in arxivv cslg for fullyconnected and convolutional neural networks for message passing gnns our pacbayes bound improves over the rademacher complexity based bound in arxivv cslg showing a tighter dependency on the maximum node degree and the maximum hidden dimension the key ingredients of our proofs are a perturbation analysis of gnns and the generalization of pacbayes analysis to nonhomogeneous gnns we perform an empirical study on several realworld graph datasets and verify that our pacbayes bound is tighter than others less,A PAC-Bayesian Approach to Generalization Bounds for Graph Neural Networks,"14 December, 2020"
765,Nandita Vijaykumar,read mapping is a fundamental yet computationallyexpensive step in many genomics applications it is used to identify potential matches and differences between fragments called reads of a sequenced genome and an already known genome called a reference genome to address the computational challenges in genome analysis many prior works propose various approaches such as filters that select the reads that must undergo expensive computation efficient heuristics and hardware acceleration while effective at reducing the computation overhead all such approaches still require the costly movement of a large amount of data from storage to the rest of the system which can significantly lower the endtoend performance of read mapping in conventional and emerging genomics systems we propose genstore the first instorage processing system designed for genome sequence analysis that greatly reduces both data movement and computational overheads of genome sequence analysis by exploiting lowcost and accurate instorage filters genstore leverages hardwaresoftware codesign to address the challenges of instorage processing supporting reads with different read lengths and error rates and different degrees of genetic variation through rigorous analysis of read mapping processes we meticulously design lowcost hardware accelerators and datacomputation flows inside a nand flashbased ssd our evaluation using a wide range of real genomic datasets shows that genstore when implemented in three modern ssds significantly improves the read mapping performance of stateoftheart software hardware baselines by times times for read sets with high similarity to the reference genome and times times for read sets with low similarity to the reference genome less,GenStore: A High-Performance and Energy-Efficient In-Storage Computing System for Genome Sequence Analysis,"21 February, 2022"
766,Nandita Vijaykumar,implicit neural representations with multilayer perceptrons mlps have recently gained prominence for a wide variety of tasks such as novel view synthesis and d object representation and rendering however a significant challenge with these representations is that both training and inference with an mlp over a large number of input coordinates to learn and represent an image video or d object require large amounts of computation and incur long processing times in this work we aim to accelerate inference and training of coordinatebased mlps for implicit neural representations by proposing a new split mlp architecture coordx with coordx the initial layers are split to learn each dimension of the input coordinates separately the intermediate features are then fused by the last layers to generate the learned signal at the corresponding coordinate point this significantly reduces the amount of computation required and leads to large speedups in training and inference while achieving similar accuracy as the baseline mlp this approach thus aims at first learning functions that are a decomposition of the original signal and then fusing them to generate the learned signal our proposed architecture can be generally used for many implicit neural representation tasks with no additional memory overheads we demonstrate a speedup of up to x compared to the baseline model for image video and d shape representation and rendering tasks less,CoordX: Accelerating Implicit Neural Representation with a Split MLP Architecture,"28 January, 2022"
767,Nandita Vijaykumar,true random number generators trng sample random physical processes to create large amounts of random numbers for various use cases including securitycritical cryptographic primitives scientific simulations machine learning applications and even recreational entertainment unfortunately not every computing system is equipped with dedicated trng hardware limiting the application space and security guarantees for such systems to open the application space and enable security guarantees for the overwhelming majority of computing systems that do not necessarily have dedicated trng hardware we develop quactrng quactrng exploits the new observation that a carefullyengineered sequence of dram commands activates four consecutive dram rows in rapid succession this quadruple activation quac causes the bitline sense amplifiers to nondeterministically converge to random values when we activate four rows that store conflicting data because the net deviation in bitline voltage fails to meet reliable sensing margins we experimentally demonstrate that quac reliably generates random values across commodity ddr dram chips from one major dram manufacturer we describe how to develop an effective trng quactrng based on quac we evaluate the quality of our trng using nist sts and find that quactrng successfully passes each test our experimental evaluations show that quactrng generates true random numbers with a throughput of gbs per dram channel outperforming the stateoftheart drambased trng by x and x for basic and throughputoptimized versions respectively we show that quactrng utilizes dram bandwidth better than the stateoftheart achieving up to x the throughput of a throughputoptimized baseline when scaling bus frequencies to gts less,QUAC-TRNG: High-Throughput True Random Number Generation Using Quadruple Row Activation in Commodity DRAM Chips,"25 May, 2021"
768,Nandita Vijaykumar,there are three domains in a modern thermallyconstrained mobile systemonchip soc compute io and memory we observe that a modern soc typically allocates a fixed power budget corresponding to worstcase performance demands to the io and memory domains even if they are underutilized the resulting unfair allocation of the power budget across domains can cause two major issues the io and memory domains can operate at a higher frequency and voltage than necessary increasing power consumption and the unused power budget of the io and memory domains cannot be used to increase the throughput of the compute domain hampering performance to avoid these issues it is crucial to dynamically orchestrate the distribution of the soc power budget across the three domains based on their actual performance demands we propose sysscale a new multidomain power management technique to improve the energy efficiency of mobile socs sysscale is based on three key ideas first sysscale introduces an accurate algorithm to predict the performance eg bandwidth and latency demands of the three soc domains second sysscale uses a new dvfs dynamic voltage and frequency scaling mechanism to distribute the soc power to each domain according to the predicted performance demands third in addition to using a global dvfs mechanism sysscale uses domainspecialized techniques to optimize the energy efficiency of each domain at different operating points we implement sysscale on an intel skylake microprocessor for mobile devices and evaluate it using a wide variety of spec cpu graphics dmark and battery life workloads eg video playback on a core skylake sysscale improves the performance of spec cpu and dmark workloads by up to and and on average respectively less,SysScale: Exploiting Multi-domain Dynamic Voltage and Frequency Scaling for Energy Efficient Mobile Processors,"18 May, 2020"
769,Nandita Vijaykumar,programmability performance portability and resource efficiency have emerged as critical challenges in harnessing complex and diverse architectures today to obtain high performance and energy efficiency while there is abundant research and thus significant improvements at different levels of the stack that address these very challenges in this thesis we observe that we are fundamentally limited by the interfaces and abstractions between the application and the underlying systemhardwarespecifically the hardwaresoftware interface the existing narrow interfaces pose two critical challenges first significant effort and expertise are required to write highperformance code to harness the full potential of todays diverse and sophisticated hardware second as a hardwaresystem designer architecting faster and more efficient systems is challenging as the vast majority of the programs semantic content gets lost in translation with todays hardwaresoftware interface moving towards the future these challenges in programmability and efficiency will be even more intractable as we architect increasingly heterogeneous and sophisticated systems this thesis makes the case for rich lowoverhead crosslayer abstractions as a highly effective means to address the above challenges these abstractions are designed to communicate higherlevel program information from the application to the underlying system and hardware in a highly efficient manner requiring only minor additions to the existing interfaces in doing so they enable a rich space of hardwaresoftware cooperative mechanisms to optimize for performance we propose different approaches to designing richer abstractions between the application system software and hardware architecture in different contexts to significantly improve programmability portability and performance in cpus and gpus less,"Enhancing Programmability, Portability, and Performance with Rich Cross-Layer Abstractions","10 November, 2019"
770,Nandita Vijaykumar,this paper summarizes the idea of chargecache which was published in hpca and examines the works significance and future potential dram latency continues to be a critical bottleneck for system performance in this work we develop a lowcost mechanism called chargecache that enables faster access to recentlyaccessed rows in dram with no modifications to dram chips our mechanism is based on the key observation that a recentlyaccessed row has more charge and thus the following access to the same row can be performed faster to exploit this observation we propose to track the addresses of recentlyaccessed rows in a table in the memory controller if a later dram request hits in that table the memory controller uses lower timing parameters leading to reduced dram latency row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency we evaluate chargecache on a wide variety of workloads and show that it provides significant performance and energy benefits for both singlecore and multicore systems less,Exploiting Row-Level Temporal Locality in DRAM to Reduce the Memory Access Latency,"8 May, 2018"
771,Nandita Vijaykumar,the application resource specificationa static specification of several parameters such as the number of threads and the scratchpad memory usage per thread blockforms a critical component of modern gpu programming models this specification determines the parallelism and hence performance of the application during execution because the corresponding onchip hardware resources are allocated and managed based on this specification this tightcoupling between the softwareprovided resource specification and resource management in hardware leads to significant challenges in programming ease portability and performance zorua is a new resource virtualization framework that decouples the programmerspecified resource usage of a gpu application from the actual allocation in the onchip hardware resources zorua enables this decoupling by virtualizing each resource transparently to the programmer we demonstrate that by providing the illusion of more resources than physically available via controlled and coordinated virtualization zorua offers several important benefits i programming ease zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available onchip resources ii portability zorua alleviates the necessity of retuning an applications resource usage when porting the application across gpu generations iii performance by dynamically allocating resources and carefully oversubscribing them when necessary zorua improves or retains the performance of applications that are already highly tuned to best utilize the resources less,"Decoupling GPU Programming Models from Resource Management for Enhanced Programming Ease, Portability, and Performance","2 May, 2018"
772,Nandita Vijaykumar,modern graphics processing units gpus are well provisioned to support the concurrent execution of thousands of threads unfortunately different bottlenecks during execution and heterogeneous application requirements create imbalances in utilization of resources in the cores for example when a gpu is bottlenecked by the available offchip memory bandwidth its computational resources are often overwhelmingly idle waiting for data from memory to arrive this work describes the coreassisted bottleneck acceleration caba framework that employs idle onchip resources to alleviate different bottlenecks in gpu execution caba provides flexible mechanisms to automatically generate assist warps that execute on gpu cores to perform specific tasks that can improve gpu performance and efficiency caba enables the use of idle computational units and pipelines to alleviate the memory bandwidth bottleneck eg by using assist warps to perform data compression to transfer less data from memory conversely the same framework can be employed to handle cases where the gpu is bottlenecked by the available computational units in which case the memory pipelines are idle and can be used by caba to speed up computation eg by performing memoization using assist warps we provide a comprehensive design and evaluation of caba to perform effective and flexible data compression in the gpu memory hierarchy to alleviate the memory bandwidth bottleneck our extensive evaluations show that caba when used to implement data compression provides an average performance improvement of as high as x across a variety of memorybandwidthsensitive gpgpu applications less,A Framework for Accelerating Bottlenecks in GPU Execution with Assist Warps,"3 February, 2016"
773,Bo Wang,"Integrative analysis of large-scale single-cell RNA sequencing (scRNA-seq) datasets can aggregate complementary biological information from different datasets. However, most existing methods fail to efficiently integrate multiple large-scale scRNA-seq datasets. We propose OCAT, One Cell At a Time, a machine learning method that sparsely encodes single-cell gene expression to integrate data from multiple sources without highly variable gene selection or explicit batch effect correction. We demonstrate that OCAT efficiently integrates multiple scRNA-seq datasets and achieves the state-of-the-art performance in cell type clustering, especially in challenging scenarios of non-overlapping cell types. In addition, OCAT can efficaciously facilitate a variety of downstream analyses.",One Cell At a Time (OCAT): a unified framework to integrate and analyze single-cell RNA-seq data,2022/12
774,Bo Wang,"Molecular carcinogenicity is a preventable cause of cancer, but systematically identifying carcinogenic compounds, which involves performing experiments on animal models, is expensive, time consuming and low throughput. As a result, carcinogenicity information is limited and building data-driven models with good prediction accuracy remains a major challenge.",A graph neural network approach for molecule carcinogenicity prediction,2022/7
775,Bo Wang,"Stroke is a devastating consequence of plaque rupture from the carotid arteries. Current management of carotid plaques involves waiting for symptoms (eg, stroke or ministroke); intervention itself has risk of stroke and not all plaques are vulnerable to rupture. There is a need to better risk-stratify plaque that causes stroke. Carotid ultrasound (US) examination is a noninvasive and inexpensive visualization of plaques, but is limited by human interpretation. We hypothesize that convolutional neural networks will identify unique features of carotid plaques for automated risk stratification.",Using Deep Convolutional Neural Networks to Automate Classification of Carotid Plaques from Ultrasound Imaging,2022-06-11 00:00:00
776,Bo Wang,"Spatially resolved transcriptomic technologies are promising tools to study complex biological processes such as mammalian embryogenesis. However, the imbalance between resolution, gene capture, and field of view of current methodologies precludes their systematic application to analyze relatively large and three-dimensional mid- and late-gestation embryos. Here, we combined DNA nanoball (DNB)-patterned arrays and in situ RNA capture to create spatial enhanced resolution omics-sequencing (Stereo-seq). We applied Stereo-seq to generate the mouse organogenesis spatiotemporal transcriptomic atlas (MOSTA), which maps with single-cell resolution and high sensitivity the kinetics and directionality of transcriptional variation during mouse organogenesis. We used this information to gain insight into the molecular basis of spatial cell heterogeneity and cell fate specification in developing tissues such as",Spatiotemporal transcriptomic atlas of mouse organogenesis using DNA nanoball-patterned arrays,2022-05-12 00:00:00
777,Bo Wang,"Macrophage colony stimulating factor-1 (CSF-1) plays a critical role in maintaining myeloid lineage cells. However, congenital global deficiency of CSF-1 (Csf1op/op) causes severe musculoskeletal defects that may indirectly affect hematopoiesis. Indeed, we show here that osteolineage-derived Csf1 prevented developmental abnormalities but had no effect on monopoiesis in adulthood. However, ubiquitous deletion of Csf1 conditionally in adulthood decreased monocyte survival, differentiation, and migration, independent of its effects on bone development. Bone histology revealed that monocytes reside near sinusoidal endothelial cells (ECs) and leptin receptor (Lepr)-expressing perivascular mesenchymal stromal cells (MSCs). Targeted deletion of Csf1 from sinusoidal ECs selectively reduced Ly6C− monocytes, whereas combined depletion of Csf1 from ECs and MSCs further decreased Ly6Chi cells.",Colony stimulating factor-1 producing endothelial cells and mesenchymal stromal cells maintain monocytes within a perivascular bone marrow niche,2022-05-10 00:00:00
778,Bo Wang,"Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. ",A sequence-to-sequence approach for document-level relation extraction,2022-04-03 00:00:00
779,Bo Wang,"A total of n=20 clinical EVLP cases were used in this study. Each case was independently assessed by n=15 study participants that included surgeons, surgical fellows, organ perfusion specialists, and EVLP assistants. Each EVLP case was de-identified and presented alongside an intended recipient. Participants were asked to determine the suitability of the lung for transplant (yes/no) based on standard EVLP assessments alone and their impression of the organ on a scale from 0-10. Biomarker scores were then revealed and participants were then asked to re-answer the transplant and lung","The Inclusion of Biomarker Scoring During Ex-Vivo Lung Perfusion Can Improve Organ Utilization and Patient Outcomes-A Retrospective, Single Center Study",2022-04-01 00:00:00
780,Bo Wang,"Critical Covid-19 is caused by immune-mediated inflammatory lung injury. Host genetic variation influences the development of illness requiring critical care1 or hospitalisation2–4 following SARS-CoV-2 infection. The GenOMICC (Genetics of Mortality in Critical Care) study enables the comparison of genomes from critically-ill cases with population controls in order to find underlying disease mechanisms. Here, we use whole genome sequencing in 7,491 critically-ill cases compared with 48,400 controls to discover and replicate 23 independent variants that significantly predispose to critical Covid-19. We identify 16 new independent associations, including variants within genes involved in interferon signalling (IL10RB, PLSCR1), leucocyte differentiation (BCL11A), and blood type antigen secretor status (FUT2). Using transcriptome-wide association and colocalisation to infer the effect of gene expression on disease",Whole genome sequencing reveals host factors underlying critical Covid-19,2022-03-07 00:00:00
781,Bo Wang,"Developments in artificial intelligence (AI) have led to an explosion of studies exploring its application to cardiovascular medicine. Due to the need for training and expertise, one area where AI could be impactful would be in the diagnosis and management of valvular heart disease. This is because AI can be applied to the multitude of data generated from clinical assessments, imaging and biochemical testing during the care of the patient. In the area of valvular heart disease, the focus of AI has been on the echocardiographic assessment and phenotyping of patient populations to identify high-risk groups. AI can assist image acquisition, view identification for review, and segmentation of valve and cardiac structures for automated analysis. Using image recognition algorithms, aortic and mitral valve disease states have been directly detected from the images themselves. Measurements obtained during ",Artificial intelligence for the echocardiographic assessment of valvular heart disease,2022-02-10 00:00:00
782,Bo Wang,"Immunopathology occurs in the lung and spleen in fatal coronavirus disease (COVID-19), involving monocytes/macrophages and plasma cells. Antiinflammatory therapy reduces mortality, but additional therapeutic targets are required. We aimed to gain mechanistic insight into COVID-19 immunopathology by targeted proteomic analysis of pulmonary and splenic tissues. Lung parenchymal and splenic tissue was obtained from 13 postmortem examinations of patients with fatal COVID-19. Control tissue was obtained from cancer resection samples (lung) and deceased organ donors (spleen). Protein was extracted from tissue by phenol extraction. Olink multiplex immunoassay panels were used for protein detection and quantification. Proteins with increased abundance in the lung included MCP-3, antiviral TRIM21, and prothrombotic TYMP. OSM and EN-RAGE/S100A12 abundance was correlated and associated",Tissue proteomic analysis identifies mechanisms and stages of immunopathology in fatal COVID-19,2022/2
783,Bo Wang,"The introduction of RNA velocity in single-cell studies has opened new ways of examining cell differentiation and tissue development. Existing RNA velocity estimation methods are based on strong assumptions of either complete observation of cells in steady states or a predefined dynamics pattern parameterized by constant coefficients. These assumptions are violated in complex and heterogenous single-cell sequencing datasets and thus limit the application of these techniques. Here we present DeepVelo, a novel method that predicts the cell-specific dynamics of splicing kinetics using Graph Convolution Networks (GCNs). DeepVelo generalizes RNA velocity to cell populations containing time-dependent kinetics and multiple lineages, which are common in developmental and pathological systems. We applied DeepVelo to disentangle multifaceted kinetics in the processes of dentate gyrus neurogenesis, pancreatic endocrinogenesis, and hindbrain development. DeepVelo infers time-varying cellular rates of transcription, splicing and degradation, recovers each cell9s stage in the underlying differentiation process and detects putative driver genes regulating these processes. DeepVelo relaxes the constraints of previous techniques and facilitates the study of more complex differentiation and lineage decision events in heterogeneous single-cell RNA sequencing data.",DeepVelo: Deep Learning extends RNA velocity to multi-lineage systems with cell-specific kinetics,2022-01-01 00:00:00
784,Bo Wang,"A common experimental output in biomedical science is a list of genes implicated in a given biological process or disease. The results of a group of studies answering the same, or similar, questions can be combined by meta-analysis to find a consensus or a more reliable answer. Ranking aggregation methods can be used to combine gene lists from various sources in meta-analyses. Evaluating a ranking aggregation method on a specific type of dataset before using it is required to support the reliability of the result since the property of a dataset can influence the performance of an algorithm. Evaluation of aggregation methods is usually based on a simulated database especially for the algorithms designed for gene lists because of the lack of a known truth for real data. However, simulated datasets tend to be too small compared to experimental data and neglect key features, including heterogeneity of quality, relevance and the inclusion of unranked lists. In this study, a group of existing methods and their variations which are suitable for meta-analysis of gene lists are compared using simulated and real data. Simulated data was used to explore the performance of the aggregation methods as a function of emulating the common scenarios of real genomics data, with various heterogeneity of quality, noise level, and a mix of unranked and ranked data using 20000 possible entities. I",Systematic comparison of ranking aggregation methods for gene lists in experimental results,2022-01-01 00:00:00
785,Bo Wang,One of the surgical options available for ischemic mitral regurgitation (MR) is mitral valve repair but is limited by recurrent regurgitation as it is experienced by a significant percentage of patients and has a negative impact on patient outcomes. Efforts to model and identify predictors of recurrent MR rely on complicated echocardiographic and clinical measurements that are subjective and not routinely collected.,Machine learning as a new frontier in mitral valve surgical strategy,2022/1
786,Bo Wang,"The COVID-19 pandemic has highlighted the urgent need for the identification of new antiviral drug therapies for a variety of diseases. COVID-19 is caused by infection with the human coronavirus SARS-CoV-2, while other related human coronaviruses cause diseases ranging from severe respiratory infections to the common cold. We developed a computational approach to identify new antiviral drug targets and repurpose clinically-relevant drug compounds for the treatment of a range of human coronavirus diseases. Our approach is based on graph convolutional networks (GCN) and involves multiscale host-virus interactome analysis coupled to off-target drug predictions. Cell-based experimental assessment reveals several clinically-relevant drug repurposing candidates predicted by the in silico analyses to have antiviral activity against human coronavirus infection. In particular, we identify the MET inhibitor ",Multiscale interactome analysis coupled with off-target drug predictions reveals drug repurposing candidates for human coronavirus disease,2021-12-02 00:00:00
787,Bo Wang,"We studied residents living in LTC homes in Ontario, Canada, who underwent PCR testing for SARS‐CoV‐2 infection from January 1 to August 31, 2020, and examined predictors of all‐cause death within 30 days after a positive test for SARS‐CoV‐2. We examined a broad range of risk factor categories including demographics, comorbidities, functional status, laboratory tests, and characteristics of the LTC facility and surrounding community were examined. In total, 304 potential predictors were evaluated for their association with mortality using machine learning (Random Forest).",Predictors of mortality among long‐term care residents with SARS‐CoV‐2 infection,2021/12
788,Bo Wang,"Single-cell assay for transposase-accessible chromatin sequencing (scATAC-seq) identifies regulated chromatin accessibility modules at the single-cell resolution. Robust evaluation is critical to the development of scATAC-seq pipelines, which calls for reproducible datasets for benchmarking. We hereby present the simATAC framework, an R package that generates scATAC-seq count matrices that highly resemble real scATAC-seq datasets in library size, sparsity, and chromatin accessibility signals. simATAC deploys statistical models derived from analyzing 90 real scATAC-seq cell groups. simATAC provides a robust and systematic approach to generate in silico scATAC-seq samples with known cell labels for assessing analytical pipelines.",simATAC: a single-cell ATAC-seq simulation framework,2021/12
789,Bo Wang,"Inherited genetic factors can influence the severity of COVID-19, but the molecular explanation underpinning a genetic association is often unclear. Intracellular antiviral defenses can inhibit the replication of viruses and reduce disease severity. To better understand the antiviral defenses relevant to COVID-19, we used interferon-stimulated gene (ISG) expression screening to reveal that 2′-5′-oligoadenylate synthetase 1 (OAS1), through ribonuclease L, potently inhibits severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). We show that a common splice-acceptor single-nucleotide polymorphism (Rs10774671) governs whether patients express prenylated OAS1 isoforms that are membrane-associated and sense-specific regions of SARS-CoV-2 RNAs or if they only express cytosolic, nonprenylated OAS1 that does not efficiently detect SARS-CoV-2. In hospitalized patients, expression of prenylated",A prenylated dsRNA sensor protects against severe COVID-19,2021-09-28 00:00:00
790,Daniel Wigdor,despite gaining traction in north america live streaming has not reached the popularity it has in china where livestreaming has a tremendous impact on the social behaviors of users to better understand this sociotechnological phenomenon we conducted a mixed methods study of live streaming practices in china we present the results of an online survey of live streaming users focusing on their broadcasting or viewing practices and the experiences they find most engaging we also interviewed active users to explore their motivations and experiences our data revealed the different categories of content that was broadcasted and how varying aspects of this content engaged viewers we also gained insight into the role reward systems and fan groupchat play in engaging users while also finding evidence that both viewers and streamers desire deeper channels and mechanisms for interaction in addition to the commenting gifting and fan groups that are available today less,"You Watch, You Give, and You Engage: A Study of Live Streaming Practices in China","15 March, 2018"
791,Daniel Wigdor,"Gestural interaction has evolved from a set of novel interaction techniques developed in research labs, to a dominant interaction modality used by millions of users everyday. Despite its widespread adoption, the design of appropriate gesture vocabularies remains a challenging task for developers and designers. Existing research has largely used Expert-Led, User-Led, or Computationally-Based methodologies to design gesture vocabularies. These methodologies leverage the expertise, experience, and capabilities of experts, users, and systems to fulfill different requirements. In practice, however, none of these methodologies provide designers with a complete, multi-faceted perspective of the many factors that influence the design of gesture vocabularies, largely because a singular set of factors has yet to be established. Additionally, these methodologies do not identify or emphasize the subset of factors that are",Iteratively Designing Gesture Vocabularies: A Survey and Analysis of Best Practices in the HCI Literature,2022-05-05 00:00:00
792,Daniel Wigdor,"An electronic device tracks, for a user performing a target acquisition movement within a 3D space, movement parameters of a plurality of input devices of the user. The electronic device predicts, for the user, a region of interest within the 3D space, based on the movement parameters. The region of interest includes a plurality of targets in close proximity. The electronic device predicts an endpoint of the target acquisition movement, within the region of interest. In some embodiments, the plurality of input devices includes an eye tracking input device, each input device corresponds to a predefined input device type, and the movement parameters include gaze data from the eye tracking input device. In some embodiments, input devices includes an eye tracking input device, a head-mounted display, and a hand-held controller, and the user's eye, hand, and head movements are coordinated.",Multimodal Kinematic Template Matching and Regression Modeling for Ray Pointing Prediction in Virtual Reality,2022-04-28 00:00:00
793,Daniel Wigdor,"Emerging input techniques that rely on sensing and recognition can misinterpret a user's intention, resulting in errors and, potentially, a negative user experience. To enhance the development of such input techniques, it is valuable to understand implications of these errors, but they can very costly to simulate. Through two controlled experiments, this work explores various low-cost methods for evaluating error acceptability of freehand mid-air gestural input in virtual reality. Using a gesture-driven game and a drawing application, the first experiment elicited error characteristics through text descriptions, video demonstrations, and a touchscreen-based interactive simulation. The results revealed that video effectively conveyed the dynamics of errors, whereas the interactive modalities effectively reproduced the user experience of effort and frustration. The second experiment contrasts the interactive touchscreen",Investigating Cross-Modal Approaches for Evaluating Error Acceptability of a Recognition-Based Input Technique,2022-03-29 00:00:00
794,Daniel Wigdor,"An electronic device tracks, for a user performing a target acquisition movement within a 3D space, movement parameters of a plurality of input devices of the user. The electronic device predicts, for the user, a region of interest within the 3D space, using a regression model, based on the movement parameters. The region of interest includes a plurality of targets in close proximity. The electronic device predicts an endpoint of the target acquisition movement, within the region of interest, using a pointer facilitation technique. In some embodiments, the plurality of input devices includes an eye tracking input device, each input device corresponds to a predefined input device type, and the movement parameters include gaze data from the eye tracking input device. In some embodiments, input devices includes an eye tracking input device, a head-mounted display, and a hand-held controller, and the user's eye, hand ",Multimodal kinematic template matching and regression modeling for ray pointing prediction in virtual reality,2022-02-22 00:00:00
795,Daniel Wigdor,"We demonstrate rich inferences about unaugmented everyday objects and hand object interactions by measuring minute skin surface deformations at the wrist using a sensing technique based on capacitance. The wristband prototype infers muscle and tendon tension, pose, and motion, which we then map to force (9 users, 13.66+/-9.84 N regression error on classes 0–49.1 N), grasp (9 users, 81+/-7% classification accuracy on 6 grasps), and continuous interaction (10 users, 99+/-1% discrimination accuracy between 6 interactions, 89–97% accuracy on 3 states within each interaction) using basic machine learning models.",Sensing Hand Interactions with Everyday Objects by Profiling Wrist Topography,2022-02-13 00:00:00
796,Daniel Wigdor,"Exploring the design space of configurations for objects in virtual scenes is a challenge within virtual reality authoring tools due to the lack of visualization capabilities, non-destructive operations, suggestions, and flexibility. This work introduces Attribute Spaces, tools for visualizing and manipulating object attributes in virtual reality during 3D content generation. Attribute Spaces enable designers to systematically explore design spaces by supporting rapid comparisons between design alternatives and offering design suggestions. Custom combinations of attributes can be grouped and manipulated simultaneously for several objects. The grouping supports the creation of custom operation combinations that can be used as tools to edit multiple attribute, as well as snapshots of promising design decisions for later review. In an evaluation of Attribute Spaces by 3D design experts, our approach was found to enhance ",Attribute Spaces: Supporting Design Space Exploration in Virtual Reality,2021-11-09 00:00:00
797,Daniel Wigdor,"Existing approaches to trading off false positive versus false negative errors in input recognition are based on imprecise ideas of how these errors affect user experience that are unlikely to hold for all situations. To inform dynamic approaches to setting such a tradeoff, two user studies were conducted on how relative preference for false positive versus false negative errors is influenced by differences in the temporal cost of error recovery, and high-level task factors (time pressure, multi-tasking). Participants completed a tile selection task in which false positive and false negative errors were injected at a fixed rate, and the temporal cost to recover from each of the two types of error was varied, and then indicated a preference for one error type or the other, and a frustration rating for the task. Responses indicate that the temporal costs of error recovery can drive both frustration and relative error type preference, and that ",False Positives vs. False Negatives: The effects of recovery time and cognitive costs on input error preference,2021-10-10 00:00:00
798,Daniel Wigdor,"Disclosed are touch sensitive devices and methods of responding to hits in touch sensitive devices that include a graphical user interface having interface elements, each associated with a program element. A hit test map updater is used to process graphical user interface information into a hit test map in connection with the rendering of the graphical user interface, such that the hit test map associates properties with interface elements appearing on the graphical user interface. An input processor is used to receive a location corresponding to an input in connection with an input event, search the hit test map in which values are associated with interface elements appearing in the graphical user interface, and identify a property of the interface element from the values. In an embodiment, the identified property is proved to a central processing system and a user interface event is generated. In an embodiment, the",System and method for performing hit testing in a graphical user interface,2021-07-20 00:00:00
799,Daniel Wigdor,"A low-latency touch sensor is disclosed for use in connection with a touch surface having first and second conductors sensitive to changes in coupling therebetween as a result of touch (and/or near-touch). A signal generator generates unique orthogonal signals, a transmitter transmits the orthogonal signals on each of the first conductors. Receivers connected to the second conductors receive signals during a measurement period. The signals measured during the measurement period are processed to determine, for each measurement period, and for each of the second conductors, a signal strength corresponding to each of the unique orthogonal signals. The signal strengths can be used as a basis to determine touch events.",Fast multi-touch sensor with user-identification techniques,2021-07-06 00:00:00
800,Daniel Wigdor,"The craft of improvisational quilting involves working without the use of a predefined pattern. Design decisions are made “in the fabric,” with design experimentation tightly interleaved with the creation of the final artifact. To investigate how this type of design process can be supported, and to address challenges faced by practitioners, this paper presents PatchProv, a system for supporting improvisational quilt design. Based on a review of popular books on improvisational quilting, a set of design principles and key challenges to improvisational quilt design were identified, and PatchProv was developed to support the unique aspects of this process. An evaluation with a small group of quilters showed enthusiasm for the approach and revealed further possibilities for how computational tools can support improvisational quilting and improvisational design practices more broadly.",PatchProv: Supporting Improvisational Design Practices for Modern Quilting,2021-05-06 00:00:00
801,Daniel Wigdor,"This work explores the design of marking menus for gaze-based AR/VR menu selection by expert and novice users. It first identifies and explains the challenges inherent in ocular motor control and current eye tracking hardware, including overshooting, incorrect selections, and false activations. Through three empirical studies, we optimized and validated design parameters to mitigate these errors while reducing completion time, task load, and eye fatigue. Based on the findings from these studies, we derived a set of design guidelines to support gaze-based marking menus in AR/VR. To overcome the overshoot errors found with eye-based expert marking menu behaviour, we developed StickyPie, a marking menu technique that enables scale-independent marking input by estimating saccade landing positions. An evaluation of StickyPie revealed that StickyPie was easier to learn than the traditional technique","StickyPie: A Gaze-Based, Scale-Invariant Marking Menu Optimized for AR/VR",2021-05-06 00:00:00
802,Daniel Wigdor,"In virtual reality (VR) environments, asymmetric bimanual interaction techniques can increase users’ input bandwidth by complementing their perceptual and motor systems (eg, using the dominant hand to select 3D UI controls anchored around the non-dominant arm). However, it is unclear how to optimize the layout of such 3D UI controls for near-body and mid-air interactions. We evaluate the performance and limitations of non-dominant arm-anchored 3D UIs in VR environments through a bimanual pointing study. Results demonstrated that targets appearing closer to the skin, located around the wrist, or placed on the medial side of the forearm could be selected more quickly than targets farther away from the skin, located around the elbow, or on the lateral side of the forearm. Based on these results, we developed Armstrong guidelines, demonstrated through a Unity plugin to enable designers to create",Armstrong: An empirical examination of pointing at non-dominant arm-anchored UIs in virtual reality,2021-05-06 00:00:00
803,Daniel Wigdor,in real settings natural body movements can be erroneously recognized by wholebody input systems as explicit input actions we call body activity not intended as input actions background activity we argue that understanding background activity is crucial to the success of alwaysavailable wholebody input in the real world to operationalize this argument we contribute a reusable study methodology and software tools to generate standardized background activity datasets composed of data from multiple kinect cameras a vicon tracker and two highdefinition video cameras using our methodology we create an example background activity dataset for a televisionoriented living room setting we use this dataset to demonstrate how it can be used to redesign a gestural interaction vocabulary to minimize conflicts with the real world the software tools and initial living room dataset are publicly available httpwwwdgptorontoedudustinbackgroundactivity less,"A Dataset of Naturally Occurring, Whole-Body Background Activity to Reduce Gesture Conflicts","21 September, 2015"
804,Yang Xu,"Languages vary considerably in syntactic structure. About 40% of the world’s languages have subject–verb–object order, and about 40% have subject–object–verb order. Extensive work has sought to explain this word order variation across languages. However, the existing approaches are not able to explain coherently the frequency distribution and evolution of word order in individual languages. We propose that variation in word order reflects different ways of balancing competing pressures of dependency locality and information locality, whereby languages favor placing elements together when they are syntactically related or contextually informative about each other. Using data from 80 languages in 17 language families and phylogenetic modeling, we demonstrate that languages evolve to balance these pressures, such that word order change is accompanied by change in the frequency distribution of the",Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality,2022-06-14 00:00:00
805,Yang Xu,"Gender associations have been a long‐standing research topic in psychological and social sciences. Although it is known that children learn aspects of gender associations at a young age, it is not well understood how they might emerge through the course of development. We investigate whether gender associations, such as the association of dresses with women and bulldozers with men, are reflected in the linguistic communication of young children from ages 1–5. Drawing on recent methods from machine learning, we use word embeddings derived from large text corpora including news articles and web pages as a proxy for gender associations in society, and we compare those with the gender associations of words uttered by caretakers and children in children's linguistic environment. We quantify gender associations in childhood language through gender probability, which measures the extent to which ",The Emergence of Gender Associations in Child Language Development,2022/6
806,Yang Xu,"Humans can flexibly extend word usages across different grammatical classes, a phenomenon known as word class conversion. Noun-to-verb conversion, or denominal verb (e.g., to Google a cheap flight), is one of the most prevalent forms of word class conversion. However, existing natural language processing systems are impoverished in interpreting and generating novel denominal verb usages. Previous work has suggested that novel denominal verb usages are comprehensible if the listener can compute the intended meaning based on shared knowledge with the speaker. Here we explore a computational formalism for this proposal couched in frame semantics. We present a formal framework, Noun2Verb, that simulates the production and comprehension of novel denominal verb usages by modeling shared knowledge of speaker and listener in semantic frames. We evaluate an incremental set of probabilistic models that learn to interpret and generate novel denominal verb usages via paraphrasing",Noun2Verb: Probabilistic frame semantics for word class conversion,2022-05-12 00:00:00
807,Yang Xu,"Slang is a predominant form of informal language making flexible and extended use of words that is notoriously hard for natural language processing systems to interpret. Existing approaches to slang interpretation tend to rely on context but ignore semantic extensions common in slang word usage. We propose a semantically informed slang interpretation (SSI) framework that considers jointly the contextual and semantic appropriateness of a candidate interpretation for a query slang. We perform rigorous evaluation on two large-scale online slang dictionaries and show that our approach not only achieves state-of-the-art accuracy for slang interpretation in English, but also does so in zero-shot and few-shot scenarios where training data is sparse. Furthermore, we show how the same framework can be applied to enhancing machine translation of slang from English to other languages. Our work creates opportunities for the automated interpretation and translation of informal language.",Semantically Informed Slang Interpretation,2022-05-02 00:00:00
808,Yang Xu,"In lexicalist linguistic theories, argument structure is assumed to be predictable from the meaning of verbs. As a result, the verb is the primary determinant of the meaning of a clause. In contrast, construction grammarians propose that argument structure is encoded in constructions (or form-meaning pairs) that are distinct from verbs. Decades of psycholinguistic research have produced substantial empirical evidence in favor of the construction view. Here we adapt several psycholinguistic studies to probe for the existence of argument structure constructions (ASCs) in Transformer-based language models (LMs). First, using a sentence sorting experiment, we find that sentences sharing the same construction are closer in embedding space than sentences sharing the same verb. Furthermore, LMs increasingly prefer grouping by construction with more input data, mirroring the behaviour of non-native language learners. Second, in a ""Jabberwocky"" priming-based experiment, we find that LMs associate ASCs with meaning, even in semantically nonsensical sentences. Our work offers the first evidence for ASCs in LMs and highlights the potential to devise novel probing methods grounded in psycholinguistic research.",Neural reality of argument structure constructions,2022-02-24 00:00:00
809,Yang Xu,"Contextualized word embeddings have demonstrated state-of-the-art performance in various natural language processing tasks including those that concern historical semantic change. However, language models such as BERT was trained primarily on contemporary corpus data. To investigate whether training on historical corpus data improves diachronic semantic analysis, we present a pre-trained BERT-based language model, HistBERT, trained on the balanced Corpus of Historical American English. We examine the effectiveness of our approach by comparing the performance of the original BERT and that of HistBERT, and we report promising results in word similarity and semantic shift analysis. Our work suggests that the effectiveness of contextual embeddings in diachronic semantic analysis is dependent on the temporal profile of the input text and care should be taken in applying this methodology to study historical semantic change.",HistBERT: A Pre-trained Language Model for Diachronic Lexical Semantic Analysis,2022-02-08 00:00:00
810,Yang Xu,"Although language is critical to supporting morality within society, it is not clear how moral language itself evolved. We investigate the evolution of moral semantics, hypothesizing that words evolved to take on moral meanings from concrete experiences through metaphorization. We test this hypothesis by analyzing moral semantic change in words from the Moral Foundations Dictionary and the Historical Thesaurus of English over the past hundreds of years. In contrast with the observation that words become concrete over time, we demonstrate that moral words in the English lexicon undergo concrete-to-abstract shifts, reflecting systematic metaphorical mappings to the moral domain. Our results provide large-scale evidence for the role of metaphor in the historical development of the English moral lexicon.",Evolution of moral semantics through metaphorization,2022/1
811,Yang Xu,"Different domains exhibit different degrees of lexical precision. Existing work has suggested that communicative need may modulate the precision of word meaning in individual domains. We extend this proposal across domains by asking why languages have more precise vocabulary in some domains than others. We hypothesize that lexical precision for a domain reflects how frequently speakers need to refer to it. We test this proposal using a cross-linguistic dataset of word-concept mappings for nine diverse domains from seven languages, and word frequencies from independent corpora. We find that the more frequent domains (except for kinship) tend to be more precise in every language, supporting a domain-level account of efficient communication on the precision of the lexicon.",Communicative need modulates lexical precision across semantic domains: A domain-level account of efficient communication,2022/1
812,Yang Xu,"One of the most influential modern theories of morality, Moral Foundations Theory, proposes that morality is formed on innate and shared modular foundations. Psychologists have studied the conceptual development of these moral foundations in childhood, but there exists no comprehensive effort on characterizing the early emergence of moral foundations in naturalistic settings. We explore the emerging order of moral foundations through child and caretaker speech. Using computational methods, we contribute an annotated dataset of moral utterances and find that the individualizing foundations emerge earlier than the binding foundations. Furthermore, caretakers tend to talk more about fairness and degradation, while children talk more about cheating. These results are robust across child gender, family's social class, and race.",The emergence of moral foundations in child language development,2022
813,Yang Xu,"Word meanings extend over time due to a functional need for maintaining communicative expressivity within a compact lexicon. Previous scholars have suggested that word meanings extend via a process of chaining, whereby novel items link to existing ones close in semantic space. Recent work has formalized this idea using computational models grounded typically in the exemplar and prototype theories of categorization that are either memory-intensive or simplistic in representation. We propose an alternative account of chaining that optimizes cognitive efficiency by trading off representational accuracy with memory complexity. We operationalize this efficient chaining as an infinite mixture model and show how it constructs the internal representations of word meaning adaptively through time while predicting the historical development of English verb meanings with precision and limited resources.",Infinite mixture chaining: Efficient temporal construction of word meaning,2022/1
814,Yang Xu,Compounding is a common type of word formation extensively studied in linguistics and cognitive psychology. A growing line of research suggests that the lexicon supports efficient communication by balancing informativeness and simplicity. We propose that the formation of novel compounds reflects a similar tradeoff between informativeness and word length. We formalize this hypothesis in information-theoretic terms and develop a computational procedure to evaluate our hypothesis on English noun compounds that emerged over the past century. We find that attested compounds achieve more efficient tradeoffs between informativeness and word length than do alternative word forms. Our work demonstrates how word formation and compositionality can be connected with information-theoretic approaches to the design of the lexicon.,Word formation supports efficient communication: The case of compounds,2022
815,Yang Xu,"Functionalist accounts of language suggest that forms are paired with meanings in ways that support efficient communication. Previous work on grammatical marking suggests that word forms have lengths that enable efficient production, and work on the semantic typology of the lexicon suggests that word meanings represent efficient partitions of semantic space. Here we establish a theoretical link between these two lines of work and present an information-theoretic analysis that captures how communicative pressures influence both form and meaning. We apply our approach to the grammatical features of number, tense, and evidentiality and show that the approach explains both which systems of feature values are attested across languages and the relative lengths of the forms for those feature values. Our approach shows that general information-theoretic principles can capture variation in both form and ",The forms and meanings of grammatical markers support efficient communication,2021-12-07 00:00:00
816,Richard Zemel,despite impressive progress in deep learning generalizing far beyond the training distribution is an important open challenge in this work we consider fewshot classification and aim to shed light on what makes some novel classes easier to learn than others and what types of learned representations generalize better to this end we define a new paradigm in terms of attributes simple building blocks of which concepts are formed as a means of quantifying the degree of relatedness of different concepts our empirical analysis reveals that supervised learning generalizes poorly to new attributes but a combination of selfsupervised pretraining with supervised finetuning leads to stronger generalization the benefit of selfsupervised pretraining and supervised finetuning is further investigated through controlled experiments using random splits of the attribute space and we find that predictability of test attributes provides an informative estimate of a models generalization ability less,Probing Few-Shot Generalization with Attributes,"30 May, 2022"
817,Richard Zemel,recent largescale natural language processing nlp systems use a pretrained large language model llm on massive and diverse corpora as a headstart in practice the pretrained model is adapted to a wide array of tasks via finetuning on taskspecific datasets llms while effective have been shown to memorize instances of training data thereby potentially revealing private information processed during pretraining the potential leakage might further propagate to the downstream tasks for which llms are finetuned on the other hand privacypreserving algorithms usually involve retraining from scratch which is prohibitively expensive for llms in this work we propose a simple easy to interpret and computationally lightweight perturbation mechanism to be applied to an already trained model at the decoding stage our perturbation mechanism is modelagnostic and can be used in conjunction with any llm we provide theoretical analysis showing that the proposed mechanism is differentially private and experimental results showing a privacyutility tradeoff less,Differentially Private Decoding in Large Language Models,"26 May, 2022"
818,Richard Zemel,as natural language processing systems become more widespread it is necessary to address fairness issues in their implementation and deployment to ensure that their negative impacts on society are understood and minimized however there is limited work that studies fairness using a multilingual and intersectional framework or on downstream tasks in this paper we introduce four multilingual equity evaluation corpora supplementary test sets designed to measure social biases and a novel statistical framework for studying unisectional and intersectional social biases in natural language processing we use these tools to measure gender racial ethnic and intersectional social biases across five models trained on emotion regression tasks in english spanish and arabic we find that many systems demonstrate statistically significant unisectional and intersectional social biases less,"Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic","7 April, 2022"
819,Richard Zemel,on timeseries data most causal discovery methods fit a new model whenever they encounter samples from a new underlying causal graph however these samples often share relevant information which is lost when following this approach specifically different samples may share the dynamics which describe the effects of their causal relations we propose amortized causal discovery a novel framework that leverages such shared dynamics to learn to infer causal relations from timeseries data this enables us to train a single amortized model that infers causal relations across samples with different underlying causal graphs and thus leverages the shared dynamics information we demonstrate experimentally that this approach implemented as a variational model leads to significant improvements in causal discovery performance and show how it can be extended to perform well under added noise and hidden confounding less,Amortized Causal Discovery: Learning to Infer Causal Graphs from Time-Series Data,"21 February, 2022"
820,Richard Zemel,given the ubiquity of deep neural networks it is important that these models do not reveal information about sensitive data that they have been trained on in model inversion attacks a malicious user attempts to recover the private dataset used to train a supervised neural network a successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset in this work we provide a probabilistic interpretation of model inversion attacks and formulate a variational objective that accounts for both diversity and accuracy in order to optimize this variational objective we choose a variational family defined in the code space of a deep generative model trained on a public auxiliary dataset that shares some structural similarity with the target dataset empirically our method substantially improves performance in terms of target attack accuracy sample realism and diversity on datasets of faces and chest xray images less,Variational Model Inversion Attacks,"26 January, 2022"
821,Richard Zemel,deep learning systems frequently fail at outofcontext ooc prediction the problem of making reliable predictions on uncommon or unusual inputs or subgroups of the training distribution to this end a number of benchmarks for measuring ooc performance have recently been introduced in this work we introduce a framework unifying the literature on ooc performance measurement and demonstrate how rich auxiliary information can be leveraged to identify candidate sets of ooc examples in existing datasets we present nooch a suite of naturallyoccurring challenge sets and show how varying notions of context can be used to probe specific ooc failure modes experimentally we explore the tradeoffs between various learning approaches on these challenge sets and demonstrate how the choices made in designing ooc benchmarks can yield varying conclusions less,Identifying and Benchmarking Natural Out-of-Context Prediction Problems,"25 October, 2021"
822,Richard Zemel,learning models that gracefully handle distribution shifts is central to research on domain generalization robust optimization and fairness a promising formulation is domaininvariant learning which identifies the key issue of learning which features are domainspecific versus domaininvariant an important assumption in this area is that the training examples are partitioned into domains or environments our focus is on the more common setting where such partitions are not provided we propose eiil a general framework for domaininvariant learning that incorporates environment inference to directly infer partitions that are maximally informative for downstream invariant learning we show that eiil outperforms invariant learning methods on the cmnist benchmark without using environment labels and significantly outperforms erm on worstgroup performance in the waterbirds and civilcomments datasets finally we establish connections between eiil and algorithmic fairness which enables eiil to improve accuracy and calibration in a fair prediction problem less,Environment Inference for Invariant Learning,"15 July, 2021"
823,Richard Zemel,sketch drawings capture the salient information of visual concepts previous work has shown that neural networks are capable of producing sketches of natural objects drawn from a small number of classes while earlier approaches focus on generation quality or retrieval we explore properties of image representations learned by training a model to produce sketches of images we show that this generative classagnostic model produces informative embeddings of images from novel examples classes and even novel datasets in a fewshot setting additionally we find that these learned representations exhibit interesting structure and compositionality less,SketchEmbedNet: Learning Novel Concepts by Imitating Drawings,"22 June, 2021"
824,Richard Zemel,slang is a common type of informal language but its flexible nature and paucity of data resources present challenges for existing natural language systems we take an initial step toward machine generation of slang by developing a framework that models the speakers word choice in slang context our framework encodes novel slang meaning by relating the conventional and slang senses of a word while incorporating syntactic and contextual knowledge in slang usage we construct the framework using a combination of probabilistic inference and neural contrastive learning we perform rigorous evaluations on three slang dictionaries and show that our approach not only outperforms stateoftheart language models but also better predicts the historical emergence of slang word usages from s to s we interpret the proposed models and find that the contrastively learned semantic space is sensitive to the similarities between slang and conventional senses of words our work creates opportunities for the automated generation and interpretation of informal language less,A Computational Framework for Slang Generation,"22 May, 2021"
825,Richard Zemel,we aim to bridge the gap between typical human and machinelearning environments by extending the standard framework of fewshot learning to an online continual setting in this setting episodes do not have separate training and testing phases and instead models are evaluated online while learning novel classes as in the real world where the presence of spatiotemporal context helps us retrieve learned skills in the past our online fewshot learning setting also features an underlying context that changes throughout time object classes are correlated within a context and inferring the correct context can lead to better performance building upon this setting we propose a new fewshot learning dataset based on large scale indoor imagery that mimics the visual experience of an agent wandering within a world furthermore we convert popular fewshot learning approaches into online versions and we also propose a new contextual prototypical memory model that can make use of spatiotemporal contextual information from the recent past less,Wandering Within a World: Online Contextualized Few-Shot Learning,"22 April, 2021"
826,Richard Zemel,fewshot classification fsc the task of adapting a classifier to unseen classes given a small labeled dataset is an important step on the path toward humanlike machine learning bayesian methods are wellsuited to tackling the fundamental issue of overfitting in the fewshot scenario because they allow practitioners to specify prior beliefs and update those beliefs in light of observed data contemporary approaches to bayesian fewshot classification maintain a posterior distribution over model parameters which is slow and requires storage that scales with model size instead we propose a gaussian process classifier based on a novel combination of plyagamma augmentation and the onevseach softmax approximation that allows us to efficiently marginalize over functions rather than model parameters we demonstrate improved accuracy and uncertainty quantification on both standard fewshot classification benchmarks and fewshot domain transfer tasks less,Bayesian Few-Shot Classification with One-vs-Each Pólya-Gamma Augmented Gaussian Processes,"21 January, 2021"
827,Richard Zemel,robustness is of central importance in machine learning and has given rise to the fields of domain generalization and invariant learning which are concerned with improving performance on a test distribution distinct from but related to the training distribution in light of recent work suggesting an intimate connection between fairness and robustness we investigate whether algorithms from robust ml can be used to improve the fairness of classifiers that are trained on biased data and tested on unbiased data we apply invariant risk minimization irm a domain generalization algorithm that employs a causal discovery inspired method to find robust predictors to the task of fairly predicting the toxicity of internet comments we show that irm achieves better outofdistribution accuracy and fairness than empirical risk minimization erm methods and analyze both the difficulties that arise when applying irm in practice and the conditions under which irm will likely be effective in this scenario we hope that this work will inspire further studies of how robust machine learning methods relate to algorithmic fairness less,Fairness and Robustness in Invariant Learning: A Case Study in Toxicity Classification,"1 December, 2020"
828,Richard Zemel,most recommender systems rs research assumes that a users utility can be maximized independently of the utility of the other agents eg other users content providers in realistic settings this is often not truethe dynamics of an rs ecosystem couple the longterm utility of all agents in this work we explore settings in which content providers cannot remain viable unless they receive a certain level of user engagement we formulate the recommendation problem in this setting as one of equilibrium selection in the induced dynamical system and show that it can be solved as an optimal constrained matching problem our model ensures the system reaches an equilibrium with maximal social welfare supported by a sufficiently diverse set of viable providers we demonstrate that even in a simple stylized dynamical rs model the standard myopic approach to recommendationalways matching a user to the best providerperforms poorly we develop several scalable techniques to solve the matching problem and also draw connections to various notions of user regret and fairness arguing that these outcomes are fairer in a utilitarian sense less,Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach,"18 August, 2020"
829,Richard Zemel,we propose a new family of efficient and expressive deep generative models of graphs called graph recurrent attention networks grans our model generates graphs one block of nodes and associated edges at a time the block size and sampling stride allow us to trade off sample quality for efficiency compared to previous rnnbased graph generative models our framework better captures the autoregressive conditioning between the alreadygenerated and tobegenerated parts of the graph using graph neural networks gnns with attention this not only reduces the dependency on node ordering but also bypasses the longterm bottleneck caused by the sequential nature of rnns moreover we parameterize the output distribution per block using a mixture of bernoulli which captures the correlations among generated edges within the block finally we propose to handle node orderings in generation by marginalizing over a family of canonical orderings on standard benchmarks we achieve stateoftheart time efficiency and sample quality compared to previous models additionally we show our model is capable of generating large graphs of up to k nodes with good quality to the best of our knowledge gran is the first deep graph generative model that can scale to this size our code is released at httpsgithubcomlrjconangran less,Efficient Graph Generation with Graph Recurrent Attention Networks,"17 July, 2020"
830,Richard Zemel,in many application areaslending education and online recommenders for examplefairness and equity concerns emerge when a machine learning system interacts with a dynamically changing environment to produce both immediate and longterm effects for individuals and demographic groups we discuss causal directed acyclic graphs dags as a unifying framework for the recent literature on fairness in such dynamical systems we show that this formulation affords several new directions of inquiry to the modeler where causal assumptions can be expressed and manipulated we emphasize the importance of computing interventional quantities in the dynamical fairness setting and show how causal assumptions enable simulation when environment dynamics are known and offpolicy estimation when dynamics are unknown of intervention on short and longterm outcomes at both the group and individual levels less,Causal Modeling for Fairness in Dynamical Systems,"6 July, 2020"
831,Richard Zemel,in many settings it is desirable to learn decisionmaking and control policies through learning or bootstrapping from expert demonstrations the most common approaches under this imitation learning il framework are behavioural cloning bc and inverse reinforcement learning irl recent methods for irl have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations a scenario in which bc methods often fail unfortunately due to multiple factors of variation directly comparing these methods does not provide adequate intuition for understanding this difference in performance in this work we present a unified probabilistic perspective on il algorithms based on divergence minimization we present fmax an fdivergence generalization of airl fu et al a stateoftheart irl method fmax enables us to relate prior irl methods such as gail ho ermon and airl fu et al and understand their algorithmic properties through the lens of divergence minimization we tease apart the differences between bc and successful irl approaches and empirically evaluate these nuances on simulated highdimensional continuous control domains our findings conclusively identify that irls statemarginal matching objective contributes most to its superior performance lastly we apply our new understanding of il methods to the problem of statemarginal matching where we demonstrate that in simulated arm pushing environments we can teach agents a diverse range of behaviours using simply handspecified state distributions and no reward functions or expert demonstrations for datasets and reproducing results please refer to httpsgithubcomkamyarghrlswissblobmasterreproducingfmaxpapermd less,A Divergence Minimization Perspective on Imitation Learning Methods,"6 November, 2019"
832,Richard Zemel,we propose the lanczos network lanczosnet which uses the lanczos algorithm to construct low rank approximations of the graph laplacian for graph convolution relying on the tridiagonal decomposition of the lanczos algorithm we not only efficiently exploit multiscale information via fast approximated computation of matrix power but also design learnable spectral filters being fully differentiable lanczosnet facilitates both graph kernel learning as well as learning node embeddings we show the connection between our lanczosnet and graph based manifold learning methods especially the diffusion maps we benchmark our model against several recent deep graph networks on citation networks and qm quantum chemistry dataset experimental results show that our model achieves the stateoftheart performance in most tasks code is released at urlhttpsgithubcomlrjconanlanczosnetwork less,LanczosNet: Multi-Scale Deep Graph Convolutional Networks,"23 October, 2019"
833,Richard Zemel,a fundamental computation for statistical inference and accurate decisionmaking is to compute the marginal probabilities or most probable states of taskrelevant variables probabilistic graphical models can efficiently represent the structure of such complex data but performing these inferences is generally difficult messagepassing algorithms such as belief propagation are a natural way to disseminate evidence amongst correlated variables while exploiting the graph structure but these algorithms can struggle when the conditional dependency graphs contain loops here we use graph neural networks gnns to learn a messagepassing algorithm that solves these inference tasks we first show that the architecture of gnns is wellmatched to inference tasks we then demonstrate the efficacy of this inference approach by training gnns on a collection of graphical models and showing that they substantially outperform belief propagation on loopy graphs our messagepassing algorithms generalize out of the training set to larger graphs and graphs with different structure less,Inference in Probabilistic Graphical Models by Graph Neural Networks,"27 June, 2019"
834,Richard Zemel,the power of machine learning systems not only promises great technical progress but risks societal harm as a recent example researchers have shown that popular word embedding algorithms exhibit stereotypical biases such as gender bias the widespread use of these algorithms in machine learning systems from automated translation services to curriculum vitae scanners can amplify stereotypes in important contexts although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations there is a lack of understanding in how word embedding bias depends on the training data in this work we develop a technique for understanding the origins of bias in word embeddings given a word embedding trained on a corpus our method identifies how perturbing the corpus will affect the bias of the resulting embedding this can be used to trace the origins of word embedding bias back to the original training documents using our method one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias we demonstrate our techniques on both a new york times and wikipedia corpus and find that our influence functionbased approximations are very accurate less,Understanding the Origins of Bias in Word Embeddings,"7 June, 2019"
835,Richard Zemel,momentum is a simple and widely used trick which allows gradientbased optimizers to pick up speed along low curvature directions its performance depends crucially on a damping coefficient large values can potentially deliver much larger speedups but are prone to oscillations and instability hence one typically resorts to small values such as or we propose aggregated momentum aggmo a variant of momentum which combines multiple velocity vectors with different parameters aggmo is trivial to implement but significantly dampens oscillations enabling it to remain stable even for aggressive values such as we reinterpret nesterovs accelerated gradient descent as a special case of aggmo and analyze rates of convergence for quadratic objectives empirically we find that aggmo is a suitable dropin replacement for other momentum methods and frequently delivers faster convergence less,Aggregated Momentum: Stability Through Passive Damping,"1 May, 2019"
836,Richard Zemel,variational autoencoders vaes are widely used deep generative models capable of learning unsupervised latent representations of data such representations are often difficult to interpret or control we consider the problem of unsupervised learning of features correlated to specific labels in a dataset we propose a vaebased generative model which we show is capable of extracting features correlated to binary labels in the data and structuring it in a latent subspace which is easy to interpret our model the conditional subspace vae csvae uses mutual information minimization to learn a lowdimensional latent subspace associated with each label that can easily be inspected and independently manipulated we demonstrate the utility of the learned representations for attribute manipulation tasks on both the toronto face and celeba datasets less,Learning Latent Subspaces in Variational Autoencoders,"14 December, 2018"
837,Richard Zemel,synthesizing programs using example inputoutputs is a classic problem in artificial intelligence we present a method for solving programming by example pbe problems by using a neural model to guide the search of a constraint logic programming system called minikanren crucially the neural model uses minikanrens internal representation as input minikanren represents a pbe problem as recursive constraints imposed by the provided examples we explore recurrent neural network and graph neural network models we contribute a modified minikanren drivable by an external agent available at httpsgithubcomxuexueneuralkanren we show that our neuralguided approach using constraints can synthesize programs faster in many cases and importantly can generalize to larger problems less,Neural Guided Constraint Logic Programming for Program Synthesis,"26 October, 2018"
838,Richard Zemel,in many machine learning applications there are multiple decisionmakers involved both automated and human the interaction between these agents often goes unaddressed in algorithmic development in this work we explore a simple version of this interaction with a twostage framework containing an automated model and an external decisionmaker the model can choose to say pass and pass the decision downstream as explored in rejection learning we extend this concept by proposing learning to defer which generalizes rejection learning by considering the effect of other agents in the decisionmaking process we propose a learning algorithm which accounts for potential biases held by external decisionmakers in a system experiments demonstrate that learning to defer can make systems not only more accurate but also less biased even when working with inconsistent or biased users we show that deferring models still greatly improve the accuracy andor fairness of the entire system less,Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer,"6 September, 2018"
839,Richard Zemel,we present graph partition neural networks gpnn an extension of graph neural networks gnns able to handle extremely large graphs gpnns alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs to efficiently partition graphs we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs we extensively test our model on a variety of semisupervised node classification tasks experimental results indicate that gpnns are either superior or comparable to stateoftheart methods on a wide variety of datasets for graphbased semisupervised classification we also show that gpnns can achieve similar performance as standard gnns with fewer propagation steps less,Graph Partition Neural Networks for Semi-Supervised Classification,"16 March, 2018"
840,Gary Baumgartner,"Design patterns are distilled from many real systems to catalog common programming practice. However, some object-oriented design patterns are distorted or overly complicated because of the lack of supporting programming language constructs or mechanisms. For this paper, we have analyzed several published design patterns looking for idiomatic ways of working around constraints of the implementation language. From this analysis, we lay a groundwork of general-purpose language constructs and mechanisms that, if provided by a statically typed, object-oriented language, would better support the implementation of design patterns and, transitively, benefit the construction of many real systems. In particular, our catalog of language constructs includes subtyping separate from inheritance, lexically scoped closure objects independent of classes, and multimethod dispatch. The proposed constructs and mechanisms are not radically new, but rather are adopted from a variety of languages and programming language research and combined in a new, orthogonal manner. We argue that by describing design patterns in terms of the proposed constructs and mechanisms, pattern descriptions become simpler and, therefore, accessible to a larger number of language communities. Constructs and mechanisms lacking in a particular language can be implemented using paradigmatic idioms.",On the interaction of object-oriented design patterns and programming languages,2019-05-31 00:00:00
841,Gary Baumgartner,"(NCBIBLAST) code for sequence alignment and Cannon’s algorithm for matrix multiplication. The first is an example of an independent task application, a type of application commonly used for grid scheduling research because of its easily decomposable nature and absence of intranode communication. The second is a popular block algorithm for parallel matrix multiplication and represents a challenging application for grid platforms because of its highly structured and synchronous communication pattern. Agent behavior completely determines the way computation is organized on the Organic Grid. We intentionally chose two applications at opposite ends of the distributed computing spectrum having very different requirements in terms of communication topology, resource use, and response to faults. We detail the design of the agent behavior and show how the different requirements can be satisfied. ",Self-Organizing Scheduling on the Organic Grid,2018-10-03 00:00:00
842,Gary Baumgartner,"The performance variation of cloud resources makes it difficult to run certain scientific applications in the cloud because of their unique synchronization and communication requirements. We propose a decentralized scheduling approach for many-task applications that assigns individual tasks to cloud nodes based on periodic performance measurements of the cloud resources. In this paper, we present a vector-based scheduling algorithm that assigns tasks to nodes based on measuring the compute performance and the queue length of those nodes. Our experiments with a set of tasks in CloudLab show that the application proceeds in three distinct phases: flooding the cloud nodes with tasks, a steady state in which all nodes are busy, and the end game in which the remaining tasks are executed on the fastest nodes. We present heuristics for these three phases and demonstrate with measurements in ",A Vector-Scheduling Approach for Running Many-Task Applications in the Cloud,2018-06-25 00:00:00
843,Gary Baumgartner,"Cloud services are transforming many computing tasks, but the unique requirements of scientific computing have caused it to lag behind in cloud adoption because of the performance variation of cloud resources. Based on our experience with the Organic Grid, we propose a framework for a hybrid cloud that will intelligently distribute work to appropriate computing resources to mitigate the impact of performance variation. We describe a cloud framework that integrates with specialized hardware and distributes work intelligently among heterogeneous computing resources. Our approach is to organize a set of computing nodes in an overlay network, to allow each node as an individual agent to position itself within the network to maximize its productivity. An application finds the resources and decides which task to run on which cloud nodes. Our simulations demonstrate that our methods can significantly reduce ",A hybrid cloud framework for scientific computing,2015-06-27 00:00:00
844,Gary Baumgartner,"Empirical optimizers like ATLAS have been very effective in optimizing computational kernels in libraries. The best choice of parameters such as tile size and degree of loop unrolling is determined in ATLAS by executing different versions of the computation. In contrast, optimizing compilers use a model-driven approach to program transformation. While the model-driven approach of optimizing compilers is generally orders of magnitude faster than ATLAS-like library generators, its effectiveness can be limited by the accuracy of the performance models used. In this paper, we describe an approach where a class of computations is modeled in terms of constituent operations that are empirically measured, thereby allowing modeling of the overall execution time. The performance model with empirically determined cost components is used to select library calls and choose data layout transformations in the context of the ",Empirical performance model-driven data layout optimization and library call selection for tensor contraction expressions,2012-03-01 00:00:00
845,Gary Baumgartner,"The need to evaluate expression trees involving large objects arises in scientific computing applications such as electronic structure calculations. Often, the tree node objects are so large that only a subset of them can fit into memory at a time. This paper addresses the problem of finding an evaluation order of the nodes in a given expression tree that uses the least amount of memory. We present an algorithm that finds an optimal evaluation order in Θ (n log 2 n) time for an n-node expression tree and prove its correctness. We demonstrate the utility of our algorithm using representative equations from quantum chemistry.",Memory-optimal evaluation of expression trees involving large objects,2011-07-01 00:00:00
846,Gary Baumgartner,"Complex tensor contraction expressions arise in accurate electronic structure models in quantum chemistry, such as the coupled cluster method. This paper addresses two complementary aspects of performance optimization of such tensor contraction expressions. Transformations using algebraic properties of commutativity and associativity can be used to significantly decrease the number of arithmetic operations required for evaluation of these expressions. The identification of common subexpressions among a set of tensor contraction expressions can result in a reduction of the total number of operations required to evaluate the tensor contractions. The first part of the paper describes an effective algorithm for operation minimization with common subexpression identification and demonstrates its effectiveness on tensor contraction expressions for coupled cluster equations. The second part of the paper highlights ",Performance optimization of tensor contraction expressions for many-body methods in quantum chemistry,2009-11-12 00:00:00
847,Gary Baumgartner,the crab nebula is the only hard xray source in the sky that is both bright enough and steady enough to be easily used as a standard candle as a result it has been used as a normalization standard by most xraygamma ray telescopes although smallscale variations in the nebula are wellknown since the start of science operations of the fermi gammaray burst monitor gbm in august a mcrab decline has been observed in the overall crab nebula flux in the kev band measured with the earth occultation technique this decline is independently confirmed with three other instruments the swift burst alert telescope swiftbat the rossi xray timing explorer proportional counter array rxtepca and the international gammaray astrophysics laboratory imager on board integral ibis a similar decline is also observed in the kev data from the rxtepca and integral joint european monitor jemx and in the kev band with gbm and integralibis observations from to kev with gbm suggest that the decline may be larger at higher energies the pulsed flux measured with rxtepca since is consistent with the pulsar spindown indicating that the observed changes are nebular correlated variations in the crab nebula flux on a year timescale are also seen independently with the pca bat and ibis from to with a flux minimum in april as of august the current flux has declined below the minimum less,When A Standard Candle Flickers,"3 February, 2011"
848,Jennifer Campbell,warmdense matter wdm is a highlyexcited state that lies at the confluence of solids plasmas and liquids and that cannot be described by equilibrium theories the transient nature of this state when created in a laboratory as well as the difficulties in probing the stronglycoupled interactions between the electrons and the ions make it challenging to develop a complete understanding of matter in this regime in this work by exciting isolated nm nanoparticles with a femtosecond laser below the ablation threshold we create uniformlyexcited wdm we then use photoelectron spectroscopy to track the instantaneous electron temperature and directly extract the strongest electronion coupling observed experimentally to date by directly comparing with stateoftheart theories we confirm that the superheated nanoparticles lie at the boundary between hot solids and plasmas with associated strong electronion coupling this is evidenced both by the fast energy loss of electrons to ions as well as a strong modulation of the electron temperature by acoustic oscillations in the nanoparticle this work demonstrates a new route for experimental exploration and theoretical validation of the exotic properties of wdm less,Direct observation of enhanced electron-phonon coupling in copper nanoparticles in the warm-dense matter regime,"28 June, 2022"
849,Jennifer Campbell,the human cell atlas hca will be made up of comprehensive reference maps of all human cells the fundamental units of life as a basis for understanding fundamental human biological processes and diagnosing monitoring and treating disease it will help scientists understand how genetic variants impact disease risk define drug toxicities discover better therapies and advance regenerative medicine a resource of such ambition and scale should be built in stages increasing in size breadth and resolution as technologies develop and understanding deepens we will therefore pursue phase as a suite of flagship projects in key tissues systems and organs we will bring together experts in biology medicine genomics technology development and computation including data analysis software engineering and visualization we will also need standardized experimental and computational methods that will allow us to compare diverse cell and tissue types and samples across human communities in consistent ways ensuring that the resulting resource is truly global this document the first version of the hca white paper was written by experts in the field with feedback and suggestions from the hca community gathered during recent international meetings the white paper released at the close of this yearlong planning process will be a living document that evolves as the hca community provides additional feedback as technological and computational advances are made and as lessons are learned during the construction of the atlas less,The Human Cell Atlas White Paper,"11 October, 2018"
850,Jennifer Campbell,we present the results of a search for potential transit signals in the full quarter data set collected during keplers primary mission that ended on may due to the onboard failure of a second reaction wheel needed to maintain high precision fixed pointing the search includes a total of targets of which were observed in every quarter and were observed in a subset of the quarters we find a total of targets that contain at least one signal that meets our detection criteria periodicity of the signal a minimum of three transit events an acceptable signaltonoise ratio and four consistency tests that suppress false positives each target containing at least one transitlike pulse sequence is searched repeatedly for other signals that meet the detection criteria indicating a multiple planet system this multiple planet search adds an additional transitlike signatures for a total of comparison of this set of detected signals with a set of known and vetted transiting planet signatures in the kepler field of view shows that the recovery rate of the search is we review ensemble properties of the detected signals and present various metrics useful in validating these potential planetary signals we highlight previously undetected planetary candidates including several small potential planets in the habitable zone of their host stars less,Detection of Potential Transit Signals in 17 Quarters of Kepler Mission Data,"13 February, 2015"
851,Jennifer Campbell,the kepler mission discovered exoplanet candidates with years of data we provide updates to the kepler planet candidate sample based upon years qq of data through a series of tests to exclude falsepositives primarily caused by eclipsing binary stars and instrumental systematics additional planetary candidates have been discovered bringing the total number known to we provide revised transit parameters and accompanying posterior distributions based on a markov chain monte carlo algorithm for the cumulative catalogue of kepler objects of interest there are now candidates in the cumulative catalogue that receive less than twice the flux the earth receives and more than have a radius less than rearth there are now a dozen candidates meeting both criteria roughly doubling the number of candidate earth analogs a majority of planetary candidates have a high probability of being bonafide planets however there are populations of likely falsepositives we discuss and suggest additional cuts that can be easily applied to the catalogue to produce a set of planetary candidates with good fidelity the full catalogue is publicly available at the nasa exoplanet archive less,Planetary Candidates Observed by Kepler V: Planet Sample from Q1-Q12 (36 Months),"29 January, 2015"
852,Jennifer Campbell,the sloan digital sky survey sdss started a new phase in august with new instrumentation and new surveys focused on galactic structure and chemical evolution measurements of the baryon oscillation feature in the clustering of galaxies and the quasar ly alpha forest and a radial velocity search for planets around stars this paper describes the first data release of sdssiii and the eighth counting from the beginning of the sdss the release includes fiveband imaging of roughly deg in the southern galactic cap bringing the total footprint of the sdss imaging to deg or over a third of the celestial sphere all the imaging data have been reprocessed with an improved skysubtraction algorithm and a final selfconsistent photometric recalibration and flatfield determination this release also includes all data from the second phase of the sloan extension for galactic understanding and evolution segue consisting of spectroscopy of approximately stars at both high and low galactic latitudes all the more than half a million stellar spectra obtained with the sdss spectrograph have been reprocessed through an improved stellar parameters pipeline which has better determination of metallicity for high metallicity stars less,The Eighth Data Release of the Sloan Digital Sky Survey: First Data from SDSS-III,"25 February, 2011"
853,David Liu,the typeii terminated ttas surface of a threedimensional ttas bulk material realizes the effective spin degree of freedom on each david star cluster with mathcalt such that the timereversal symmetry is realized anomalously despite the fact that bulk threedimensional ttas material has an even number of electrons per unit cell with mathcalt this surface is effectively viewed as a spin triangular lattice magnet except with a fully gapped topological bulk we further propose this surface termination realizes a spinon fermi surface spin liquid with the surface fractionalization but with a nonexotic threedimensional bulk we analyze possible experimental consequences especially the surface spectroscopic measurements of the typeii terminated surface spin liquid less,Fractionalization on the Surface: Is Type-II Terminated $1T$-TaS$_2$ Surface an Anomalously Realized Spin Liquid?,"1 July, 2022"
854,David Liu,the pandora software development kit and algorithm libraries provide patternrecognition logic essential to the reconstruction of particle interactions in liquid argon time projection chamber detectors pandora is the primary event reconstruction software used at protodunesp a prototype for the deep underground neutrino experiment far detector protodunesp located at cern is exposed to a chargedparticle test beam this paper gives an overview of the pandora reconstruction algorithms and how they have been tailored for use at protodunesp in complex events with numerous cosmicray and beam background particles the simulated reconstruction and identification efficiency for triggered testbeam particles is above for the majority of particle type and beam momentum combinations specifically simulated gevc charged pions and protons are correctly reconstructed and identified with efficiencies of pm and pm respectively the efficiencies measured for testbeam data are shown to be within of those predicted by the simulation less,Reconstruction of interactions in the ProtoDUNE-SP detector with Pandora,"29 June, 2022"
855,David Liu,biobased energy particularly corn starchbased ethanol and other liquid renewable fuels are a major element of federal and state energy policies in the united states these policies are motivated by energy security and climate change mitigation objectives but corn ethanol does not substantially reduce greenhouse gas emissions when compared to petroleumbased fuels corn production also imposes substantial negative externalities eg nitrogen leaching higher food prices water scarcity and indirect land use change in this paper we utilize a partial equilibrium model of cornsoy production and trade to analyze the potential of reduced us demand for corn as a biobased energy feedstock to mitigate increases in nitrogen leaching crop production and land use associated with growing global populations and income from to we estimate that a demand reduction would sustain land use and nitrogen leaching below levels through the year and a reduction would do so through outcomes are similar across major watersheds where corn and soy are intensively farmed less,Reducing US Biofuels Requirements Mitigates Short-term Impacts of Global Population and Income Growth on Agricultural Environmental Outcomes,"28 June, 2022"
856,David Liu,we have identified xmm j at a distance of pc as a binary system consisting of a normal star and a probable dormant neutron star optical spectra exhibit a slightly evolved ftype single star displaying periodic doppler shifts with a day keplerian circular orbit with no indication of light from a secondary component optical and uv photometry reveal ellipsoidal variations with half the orbital period due to the tidal deformation of the f star the mass of the unseen companion is constrained to the range modot at confidence with the median of the mass distribution at modot the typical mass of known neutron stars a mainsequence star cannot masquerade as the dark companion the distribution of possible companion masses still allows for the possibility of a very massive white dwarf the companion itself could also be a close pair consisting of a white dwarf and an m star or two white dwarfs although the binary evolution that would lead to such a close triple system is unlikely similar ambiguities regarding the certain identification of a dormant neutron star are bound to affect most future discoveries of this type of noninteracting system if the system indeed contains a dormant neutron star it will become in the future a bright xray source and might even host a millisecond pulsar less,Probable Dormant Neutron Star in a Short-Period Binary System,"22 June, 2022"
857,David Liu,we report a precise measurement of the parityviolating asymmetry arm pv in the elastic scattering of longitudinally polarized electrons from rm ca we measure arm pv pm rm statpm rm syst parts per billion leading to an extraction of the neutral weak form factor frm w q fm pm rm statpm rm syst and the charge minus the weak form factor frm ch frm w pm the resulting neutron skin thickness rnrp pm rm exp pm rm modelfm is relatively thin yet consistent with many model calculations the combined crex and prex results will have implications for future energy density functional calculations and on the density dependence of the symmetry energy of nuclear matter less,Precision Determination of the Neutral Weak Form Factor of $^{48}$Ca,"16 June, 2022"
858,David Liu,plasma wakefield acceleration is a promising technology to reduce the size of particle accelerators use of high energy protons to drive wakefields in plasma has been demonstrated during run of the awake programme at cern protons of energy gev drove wakefields that accelerated electrons to gev in under m of plasma the awake collaboration is now embarking on run with the main aims to demonstrate stable accelerating gradients of gvm preserve emittance of the electron bunches during acceleration and develop plasma sources scalable to s of metres and beyond by the end of run the awake scheme should be able to provide electron beams for particle physics experiments and several possible experiments have already been evaluated this article summarises the programme of awake run and how it will be achieved as well as the possible application of the awake scheme to novel particle physics experiments less,The AWAKE Run 2 programme and beyond,"13 June, 2022"
859,David Liu,the phenix collaboration presents a systematic study of production from pp pal pau dau and heau collisions at sqrtsnn gev measurements were performed with different centrality selections as well as the total inelastic selection for all collision systems for collisions the nuclear modification factors rxa are consistent with unity for pt above gevc but exhibit an enhancement in peripheral collisions and a suppression in central collisions the enhancement and suppression characteristics are similar for all systems for the same centrality class it is shown that for highpt production the nucleons in the d and he interact mostly independently with the au nucleus and that the counter intuitive centrality dependence is likely due to a physical correlation between multiplicity and the presence of a hard scattering process these observations disfavor models where parton energy loss has a significant contribution to nuclear modifications in small systems nuclear modifications at lower pt resemble the cronin effect an increase followed by a peak in central or inelastic collisions and a plateau in peripheral collisions the peak height has a characteristic ordering by system size as pau dau heau pal for collisions with au ions current calculations based on initial state cold nuclear matter effects result in the opposite order suggesting the presence of other contributions to nuclear modifications in particular at lower pt less,"Systematic study of nuclear effects in $p$$+$Al, $p$$+$Au, $d$$+$Au, and $^{3}$He$+$Au collisions at $\sqrt{s_{_{NN}}}=200$ GeV using $π^0$ production","6 June, 2022"
860,David Liu,our understanding of the structure of the brain and its relationships with human traits is largely determined by how we represent the structural connectome standard practice divides the brain into regions of interest rois and represents the connectome as an adjacency matrix having cells measuring connectivity between pairs of rois statistical analyses are then heavily driven by the largely arbitrary choice of rois in this article we propose a novel tractographybased representation of brain connectomes which clusters fiber endpoints to define a data adaptive parcellation targeted to explain variation among individuals and predict human traits this representation leads to principal parcellation analysis ppa representing individual brain connectomes by compositional vectors building on a basis system of fiber bundles that captures the connectivity at the population level ppa reduces subjectivity and facilitates statistical analyses we illustrate the proposed approach through applications to data from the human connectome project hcp and show that ppa connectomes improve power in predicting human traits over stateoftheart methods based on classical connectomes while dramatically improving parsimony and maintaining interpretability our ppa package is publicly available on github and can be implemented routinely for diffusion image data less,PPA: Principal Parcellation Analysis for Brain Connectomes and Multiple Traits,"1 June, 2022"
861,David Liu,are the embeddings of a graphs degenerate core stable what happens to the embeddings of nodes in the degenerate core as we systematically remove periphery nodes by repeated peeling off kcores we discover three patterns wrt instability in degeneratecore embeddings across a variety of popular graph embedding algorithms and datasets we use regression to quantify the change point in graph embedding stability furthermore we present the stable algorithm which takes an existing graph embedding algorithm and makes it stable we show the effectiveness of stable in terms of making the degeneratecore embedding stable and still producing stateoftheart link prediction performance less,Identifying and Mitigating Instability in Embeddings of the Degenerate Core,"21 May, 2022"
862,David Liu,the electronion collider eic is a cuttingedge accelerator facility that will study the nature of the glue that binds the building blocks of the visible matter in the universe the proposed experiment will be realized at brookhaven national laboratory in approximately years from now with detector design and rd currently ongoing notably eic is one of the first largescale facilities to leverage artificial intelligence ai already starting from the design and rd phases the eic comprehensive chromodynamics experiment ecce is a consortium that proposed a detector design based on a t solenoid the eic detector proposal review concluded that the ecce design will serve as the reference design for an eic detector herein we describe a comprehensive optimization of the ecce tracker using ai the work required a complex parametrization of the simulated detector system our approach dealt with an optimization problem in a multidimensional design space driven by multiple objectives that encode the detector performance while satisfying several mechanical constraints we describe our strategy and show results obtained for the ecce tracking system the aiassisted design is agnostic to the simulation framework and can be extended to other subdetectors or to a system of subdetectors to further optimize the performance of the eic detector less,AI-assisted Optimization of the ECCE Tracking System at the Electron Ion Collider,"19 May, 2022"
863,David Liu,spontaneous lowfrequency oscillations on the order of several hertz are the drivers of many crucial processes in nature from bacterial swimming to mammal gaits the conversion of static energy inputs into slowly oscillating electrical and mechanical power is key to the autonomy of organisms across scales however the fabrication of slow artificial oscillators at micrometre scales remains a major roadblock towards the development of fullyautonomous microrobots here we report the emergence of a lowfrequency relaxation oscillator from a simple collective of active microparticles interacting at the airliquid interface of a peroxide drop their collective oscillations form chemomechanical and electrochemical limit cycles that enable the transduction of ambient chemical energy into periodic mechanical motion and onboard electrical currents surprisingly the collective can oscillate robustly even as more particles are introduced but only when we add a single particle with modified reactivity to intentionally break the systems permutation symmetry we explain such emergent order through a novel thermodynamic mechanism for asymmetryinduced order the energy harvested from the stabilized system oscillations enables the use of onboard electronic components which we demonstrate by cyclically and synchronously driving microrobotic arms this work highlights a new strategy for achieving lowfrequency oscillations at the microscale that are otherwise difficult to observe outside of natural systems paving the way for future microrobotic autonomy less,Emergent Microrobotic Oscillators via Asymmetry-Induced Order,"19 May, 2022"
864,Lueder Kahrs,"The laryngeal adductor reflex (LAR) is a vital reflex of the human larynx. LAR malfunctions may cause life-threatening aspiration events. An objective, noninvasive, and reproducible method for LAR assessment is still lacking. Stimulation of the larynx by droplet impact, termed Microdroplet Impulse Testing of the LAR (MIT-LAR), may remedy this situation. However, droplet instability and imprecise stimulus application thus far prevented MIT-LAR from gaining clinical relevance. We present a system comprising two alternative, custom-built stereo laryngoscopes, each offering a distinct set of properties, a droplet applicator module, and image/point cloud processing algorithms to enable a targeted, droplet-based LAR stimulation. Droplet impact site prediction (ISP) is achieved by droplet trajectory identification and spatial target reconstruction. The reconstruction and ISP accuracies were experimentally evaluated",Stereo laryngoscopic impact site prediction for droplet-based stimulation of the laryngeal adductor reflex,2021-08-06 00:00:00
865,Lueder Kahrs,"Hypothesis: This study compares the reaching ability of two classes of transcanal endoscopic ear surgery (TEES) instruments when operating on difficult to access anatomical targets; two novel instruments with steerable flexible tips (SFT-A and SFT-B) and suction capability are compared with standard commercially available tools.Background: TEES surgeons identified the need for a new surgical instrument that can enable accessibility of all areas visualized by the endoscope. This motivated the development of the two instrument prototypes.Methods: Six temporal bone models were 3D printed based on CT data from five cholesteatoma patients. Four anatomical targets were marked on each model. Using these targets, the reaching ability while using four standard TEES instruments were compared with the SFT-A and SFT-B prototypes by five surgeon participants. Results were analysed to compare success rates of ",A novel instrument for endoscopic ear surgery with a steerable flexible tip: a pediatric anatomical validation study,2021-07-07 00:00:00
866,Lueder Kahrs,"The consideration of predictive uncertainty in medical imaging with deep learning is of utmost importance. We apply estimation of both aleatoric and epistemic uncertainty by variational Bayesian inference with Monte Carlo dropout to regression tasks and show that predictive uncertainty is systematically underestimated. We apply  scaling with a single scalar value; a simple, yet effective calibration method for both types of uncertainty. The performance of our approach is evaluated on a variety of common medical regression data sets using different state-of-the-art convolutional network architectures. In our experiments,  scaling is able to reliably recalibrate predictive uncertainty. It is easy to implement and maintains the accuracy. Well-calibrated uncertainty in regression allows robust rejection of unreliable predictions or detection of out-of-distribution samples. Our source code is available at https://github.com/mlaves/well-calibrated-regression-uncertainty",Recalibration of Aleatoric and Epistemic Regression Uncertainty in Medical Imaging,2021-04-26 00:00:00
867,Lueder Kahrs,"It is not always possible to create linear access to the larynx using a rigid operating laryngoscope for microlaryngoscopy. In this study, we evaluate the usability of a novel curved surgical prototype with flexible instruments for the larynx (sMAC) in a simulation dummy and human body donor. In a user study (n = 6), head and neck surgeons as well as medical students tested the system for visualization quality and accessibility of laryngeal landmarks on an intubation dummy and human cadaver. A biopsy of the epiglottis was taken from the body donor. Photographic and time documentation was carried out.",Evaluation of a curved surgical prototype in a human larynx,2021-04-22 00:00:00
868,Lueder Kahrs,"Fast and accurate depth estimation is an essential task in computer-assisted surgery and robotics, especially for endoscopic and microscopic procedures. We propose a real-time stereo matching model using a staged, coarse-to-fine architecture to estimate disparity from medical stereo camera data with self-supervised learning. Our model processes images with a resolution of  pixels beyond 60 fps, with similar accuracy to the semi-global matching algorithm, and does not require any ground truth depth for training. We evaluated our model on two stereo endoscopic datasets from the literature. A mean absolute error below 1.5 mm and root mean square error below 1.9 mm were identified.",Real-Time Coarse-to-Fine Depth Estimation on Stereo Endoscopic Images With Self-Supervised Learning,2021-04-13 00:00:00
869,Lueder Kahrs,"Our concept is characterized by the use of bone cement, which enables fixation of a specific configuration for each individual surgical template. This well-established medical product was selected to ensure future intraoperative fabrication of the template under sterile conditions. For customization, a manually operated alignment device is proposed that temporarily defines the planned trajectory until the bone cement is hardened. Experiments (n  =  10) with half-skull phantoms were performed. Analysis of accuracy comprises targeting validations and experiments including drilling in bone substitutes.",Concept description and accuracy evaluation of a moldable surgical targeting system,2021/2
870,Lueder Kahrs,"Despite great efforts, transoral robotic laser surgery has not been established clinically. Patient benefits are yet to be proven to accept shortcomings of robotic systems. In particular, laryngeal reachability and transition from microscope to accurate endoscopic laser ablation have not been achieved. We have addressed those challenges with a highly integrated robotic endoscope for non-contact endolaryngeal laser surgery. The current performance status has been assessed in multi-level user studies. In addition, the system was deployed to an ex vivo porcine larynx. The robotic design comprises an extensible continuum manipulator with multifunctional tip. The latter features laser optics, stereo vision, and illumination. Vision-based performance assessment is derived from depth estimation and scene tracking. Novices and experts (n = 20) conducted teleoperated delineation tasks to mimic laser ablation of ",Preclinical performance evaluation of a robotic endoscope for non-contact laser surgery,2021/2
871,Lueder Kahrs,"The prototype was built from established medical devices, namely a hyperangulated videolaryngoscope and modified flexible instruments as well as three‐dimensional printed parts. Feasibility of laryngeal manipulation was evaluated in a user study (n = 19) with a porcine ex vivo laryngeal model. Using three different visualization technologies, the participants performed various fine motor skills tasks and rated the usability of the system on a 5‐point Likert scale.",Adding flexible instrumentation to a curved videolaryngoscope: a novel tool for laryngeal surgery,2021/2
872,Lueder Kahrs,"This work presents the design of a novel compliant steerable tip (CST) instrument to facilitate transcanal (or totally) endoscopic ear surgery (TEES). The evolution of the instrument’s design is shown, where prototypes were evaluated by surgeons and their feedback was used to inform the design changes for the next prototype iteration. The final prototype enables the surgeon to articulate the compliant tip to achieve the desired bending curvature while automatically locking in place and providing dissection and suction capabilities. Pre-clinical validation testing was performed in goat and human cadaver models by two surgeons who successfully removed an allograft from the middle ear. Time and the number of blockages while suctioning saline in both cadaver models were measured and compared with current instruments used during TEES. The CST took significantly less time to suction saline within a flooded ","Design, prototype development and pre-clinical validation of a novel instrument with a compliant steerable tip to facilitate endoscopic ear surgery",2021-01-02 00:00:00
873,Lueder Kahrs,"The consideration of predictive uncertainty in medical imaging with deep learning is of utmost importance. We apply estimation of predictive uncertainty by variational Bayesian inference with Monte Carlo dropout to regression tasks and show why predictive uncertainty is systematically underestimated. We suggest using {\em scaling} with a single scalar value; a simple, yet effective calibration method for both aleatoric and epistemic uncertainty. The performance of our approach is evaluated on a variety of common medical regression data sets using different state-of-the-art convolutional network architectures. In all experiments,  scaling is able to reliably recalibrate predictive uncertainty. It is easy to implement and maintains the accuracy. Well-calibrated uncertainty in regression allows robust rejection of unreliable predictions or detection of out-of-distribution samples. ",Well-calibrated regression uncertainty in medical imaging with deep learning,2020-09-21 00:00:00
874,Lueder Kahrs,"This work presents a droplet applicator module to generate stable droplets with different muzzle energies for the reproducible endoscopic stimulation of the laryngeal adductor reflex (LAR). The LAR is a protective reflex of the human larynx; an abnormal LAR performance may cause aspiration pneumonia. A pathological LAR can be detected by evaluating its onset latency. The reflex can be triggered by shooting a droplet onto the laryngeal mucosa, which is referred to as Microdroplet Impulse Testing of the LAR (MIT-LAR). Stimulation intensity variation is desired as the reflex threshold may vary inter-individually. The kinetic energy of a droplet after detachment from the nozzle, i.e., its muzzle energy, is considered an appropriate metric for the LAR stimulation intensity. In this work, a suitable nozzle channel geometry is identified based on the experimental evaluation of droplet formation using three different nozzle ",Droplet applicator module for reproducible and controlled endoscopic laryngeal adductor reflex stimulation,2020-07-07 00:00:00
875,Lueder Kahrs,"Four Euclidean distances between landmarks in the larynx and pharynx were analyzed based on CT data of 66 patients. Distance (1): labium inferius oris—posterior pharyngeal wall at the cervical vertebra C1 (atlas), anterior edge of the tuberculum anterius atlantis. Distance (2): posterior pharyngeal wall adjacent to C1—entrance of pyriform sinus. Distance (3): inferior edge of the uvula—superior edge of the epiglottis. Distance (4): base of the vallecula—posterior pharyngeal wall. The minimum angular field of view α required to observe the glottis with a rigid transoral laryngoscope was derived trigonometrically from distances (2) and (4).",Euclidean distances of laryngopharyngeal structures obtained from CT data for preclinical development of laryngoscopic devices,2019-12-19 00:00:00
876,Lueder Kahrs,noise in speckleprone optical coherence tomography tends to obfuscate important details necessary for medical diagnosis in this paper a denoising approach that preserves disease characteristics on retinal optical coherence tomography images in ophthalmology is presented by combining a deep convolutional autoencoder with a priorly trained resnet image classifier as regularizer the perceptibility of delicate details is encouraged and only informationless background noise is filtered out with our approach higher peak signaltonoise ratios with mathrmpsnr mathrmdb and higher classification accuracy of mathrmacc can be achieved for denoised images compared to stateoftheart denoising with mathrmpsnr mathrmdb or mathrmacc depending on the method it is shown that regularized autoencoders are capable of denoising retinal oct images without blurring details of diseases less,Semantic denoising autoencoders for retinal optical coherence tomography,"23 March, 2019"
877,Lueder Kahrs,purpose the facial recess is a delicate structure that must be protected in minimally invasive cochlear implant surgery current research estimates the drill trajectory by using endoscopy of the unique mastoid patterns however missing depth information limits available features for a registration to preoperative ct data therefore this paper evaluates oct for enhanced imaging of drill holes in mastoid bone and compares oct data to original endoscopic images methods a catheterbased oct probe is inserted into a drill trajectory of a mastoid phantom in a translationrotation manner to acquire the inner surface state the images are undistorted and stitched to create volumentric data of the drill hole the mastoid cell pattern is segmented automatically and compared to ground truth results the mastoid pattern segmented on images acquired with oct show a similarity of j to ground truth based on endoscopic images and measured with the jaccard metric leveraged by additional depth information automated segmentation tends to be more robust and failsafe compared to endoscopic images conclusion the feasibility of using a clinically approved oct probe for imaging the drill hole in cochlear implantation is shown the resulting volumentric images provide additional information on the shape of caveties in the bone structure which will be useful for imagetopatient registration and to estimate the drill trajectory this will be another step towards safe minimally invasive cochlear implantation less,Endoscopic vs. volumetric OCT imaging of mastoid bone structure for pose estimation in minimally invasive cochlear implant surgery,"23 March, 2019"
878,Moshe Gabel,"Approaches for evaluating functions over distributed data streams are increasingly important as data sources become more geographically distributed. However, existing methodologies are limited to small classes of functions, requiring non-trivial effort and substantial mathematical sophistication to tailor them to new functions.",Automon: Automatic distributed monitoring for arbitrary multivariate functions,2022-06-10 00:00:00
879,Moshe Gabel,"Methods and systems for data management are described, particularly for processing global queries. Each global query includes a user-defined query constraint value, such as laxity or query response time limit. The query receiving node maintains a copy of the previously updated data from all of its children node. The query receiving node first searches for the requested query data in its local data storage to minimize children node query. If any portion of the requested data in the local data storage fails to meet the query constraint value, then the child node from which the data came from is tasked with recursively executing the global query.",Method to improve global query performance in an edge network,2022-06-09 00:00:00
880,Moshe Gabel,"DNN inference is time-consuming and resource hungry. Partitioning and early exit are ways to run DNNs efficiently on the edge. Partitioning balances the computation load on multiple servers, and early exit offers to quit the inference process sooner and save time. Usually, these two are considered separate steps with limited flexibility. This work combines partitioning and early exit and proposes a performance model to estimate both inference latency and accuracy. We use this performance model to offer the best partitioned/early exit DNN based on deployment information and user preferences. Our experiments show that the flexibility in number and position of partitioning points and placement on available devices plays an important role in deciding the best output. In the future, we plan to turn this work into a"" one-click"" system to train and optimize models for edge computing.",Combining DNN partitioning and early exit,2022-04-05 00:00:00
881,Moshe Gabel,"Pervasive sensing using wearables for health monitoring presents a promising and unique opportunity to widely manage illnesses and conditions. To better understand the capabilities and limitations of using wearable devices for health monitoring, systems need to be developed and studies conducted. We conducted one such study for monitoring patients with Chronic Obstructive Pulmonary Disease (COPD), in which we aim to understand the disease and predict patient outcomes. However, despite a carefully well-planned and well-conducted study that resulted in a very large dataset, some non-obvious design oversights meant the data was much less useful. We analyze the shortcomings of our study to construct lessons and concrete actions to avoid these pitfalls. We ratify these lessons by briefly discussing a second iteration of our study, in which we apply these lessons and obtain much better outcomes. ",Hindsight is 20/20: Retrospective lessons for conducting longitudinal wearable sensing studies,2022-03-21 00:00:00
882,Moshe Gabel,"Acoustic speech characteristics have previously been identified as possible indicators of respiratory disease when recorded in controlled lab settings. However, the ability to measure and leverage these indicators during people’s everyday lives has been largely under-explored. In this study, we use continuous audio data from smartwatches worn by individuals suffering from COPD, as well as symptom information through daily self-reports. By applying pre-trained models for voice activity detection and speaker verification models, we are able to isolate moments of the user’s own speech and extract important speech features. We then use those features in an isolation forest outlier detector to discriminate between days with normal and worsening symptoms, achieving an AUC of nearly 0.60 on this challenging problem.",Unobtrusive monitoring of COPD patients using speech collected from smartwatches in the wild,2022-03-21 00:00:00
883,Moshe Gabel,"Containers, originally designed for cloud environments, are increasingly popular for provisioning workers outside the cloud, for example in mobile and edge computing. These settings, however, bring new challenges: high latency links, limited bandwidth, and resource-constrained workers. The result is longer provisioning times when deploying new workers or updating existing ones, much of it due to network traffic.",Starlight: Fast Container Provisioning on the Edge and over the {WAN},2022
884,Moshe Gabel,"Chronic obstructive pulmonary disease (COPD) is one of the leading causes of human mortality worldwide. Traditionally, estimating COPD severity has been done in controlled clinical conditions using cough sounds, respiration, and heart rate variability, with the latter reporting insights on the autonomic dysfunction caused by the disease. Advancements in remote monitoring and wearable device technologies, in turn, have allowed for remote COPD monitoring in daily life conditions. In this study, we explore the potential for predicting COPD severity and exacerbation using a low-cost wearable device that measures heart rate and activity data. We collected smartwatch sensor data from 35 COPD patients over a period of three months. Our evaluation shows that future trajectory of the disease can be predicted using only the first few days of continuous unobtrusive wearable data collected from COPD patients.",Remote copd severity and exacerbation detection using heart rate and activity data measured from a wearable device,2021-11-01 00:00:00
885,Moshe Gabel,"Continuous monitoring of cough may provide insights into the health of individuals as well as the effectiveness of treatments. Smart-watches, in particular, are highly promising for such monitoring: they are inexpensive, unobtrusive, programmable, and have a variety of sensors. However, current mobile cough detection systems are not designed for smartwatches, and perform poorly when applied to real-world smartwatch data since they are often evaluated on data collected in the lab.In this work we propose CoughWatch, a lightweight cough detector for smartwatches that uses audio and movement data for in-the-wild cough detection. On our in-the-wild data, CoughWatch achieves a precision of 82% and recall of 55%, compared to 6% precision and 19% recall achieved by the current state-of-the-art approach. ",Coughwatch: Real-World Cough Detection using Smartwatches,2021-06-06 00:00:00
886,Moshe Gabel,"Tracking the value of a function computed from a dynamic, distributed data stream is a challenging problem with many real-world applications. Continuously forwarding data updates can be costly, yet complex functions are difficult to evaluate when data is not centralized. One general approach to continuous distributed monitoring is the Geometric Monitoring (GM) family of techniques. GM reduces the functional monitoring problem to a set of local constraints that each node checks locally, and uses a simple protocol to update those constraints as needed.While most work on GM focuses on reducing the number of messages exchanged by the common GM protocol, with one recent notable exception, there has been little attention to reducing the size of those messages, which impacts bandwidth.We propose the Distance Scheme: a novel bandwidth-efficient variation of the GM protocol that reduces the size of most ",A Distance-Based Scheme for Reducing Bandwidth in Distributed Geometric Monitoring,2021-04-19 00:00:00
887,Moshe Gabel,"Peripheral blood oxygen saturation (SpO 2 ) is a vital health signal with many clinical applications. Modern wrist-worn devices, such as the Apple Watch, FitBit, and Samsung Gear, have pulse oximeter sensors, making them theoretically capable of measuring SpO 2 . However, current techniques for SpO 2 measurements using pulse oximeter sensors are based on readings taken from the fingertip. Readings collected from the wrist are unreliable and often inaccurate, due to motion and insufficient skin contact. Enabling accurate oxygen saturation monitoring on wearable devices would allow continuous health monitoring and open up new avenues of research. In this work, we explore the reliability of SpO 2 measurements from the wrist. Using a custom wrist-worn pulse oximeter, we find that existing algorithms used in traditional fingertip SpO 2 sensors are a poor match for taking measurements from the wrist and",WristO2: Reliable peripheral oxygen saturation readings from wrist-worn pulse oximeters,2021-03-22 00:00:00
888,Moshe Gabel,"Smartwatches can collect heart rate data unobtrusively and continuously, making them a promising tool for conducting long term studies, monitoring chronic conditions, and providing timely intervention. Healthcare applications, however, require us to understand the reliability of collected readings, both in terms of quality and quantity. The accuracy of optical heart rate (HR) measurements has been studied extensively in recent years, identifying several common causes of errors. For example, previous research has demonstrated that inaccurate HR readings occur more frequently in dark skin as compared to light skin due to melanin absorption. Smartwatches therefore implement a confidence mechanism to estimate reliability of HR readings. We study the effect of skin tone on the reliability of confidence estimation of seven consumer-grade WearOS smartwatches. We find that some watches systematically","Skin tone, confidence, and data quality of heart rate sensing in WearOS smartwatches",2021-03-22 00:00:00
889,Moshe Gabel,"Deep reinforcement learning (RL) has made groundbreaking advancements in robotics, data center management and other applications. Unfortunately, system-level bottlenecks in RL workloads are poorly understood; we observe fundamental structural differences in RL workloads that make them inherently less GPU-bound than supervised learning (SL). To explain where training time is spent in RL workloads, we propose RL-Scope, a cross-stack profiler that scopes low-level CPU/GPU resource usage to high-level algorithmic operations, and provides accurate insights by correcting for profiling overhead. Using RL-Scope, we survey RL workloads across its major dimensions including ML backend, RL algorithm, and simulator. For ML backends, we explain a 2.3× difference in runtime between equivalent PyTorch and TensorFlow algorithm implementations, and identify a bottleneck rooted in overly abstracted algorithm implementations.",RL-Scope: Cross-stack Profiling for Deep Reinforcement Learning Workloads,2021-03-15 00:00:00
890,Moshe Gabel,"This paper explores the CO2 footprint of IoT applications by using system dynamics modeling to estimate the CO2 emissions over time from a wireless video analytics application. We model the impact of the application design and the mobile infrastructure on the short and long term emissions produced by running the application on both cloud and edge computing infrastructures. Our analysis shows that the base station radio and the wide-area data network are major contributors of CO2 emissions. We find that CO2 emissions can be reduced by 50% by placing edge centers near the base stations, exploiting new features of the 5G mobile network, and scheduling data uploads judiciously. We also analyze the long term effects of application design choices and increased user base on carbon emissions.",Sustainable Computing on the Edge: A System Dynamics Perspective,2021-02-24 00:00:00
891,Moshe Gabel,deep reinforcement learning rl has made groundbreaking advancements in robotics data center management and other applications unfortunately systemlevel bottlenecks in rl workloads are poorly understood we observe fundamental structural differences in rl workloads that make them inherently less gpubound than supervised learning sl to explain where training time is spent in rl workloads we propose rlscope a crossstack profiler that scopes lowlevel cpugpu resource usage to highlevel algorithmic operations and provides accurate insights by correcting for profiling overhead using rlscope we survey rl workloads across its major dimensions including ml backend rl algorithm and simulator for ml backends we explain a times difference in runtime between equivalent pytorch and tensorflow algorithm implementations and identify a bottleneck rooted in overly abstracted algorithm implementations for rl algorithms and simulators we show that onpolicy algorithms are at least times more simulationbound than offpolicy algorithms finally we profile a scaleup workload and demonstrate that gpu utilization metrics reported by commonly used tools dramatically inflate gpu usage whereas rlscope reports true gpubound time rlscope is an opensource tool available at httpsgithubcomuoftecosystemrlscope less,RL-Scope: Cross-Stack Profiling for Deep Reinforcement Learning Workloads,"4 March, 2021"
892,Moshe Gabel,can deep neural networks learn to solve any task and in particular problems of high complexity this question attracts a lot of interest with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability in this work we offer a different perspective on this question given the common assumption that textitnp neq textitconp we prove that any polynomialtime sample generator for an textitnphard problem samples in fact from an easier subproblem we empirically explore a case study conjunctive query containment and show how common data generation techniques generate biased datasets that lead practitioners to overestimate model accuracy our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems the reason being the difficulty of generating sufficiently large and unbiased training sets less,"It's Not What Machines Can Learn, It's What We Cannot Teach","28 June, 2020"
893,Stephen Cook,abridged we describe here the most ambitious survey currently planned in the optical the large synoptic survey telescope lsst a vast array of science will be enabled by a single widedeepfast sky survey and lsst will have unique survey capability in the faint time domain the lsst design is driven by four main science themes probing dark energy and dark matter taking an inventory of the solar system exploring the transient optical sky and mapping the milky way lsst will be a widefield groundbased system sited at cerro pachn in northern chile the telescope will have an m m effective primary mirror a deg field of view and a gigapixel camera the standard observing sequence will consist of pairs of second exposures in a given field with two such visits in each pointing in a given night with these repeats the lsst system is capable of imaging about square degrees of sky in a single filter in three nights the typical pointsource depth in a single visit in r will be sim ab the project is in the construction phase and will begin regular survey operations by the survey area will be contained within deg with circ and will be imaged multiple times in six bands ugrizy covering the wavelength range nm about of the observing time will be devoted to a deepwidefast survey mode which will uniformly observe a deg region about times summed over all six bands during the anticipated years of operations and yield a coadded map to rsim the remaining of the observing time will be allocated to projects such as a very deep and fast time domain survey the goal is to make lsst data products including a relational database of about trillion observations of billion objects available to the public and scientists around the world less,LSST: from Science Drivers to Reference Design and Anticipated Data Products,"23 May, 2018"
894,Stephen Cook,spectral molecular imaging is a new imaging technique able to discriminate and quantify different components of tissue simultaneously at high spatial and high energy resolution our mars scanner is an xray based small animal ct system designed to be used in the diagnostic energy range to kev in this paper we demonstrate the use of the mars scanner equipped with the medipixrx spectroscopic photonprocessing detector to discriminate fat calcium and water in tissue we present data collected from a sample of lamb meat including bone as an illustrative example of human tissue imaging the data is analyzed using our d algebraic reconstruction algorithm marsart and by material decomposition based on a constrained linear least squares algorithm the results presented here clearly show the quantification of lipidlike waterlike and bonelike components of tissue however it is also clear to us that better algorithms could extract more information of clinical interest from our data because we are one of the first to present data from multienergy photonprocessing small animal ct systems we make the raw partial and fully processed data available with the intention that others can analyze it using their familiar routines the raw partially processed and fully processed data of lamb tissue along with the phantom calibration data can be found at httphdlhandlenet less,MARS spectral molecular imaging of lamb tissue: data collection and image analysis,"23 January, 2014"
895,Stephen Cook,removed by arxiv administration this article was plagiarized directly from stephen cooks description of the problem for the clay mathematics institute see httpgaussclaymathorgmillenniumpvsnppvsnppdf for the original text less,The P versus NP Problem,"21 January, 2010"
896,Stephen Cook,the development of hybrid cmos detectors hcds for xray telescope focal planes will place them in con tention with ccds on future satellite missions due to their faster frame rates flexible readout scenarios lower power consumption and inherent radiation hardness ccds have been used with great success on the current generation of xray telescopes eg chandra xmm suzaku and swift however their bucketbrigade readout architecture which transfers charge across the chip with discrete component readout electronics results in clockrate limited readout speeds that cause pileup saturation of bright sources and an inherent susceptibility to radiation induced displacement damage that limits mission lifetime in contrast hcds read pixels with low power onchip multiplexer electronics in a random access fashion faster frame rates achieved with multioutput readout design will allow the next generations larger effective area telescopes to observe bright sources free of pileup radiation damaged lattice sites effect a single pixel instead of an entire row random access multioutput readout will allow for novel readout modes such as simultaneous brightsourcefastwholechipslow readout in order for hcds to be useful as xray detectors they must show noise and energy resolution performance similar to ccds while retaining advantages inherent to hcds we will report on readnoise conversion gain and energy resolution measurements of an xray enhanced teledyne hawaiirg hrg hcd and describe techniques of hrg data reduction less,Measurements of Si Hybrid CMOS X-Ray Detector Characteristics,"15 September, 2009"
897,Stephen Cook,tephen cook posited sat is npcomplete in if sat is npcomplete then as is generally accepted any polynomial solution of it must also present a polynomial solution of all np decision problems it is here argued however that np is not of necessity equivalent to p where it is shown that sat is contained in p this due to a paradox of nature addressed by both godel and russell in regards to the pnp system in total less,Does NP not equal P?,"11 September, 2002"
898,Derek Corneil,"In this paper, we consider the problem of the recognition of various kinds of orderings produced by graph searches. To this aim, we introduce a new framework, the Tie-Breaking Label Search (TBLS), in order to handle a broad variety of searches. This new model is based on partial orders defined on the label set and it unifies the General Label Search (GLS) formalism of Krueger et al. (2011), and the “pattern-conditions” formalism of Corneil and Krueger (2008). It allows us to derive some general properties including new pattern-conditions (yielding memory-efficient certificates) for many usual searches, including BFS, DFS, LBFS and LDFS. Furthermore, the new model allows easy expression of multi-sweep uses of searches that depend on previous (search) orderings of the graph’s vertex set.",A tie-break model for graph search,30 January 2016
899,Derek Corneil,"In this paper we study how graph searching on a cocomparability graph $G$ can be used to produce cocomp orderings (i.e., orderings that are linear extensions of some transitive orientation of $\overline{G}$) that yield simple algorithms for various intractable problems in general. Such techniques have been used to find a simple certifying algorithm for the minimum path cover problem. In particular we present a characterization of the searches that preserve cocomp orderings when used as a “$^+$” sweep. This allows us to present a toolbox of different graph searches and a framework to solve various problems on cocomparability graphs. We illustrate these techniques by describing a very simple certifying algorithm for the maximum independent set problem as well as a simple permutation graph recognition algorithm.",On the Power of Graph Searching for Cocomparability Graphs,2016
900,Derek Corneil,"The notion of searching a graph, in particular visiting each vertex in a systematic preordained fashion, is as old as graph theory itself. Indeed, Euler’s paper in 1736 [13] presented conditions on the vertex degrees of a graph that would certify the presence or absence of a path (or circuit) of edges visiting each edge exactly once. Later it was shown by Fleury that an easy algorithm to find such a path (or circuit) can be achieved using depth-first search (DFS) [14]. In the late nineteenth century, C. P. Trémaux [22] and G. Tarry [28] presented DFS-based algorithms for maze traversal; similarly, breadth-first search (BFS) algorithms were used to find the shortest possible successful maze traversals.",Unified View of Graph Searching and LDFS-Based Certifying Algorithms,22 April 2016
901,Derek Corneil,"A cocomparability graph is a graph whose complement admits a transitive orientation. An interval graph is the intersection graph of a family of intervals on the real line. In this paper we investigate the relationships between interval and cocomparability graphs. This study is motivated by recent results Corneil,Dalton, Habib (2013) and Dusart, Habib (2016) and that show that for some problems, the algorithm used on interval graphs can also be used with small modifications on cocomparability graphs. Many of these algorithms are based on graph searches that preserve cocomparability orderings.",Maximal cliques structure for cocomparability graphs and applications,7 Nov 2016
902,Derek Corneil,"Asteroidal Triple-free (AT-free) graphs have received considerable attention due to their inclusion of various important graphs families, such as interval and cocomparability graphs. The asteroidal number of a graph is the size of a largest subset of vertices such that the removal of the closed neighborhood of any vertex in the set leaves the remaining vertices of the set in the same connected component. (AT-free graphs have asteroidal number at most 2.) In this article, we characterize graphs of bounded asteroidal number by means of a vertex elimination ordering, thereby solving a long-standing open question in algorithmic graph theory. Similar characterizations are known for chordal, interval, and cocomparability graphs.",Vertex Ordering Characterizations of Graphs of Bounded Asteroidal Number,01 April 2014
903,Derek Corneil,"For graph $G(V,E)$, a minimum path cover (MPC) is a minimum cardinality set of vertex disjoint paths that cover $V$ (i.e., every vertex of $G$ is in exactly one path in the cover). This problem is a natural generalization of the Hamiltonian path problem. Cocomparability graphs (the complements of graphs that have an acyclic transitive orientation of their edge sets) are a well studied subfamily of perfect graphs that includes many popular families of graphs such as interval, permutation, and cographs. Furthermore, for every cocomparability graph $G$ and acyclic transitive orientation of the edges of $\overline{G}$ there is a corresponding poset $P_G$; it is easy to see that an MPC of $G$ is a linear extension of $P_G$ that minimizes the bump number of $P_G$. Although there are directly graph-theoretical MPC algorithms (i.e., algorithms that do not rely on poset formulations) for various subfamilies of cocomparability graphs, notably interval graphs, until now all MPC algorithms for cocomparability graphs themselves have been based on the bump number algorithms for posets. In this paper we present the first directly graph-theoretical MPC algorithm for cocomparability graphs; this algorithm is based on two consecutive graph searches followed by a certifying algorithm. Surprisingly, except for a lexicographic depth first search (LDFS) preprocessing step, this algorithm is identical to the corresponding algorithm for interval graphs. The running time of the algorithm is $O({\rm min}(n^2, n + {\rm mloglogn}))$, with the nonlinearity coming from LDFS.",LDFS-Based Certifying Algorithm for the Minimum Path Cover Problem on Cocomparability Graphs,2013
904,Derek Corneil,a cocomparability graph is a graph whose complement admits a transitive orientation an interval graph is the intersection graph of a family of intervals on the real line in this paper we investigate the relationships between interval and cocomparability graphs this study is motivated by recent results corneildalton habib and dusart habib and that show that for some problems the algorithm used on interval graphs can also be used with small modifications on cocomparability graphs many of these algorithms are based on graph searches that preserve cocomparability orderings first we propose a characterization of cocomparability graphs via a lattice structure on the set of their maximal cliques using this characterization we can prove that every maximal interval subgraph of a cocomparability graph g is also a maximal chordal subgraph of g although the size of this lattice of maximal cliques can be exponential in the size of the graph it can be used as a framework to design and prove algorithms on cocomparability graphs in particular we show that a new graph search namely local maximal neighborhood search localmns leads to an onmlogn time algorithm to find a maximal interval subgraph of a cocomparability graph similarly we propose a linear time algorithm to compute all simplicial vertices in a cocomparability graph in both cases we improve on the current state of knowledge less,Maximal cliques structure for cocomparability graphs and applications,"7 November, 2016"
905,Derek Corneil,circle graphs are the intersection graphs of chords in a circle this paper presents the first subquadratic recognition algorithm for the class of circle graphs our algorithm is on m times the inverse ackermann function n m whose value is smaller than for any practical graph the algorithm is based on a new incremental lexicographic breadthfirst search characterization of circle graphs and a new efficient datastructure for circle graphs both developed in the paper the algorithm is an extension of a split decomposition algorithm with the same running time developed by the authors in a companion paper less,Practical and Efficient Circle Graph Recognition,"31 October, 2012"
906,Derek Corneil,given a graph g the longest path problem asks to compute a simple path of g with the largest number of vertices this problem is the most natural optimization version of the well known and well studied hamiltonian path problem and thus it is nphard on general graphs however in contrast to the hamiltonian path problem there are only few restricted graph families such as trees and some small graph classes where polynomial algorithms for the longest path problem have been found recently it has been shown that this problem can be solved in polynomial time on interval graphs by applying dynamic programming to a characterizing ordering of the vertices of the given graph citelongestintalgo thus answering an open question in the present paper we provide the first polynomial algorithm for the longest path problem on a much greater class namely on cocomparability graphs our algorithm uses a similar but essentially simpler dynamic programming approach which is applied to a lexicographic depth first search ldfs characterizing ordering of the vertices of a cocomparability graph therefore our results provide evidence that this general dynamic programming approach can be used in a more general setting leading to efficient algorithms for the longest path problem on greater classes of graphs ldfs has recently been introduced in citecorneilldfs since then a similar phenomenon of extending an existing interval graph algorithm to cocomparability graphs by using an ldfs preprocessing step has also been observed for the minimum path cover problem citecorneilmpc therefore more interestingly our results also provide evidence that cocomparability graphs present an interval graph structure when they are considered using an ldfs ordering of their vertices which may lead to other new and more efficient combinatorial algorithms less,A Simple Polynomial Algorithm for the Longest Path Problem on Cocomparability Graphs,"26 April, 2010"
907,Derek Corneil,networks have been used to model many realworld phenomena to better understand the phenomena and to guide experiments in order to predict their behavior since incorrect models lead to incorrect predictions it is vital to have a correct model as a result new techniques and models for analyzing and modeling realworld networks have recently been introduced one example of large and complex networks involves proteinprotein interaction ppi networks we demonstrate that the currently popular scalefree model of ppi networks fails to fit the data in several respects we show that a random geometric model provides a much more accurate model of the ppi data less,Modeling Interactome: Scale-Free or Geometric?,"16 April, 2004"
908,Eugene Fiume,"A system and method for triggering animated paralingual behavior from dialogue. The method including: receiving a corpus of dialogue including a plurality of samples, each sample including dialogue with aligned sequences of phonemes or sub-phonemes; extracting properties of measured quantities for each sample; generating a statistical profile by statistically classifying the extracted properties as a function of each sequence of phonemes or sub-phonemes; receiving a stream of phonemes or sub-phonemes and triggering paralingual behavior when the properties of any of the phonemes or sub-phonemes deviate from the statistical profile beyond a predetermined threshold; outputting the triggered paralingual behavior for animation",System and method for triggering animated paralingual behavior from dialogue,2022-02-17 00:00:00
909,Eugene Fiume,"A system and method for animated lip synchronization. The method includes: capturing speech input; parsing the speech input into phenomes; aligning the phonemes to the corresponding portions of the speech input; mapping the phonemes to visemes; synchronizing the visemes into viseme action units, the viseme action units comprising jaw and lip contributions for each of the phonemes; and outputting the viseme action units.",System and method for animated lip synchronization,2021-05-13 00:00:00
910,Eugene Fiume,"Realistic computer graphics will change the way people think and communicate. Achieving deeper success as a ubiquitous medium will require a more resonant understanding of visual modelling that must embrace mathematical, philosophical, cultural, perceptual and social aspects. With an interleaved understanding, people will be able to create visual ontologies that better align to their expressive needs. In turn, this will naturally lead to ubiquitous supporting technologies. First we need good visual models. A model induces an ontology of things that inevitably omits aspects of the phenomenon, whether desired or not. Thus modelling a model’s incompleteness is crucial, for it allows us to account for artifacts, errors, and ontological surprises such as the “uncanny valley”. Over the years, my choice of tools to model models has been mathematics. In this paper, I will speak to how little progress we have made and ",Visual models and ontologies,2020-11-25 00:00:00
911,Eugene Fiume,"A system and method for animated lip synchronization. The method includes: capturing speech input; parsing the speech input into phenomes; aligning the phonemes to the corresponding portions of the speech input; mapping the phonemes to visemes; synchronizing the visemes into viseme action units, the viseme action units comprising jaw and lip contributions for each of the phonemes; and outputting the viseme action units.",System and method for animated lip synchronization,2020-11-17 00:00:00
912,Eugene Fiume,"Cyberpunk 2077 is a highly anticipated massive open-world video game, with a complex, branching narrative. This talk details new research and innovative workflow contributions, developed by jali, toward the generation of an unprecedented number of hours of realistic, expressive speech animation in ten languages, often with multiple languages interleaved within individual sentences. The speech animation workflow is largely automatic but remains under animator control, using a combination of audio and tagged text transcripts. We use insights from anatomy, perception, and the psycho-linguistic literature to develop independent and combined language models that drive procedural animation of the mouth and paralingual (speech supportive non-verbal expression) motion of the neck, brows and eyes. Directorial tags in the speech transcript further enable the integration of performance capture driven facial",JALI-Driven Expressive Facial Animation and Multilingual Speech in Cyberpunk 2077,2020-08-17 00:00:00
913,Eugene Fiume,"Computer implemented method for rendering an image of a three-dimensional scene on an image plane by encoding at least a luminosity in the image plane by a luminosity function. The value of the luminosity can be computed at substantially each point of the image plane by using a set of stored input data describing the scene. The method includes constructing the luminosity function as equivalent to a first linear combination involving the functions of a first set of functions, and computing at least the value of the coefficients of the first linear combination, by solving a first linear system, obtained by using at least the functions of the first linear combination, at least a subset of the first subset of the image plane, and the luminosity at the points of said subset. The method further includes storing the value of the coefficients of the first linear combination and at least the information needed to associate each coefficient to the ",Hölder adaptive image synthesis,2019-11-12 00:00:00
914,Eugene Fiume,"Although geometry arising ""in the wild"" most often comes in the form of a surface representation, a plethora of geometrical and physical applications require the construction of volumetric embeddings either of the geometry itself or the domain surrounding it. Cartesian cut-cell-based mesh generation provides an attractive solution in which volumetric elements are constructed from the intersection of the input surface geometry with a uniform or adaptive hexahedral grid. This choice, especially common in computational fluid dynamics, has the potential to efficiently generate accurate, surface-conforming cells; unfortunately, current solutions are often slow, fragile, or cannot handle many common topological situations. We therefore propose a novel, robust cut-cell construction technique for triangle surface meshes that explicitly computes the precise geometry of the intersection cells, even on meshes that are open or non",Mandoline: robust cut-cell generation for arbitrary triangle meshes,2019-11-08 00:00:00
915,Eugene Fiume,"Motion capture is often retargeted to new, and sometimes drastically different, characters. When the characters take on realistic human shapes, however, we become more sensitive to the motion looking right. This means adapting it to be consistent with the physical constraints imposed by different body shapes. We show how to take realistic 3D human shapes, approximate them using a simplified representation, and animate them so that they move realistically using physically‐based retargeting. We develop a novel spacetime optimization approach that learns and robustly adapts physical controllers to new bodies and constraints. The approach automatically adapts the motion of the mocap subject to the body shape of a target subject. This motion respects the physical properties of the new body and every body shape results in a different and appropriate movement. This makes it easy to create a varied set of ",Robust Physics‐based Motion Retargeting with Realistic Body Shapes,2018/12
916,Eugene Fiume,"This down-to-earth introduction to computation makes use of the broad array of techniques available in the modern computing environment. A self-contained guide for engineers and other users of computational methods, it has been successfully adopted as a text in teaching the next generation of mathematicians and computer graphics majors.","An introduction to scientific, symbolic, and graphical computation",2018-10-08 00:00:00
917,Eugene Fiume,radiance is widely regarded as the principal quantity in light transport theory yet the concept of radiance in use today has remained mostly unchanged since lamberts work in the th century his formulation of the measurement of light intensity is based on classical differentials and is known to suffer from several theoretical and practical limitations after tracing the historic development of radiance and its shortcomings we provide a modern formulation of light intensity measurements that models radiance as a differential form we demonstrate the utility of this use of exterior calculus for questions in light transport theory by rigorously deriving the cosine term and the area formulation without the need for postulates or heuristic arguments the formulation of radiance as a differential form introduced in this paper hence provides the first step towards a modern theory of light transport less,On the Mathematical Formulation of Radiance,"20 May, 2012"
918,Geoffrey Hinton,recent progress in medical artificial intelligence ai has delivered systems that can reach clinical expert level performance however such systems tend to demonstrate suboptimal outofdistribution performance when evaluated in clinical settings different from the training environment a common mitigation strategy is to develop separate systems for each clinical setting using sitespecific data however this quickly becomes impractical as medical data is timeconsuming to acquire and expensive to annotate thus the problem of dataefficient generalization presents an ongoing difficulty for medical ai development although progress in representation learning shows promise their benefits have not been rigorously studied specifically for outofdistribution settings to meet these challenges we present remedis a unified representation learning strategy to improve robustness and dataefficiency of medical imaging ai remedis uses a generic combination of largescale supervised transfer learning with selfsupervised learning and requires little taskspecific customization we study a diverse range of medical imaging tasks and simulate three realistic application scenarios using retrospective data remedis exhibits significantly improved indistribution performance with up to relative improvement in diagnostic accuracy over a strong supervised baseline more importantly our strategy leads to strong dataefficient generalization of medical imaging ai matching strong supervised baselines using between to of retraining data across tasks these results suggest that remedis can significantly accelerate the lifecycle of medical imaging ai development thereby presenting an important step forward for medical imaging ai to deliver broad impact less,Robust and Efficient Medical Imaging with Self-Supervision,"3 July, 2022"
919,Geoffrey Hinton,we present pixseq a simple and generic framework for object detection unlike existing approaches that explicitly integrate prior knowledge about the task we cast object detection as a language modeling task conditioned on the observed pixel inputs object descriptions eg bounding boxes and class labels are expressed as sequences of discrete tokens and we train a neural network to perceive the image and generate the desired sequence our approach is based mainly on the intuition that if a neural network knows about where and what the objects are we just need to teach it how to read them out beyond the use of taskspecific data augmentations our approach makes minimal assumptions about the task yet it achieves competitive results on the challenging coco dataset compared to highly specialized and well optimized detection algorithms less,Pix2seq: A Language Modeling Framework for Object Detection,"27 March, 2022"
920,Geoffrey Hinton,deep neural networks dnns are powerful blackbox predictors that have achieved impressive performance on a wide variety of tasks however their accuracy comes at the cost of intelligibility it is usually unclear how they make their decisions this hinders their applicability to high stakes decisionmaking domains such as healthcare we propose neural additive models nams which combine some of the expressivity of dnns with the inherent intelligibility of generalized additive models nams learn a linear combination of neural networks that each attend to a single input feature these networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output our experiments on regression and classification datasets show that nams are more accurate than widely used intelligible models such as logistic regression and shallow decision trees they perform similarly to existing stateoftheart generalized additive models in accuracy but are more flexible because they are based on neural nets instead of boosted trees to demonstrate this we show how nams can be used for multitask learning on synthetic data and on the compas recidivism data due to their composability and demonstrate that the differentiability of nams allows them to train more complex interpretable models for covid less,Neural Additive Models: Interpretable Machine Learning with Neural Nets,"24 October, 2021"
921,Geoffrey Hinton,effective training of deep neural networks can be challenging and there remain many open questions on how to best learn these models recently developed methods to improve neural network training examine teaching providing learned information during the training process to improve downstream model performance in this paper we take steps towards extending the scope of teaching we propose a flexible teaching framework using commentaries learned metainformation helpful for training on a particular task we present gradientbased methods to learn commentaries leveraging recent work on implicit differentiation for scalability we explore diverse applications of commentaries from weighting training examples to parameterising labeldependent data augmentation policies to representing attention masks that highlight salient image regions we find that commentaries can improve training speed andor performance and provide insights about the dataset and training process we also observe that commentaries generalise they can be reused when training new models to obtain performance benefits suggesting a usecase where commentaries are stored with a dataset and leveraged in future for improved model training less,Teaching with Commentaries,"11 March, 2021"
922,Geoffrey Hinton,capsule networks aim to parse images into a hierarchy of objects parts and relations while promising they remain limited by an inability to learn effective low level part descriptions to address this issue we propose a way to learn primary capsule encoders that detect atomic parts from a single image during training we exploit motion as a powerful perceptual cue for part definition with an expressive decoder for part generation within a layered image model with occlusion experiments demonstrate robust part discovery in the presence of multiple objects cluttered backgrounds and occlusion the part decoder infers the underlying shape masks effectively filling in occluded regions of the detected shapes we evaluate flowcapsules on unsupervised part segmentation and unsupervised image classification less,Unsupervised part representation by Flow Capsules,"19 February, 2021"
923,Geoffrey Hinton,in geoffrey hinton proposed the concept of training deep neural networks dnns and an improved model training method to break the bottleneck of neural network development more recently the introduction of alphago in demonstrated the powerful learning ability of deep learning and its enormous potential deep learning has been increasingly used to develop stateoftheart software engineering se research tools due to its ability to boost performance for various se tasks there are many factors eg deep learning model selection internal structure differences and model optimization techniques that may have an impact on the performance of dnns applied in se few works to date focus on summarizing classifying and analyzing the application of deep learning techniques in se to fill this gap we performed a survey to analyse the relevant studies published since we first provide an example to illustrate how deep learning techniques are used in se we then summarize and classify different deep learning techniques used in se we analyzed key optimization technologies used in these deep learning models and finally describe a range of key research topics using dnns in se based on our findings we present a set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities for future work less,A Survey on Deep Learning for Software Engineering,"30 November, 2020"
924,Geoffrey Hinton,techniques such as ensembling and distillation promise model quality improvements when paired with almost any base model however due to increased testtime cost for ensembles and increased complexity of the training pipeline for distillation these techniques are challenging to use in industrial settings in this paper we explore a variant of distillation which is relatively straightforward to use as it does not require a complicated multistage setup or many new hyperparameters our first claim is that online distillation enables us to use extra parallelism to fit very large datasets about twice as fast crucially we can still speed up training even after we have already reached the point at which additional parallelism provides no benefit for synchronous or asynchronous stochastic gradient descent two neural networks trained on disjoint subsets of the data can share knowledge by encouraging each model to agree with the predictions the other model would have made these predictions can come from a stale version of the other model so they can be safely computed using weights that only rarely get transmitted our second claim is that online distillation is a costeffective way to make the exact predictions of a model dramatically more reproducible we support our claims using experiments on the criteo display ad challenge dataset imagenet and the largest todate dataset used for neural language modeling containing times tokens and based on the common crawl repository of web data less,Large scale distributed neural network training through online distillation,"20 August, 2020"
925,Geoffrey Hinton,this paper presents simclr a simple framework for contrastive learning of visual representations we simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank in order to understand what enables the contrastive prediction tasks to learn useful representations we systematically study the major components of our framework we show that composition of data augmentations plays a critical role in defining effective predictive tasks introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations and contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning by combining these findings we are able to considerably outperform previous methods for selfsupervised and semisupervised learning on imagenet a linear classifier trained on selfsupervised representations learned by simclr achieves top accuracy which is a relative improvement over previous stateoftheart matching the performance of a supervised resnet when finetuned on only of the labels we achieve top accuracy outperforming alexnet with x fewer labels less,A Simple Framework for Contrastive Learning of Visual Representations,"30 June, 2020"
926,Geoffrey Hinton,the generalization and learning speed of a multiclass neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels smoothing the labels in this way prevents the network from becoming overconfident and label smoothing has been used in many stateoftheart models including image classification language translation and speech recognition despite its widespread use label smoothing is still poorly understood here we show empirically that in addition to improving generalization label smoothing improves model calibration which can significantly improve beamsearch however we also observe that if a teacher network is trained with label smoothing knowledge distillation into a student network is much less effective to explain these observations we visualize how label smoothing changes the representations learned by the penultimate layer of the network we show that label smoothing encourages the representations of training examples from the same class to group in tight clusters this results in loss of information in the logits about resemblances between instances of different classes which is necessary for distillation but does not hurt generalization or calibration of the models predictions less,When Does Label Smoothing Help?,"10 June, 2020"
927,Geoffrey Hinton,any solid object can be decomposed into a collection of convex polytopes in short convexes when a small number of convexes are used such a decomposition can be thought of as a piecewise approximation of the geometry this decomposition is fundamental in computer graphics where it provides one of the most common ways to approximate geometry for example in realtime physics simulation a convex object also has the property of being simultaneously an explicit and implicit representation one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull or implicitly as the collection of halfspace constraints or support functions their implicit representation makes them particularly well suited for neural network training as they abstract away from the topology of the geometry they need to represent however at testing time convexes can also generate explicit representations polygonal meshes which can then be used in any downstream application we introduce a network architecture to represent a low dimensional family of convexes this family is automatically derived via an autoencoding process we investigate the applications of this architecture including automatic convex decomposition image to d reconstruction and partbased shape retrieval less,CvxNet: Learnable Convex Decomposition,"12 April, 2020"
928,Geoffrey Hinton,adversarial examples raise questions about whether neural network models are sensitive to the same visual features as humans in this paper we first detect adversarial examples or otherwise corrupted images based on a classconditional reconstruction of the input to specifically attack our detection mechanism we propose the reconstructive attack which seeks both to cause a misclassification and a low reconstruction error this reconstructive attack produces undetected adversarial examples but with much smaller success rate among all these attacks we find that capsnets always perform better than convolutional networks then we diagnose the adversarial examples for capsnets and find that the success of the reconstructive attack is highly related to the visual similarity between the source and target class additionally the resulting perturbations can cause the input image to appear visually more like the target class and hence become nonadversarial this suggests that capsnets use features that are more aligned with human perception and have the potential to address the central issue raised by adversarial examples less,Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions,"18 February, 2020"
929,Geoffrey Hinton,objects are composed of a set of geometrically organized parts we introduce an unsupervised capsule autoencoder scae which explicitly uses geometric relationships between parts to reason about objects since these relationships do not depend on the viewpoint our model is robust to viewpoint changes scae consists of two stages in the first stage the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates in the second stage scae predicts parameters of a few object capsules which are then used to reconstruct part poses inference in this model is amortized and performed by offtheshelf neural encoders unlike in previous capsule networks we find that object capsule presences are highly informative of the object class which leads to stateoftheart results for unsupervised classification on svhn and mnist the code is available at httpsgithubcomgoogleresearchgoogleresearchtreemasterstackedcapsuleautoencoders less,Stacked Capsule Autoencoders,"2 December, 2019"
930,Geoffrey Hinton,recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models we examine methods for comparing neural network representations based on canonical correlation analysis cca we show that cca belongs to a family of statistics for measuring multivariate similarity but that neither cca nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points we introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation this similarity index is equivalent to centered kernel alignment cka and is also closely connected to cca unlike cca cka can reliably identify correspondences between representations in networks trained from different initializations less,Similarity of Neural Network Representations Revisited,"19 July, 2019"
931,Geoffrey Hinton,we explore and expand the textitsoft nearest neighbor loss to measure the textitentanglement of class manifolds in representation space ie how close pairs of points from the same class are relative to pairs of points from different classes we demonstrate several use cases of the loss as an analytical tool it provides insights into the evolution of class similarity structures during learning surprisingly we find that textitmaximizing the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer possibly because it encourages representations to identify classindependent similarity structures maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to bettercalibrated estimates of uncertainty on outlier data data that is not from the training distribution can be recognized by observing that in the hidden layers it has fewer than the normal number of neighbors from the predicted class less,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,"5 February, 2019"
932,Geoffrey Hinton,we present a simple technique that allows capsule models to detect adversarial images in addition to being trained to classify images the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct toplevel capsule adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the toplevel capsule for that class we show that setting a threshold on the l distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets the same technique works quite well for cnns that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax we then explore a stronger whitebox attack that takes the reconstruction error into account this attack is able to fool our detection technique but in order to make the model change its prediction to another class the attack must typically make the adversarial image resemble images of the other class less,DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules,"16 November, 2018"
933,Geoffrey Hinton,deep neural networks have proved to be a very effective way to perform classification tasks they excel when the input data is high dimensional the relationship between the input and the output is complicated and the number of labeled training examples is large but it is hard to explain why a learned network makes a particular classification decision on a particular test case this is due to their reliance on distributed hierarchical representations if we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead explaining a particular decision would be much easier we describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data less,Distilling a Neural Network Into a Soft Decision Tree,"27 November, 2017"
934,Geoffrey Hinton,we systematically explore regularizing neural networks by penalizing low entropy output distributions we show that penalizing low entropy output distributions which has been shown to improve exploration in reinforcement learning acts as a strong regularizer in supervised learning furthermore we connect a maximum entropy based confidence penalty to label smoothing through the direction of the kl divergence we exhaustively evaluate the proposed confidence penalty and label smoothing on common benchmarks image classification mnist and cifar language modeling penn treebank machine translation wmt englishtogerman and speech recognition timit and wsj we find that both label smoothing and the confidence penalty improve stateoftheart models across benchmarks without modifying existing hyperparameters suggesting the wide applicability of these regularizers less,Regularizing Neural Networks by Penalizing Confident Output Distributions,"23 January, 2017"
935,Geoffrey Hinton,we present a framework for efficient inference in structured image models that explicitly reason about objects we achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time crucially the model itself learns to choose the appropriate number of inference steps we use this scheme to learn to perform inference in partially specified d models variablesized variational autoencoders and fully specified d models probabilistic renderers we show that such models learn to identify multiple objects counting locating and classifying the elements of a scene without any supervision eg decomposing d images with various numbers of objects in a single forward pass of a neural network we further show that the networks produce accurate inferences when compared to supervised counterparts and that their structure leads to improved generalization less,"Attend, Infer, Repeat: Fast Scene Understanding with Generative Models","12 August, 2016"
936,Geoffrey Hinton,syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades as a result the most accurate parsers are domain specific complex and inefficient in this paper we show that the domain agnostic attentionenhanced sequencetosequence model achieves stateoftheart results on the most widely used syntactic constituency parsing dataset when trained on a large synthetic corpus that was annotated using existing parsers it also matches the performance of standard parsers when trained only on a small humanannotated dataset which shows that this model is highly dataefficient in contrast to sequencetosequence models without the attention mechanism our parser is also fast processing over a hundred sentences per second with an unoptimized cpu implementation less,Grammar as a Foreign Language,"9 June, 2015"
937,Geoffrey Hinton,a very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions unfortunately making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users especially if the individual models are large neural nets caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique we achieve some surprising results on mnist and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model we also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish finegrained classes that the full models confuse unlike a mixture of experts these specialist models can be trained rapidly and in parallel less,Distilling the Knowledge in a Neural Network,"9 March, 2015"
938,Geoffrey Hinton,recurrent neural networks rnns are a powerful model for sequential data endtoend training methods such as connectionist temporal classification make it possible to train rnns for sequence labelling problems where the inputoutput alignment is unknown the combination of these methods with the long shortterm memory rnn architecture has proved particularly fruitful delivering stateoftheart results in cursive handwriting recognition however rnn performance in speech recognition has so far been disappointing with better results returned by deep feedforward networks this paper investigates emphdeep recurrent neural networks which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers rnns when trained endtoend with suitable regularisation we find that deep long shortterm memory rnns achieve a test set error of on the timit phoneme recognition benchmark which to our knowledge is the best recorded score less,Speech Recognition with Deep Recurrent Neural Networks,"22 March, 2013"
939,Geoffrey Hinton,when a large feedforward neural network is trained on a small training set it typically performs poorly on heldout test data this overfitting is greatly reduced by randomly omitting half of the feature detectors on each training case this prevents complex coadaptations in which a feature detector is only helpful in the context of several other specific feature detectors instead each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate random dropout gives big improvements on many benchmark tasks and sets new records for speech and object recognition less,Improving neural networks by preventing co-adaptation of feature detectors,"3 July, 2012"
940,Geoffrey Hinton,an efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables after learning each layer samples from the posterior distributions for that layer are used as training data for learning the next layer this approach is commonly used with restricted boltzmann machines which are undirected graphical models with a single hidden layer but it can also be used with mixtures of factor analysers mfas which are directed graphical models in this paper we present a greedy layerwise learning algorithm for deep mixtures of factor analysers dmfas even though a dmfa can be converted to an equivalent shallow mfa by multiplying together the factor loading matrices at different levels learning and inference are much more efficient in a dmfa and the sharing of each lowerlevel factor loading matrix by many different higher level mfas prevents overfitting we demonstrate empirically that dmfas learn better density models than both mfas and two types of restricted boltzmann machine on a wide variety of datasets less,Deep Mixtures of Factor Analysers,"18 June, 2012"
941,Ken Jackson,"A growing number of studies report increased concussion-related health care utilization in recent years, but factors impacting care-seeking behaviors among youth following a concussion are not well described. This study aimed to evaluate the influence of insurance type on the rate and type of initial concussion visits and the time from injury to the initial visit in youth. We extracted and analyzed initial concussion-related medical visits for youth ages 10 to 17 from electronic health records. Patients must have visited Nationwide Children’s Hospital’s (NCH) concussion clinic at least once between 7/1/2012 and 12/31/2017. We evaluated the trends and patterns of initial concussion visits across the study period using regression analyses. Of 4955 unique concussion visits included, 60.1% were males, 80.5% were white, and 69.5% were paid by private insurance. Patients’ average age was 13.9 years (SD = 3.7). The rate of the initial concussion visits per 10,000 NCH visits was consistently higher in privately insured than publicly insured youth throughout the study period (P < .0001). Privately insured youth had greater odds of initial concussion visits to sports medicine clinics (AOR = 1.45, 95% CI = 1.20, 1.76) but lower odds of initial concussion visits to the ED/urgent care (AOR = 0.74, 95% CI = 0.60, 0.90) than publicly insured youth. Days from injury to initial concussion visit significantly decreased among both insurance types throughout the study (P < .0001), with a greater decrease observed in publicly insured than privately insured youth (P = .011). Results on the differences in the rate, type, and time of initial concussion-related",Influence of insurance type on rate and type of initial concussion-related medical visits among youth,2021/12
942,Ken Jackson,"The simulation of correlated multivariate Poisson processes with negative correlation between their components has many important applications in Finance, Insurance, Geophysics, and many other areas of applied probability. Introduced in our earlier work, the Backward Simulation (BS) approach to the simulation of correlated multivariate Poisson processes is able to capture a wide range of correlation, including extreme positive and extreme negative correlation, that is not possible with other approaches such as the forward simulation approach. Moreover, the BS approach enables simple and efficient generation of sample paths of correlated multivariate Poisson processes. In this work, we extend the BS approach to multivariate mixed Poisson processes.",Backward simulation of multivariate mixed Poisson processes,2021-11-22 00:00:00
943,Ken Jackson,"We develop a mixed least squares Monte Carlo--partial differential equation (LSMC-PDE) method for pricing Bermudan style options on assets under stochastic volatility. The algorithm is formulated for an arbitrary number of assets and volatility processes, and we prove the algorithm converges almost surely for a class of models. We also introduce a multilevel Monte Carlo/multigrid method to improve the algorithm's computational complexity. Our numerical examples focus on the single () and multidimensional (4d) Heston models, and we compare our hybrid algorithm with classical LSMC approaches. In each case, we find that the hybrid algorithm outperforms standard LSMC in terms of estimating prices and optimal exercise boundaries.",Mixing LSMC and PDE methods to price Bermudan options,2020
944,Ken Jackson,"Variable Annuity (VA) products expose insurance companies to considerable risk because of the guarantees they provide to buyers of these products. Managing and hedging these risks requires insurers to find the value of key risk metrics for a large portfolio of VA products. In practice, many companies rely on nested Monte Carlo (MC) simulations to find key risk metrics. MC simulations are computationally demanding, forcing insurance companies to invest hundreds of thousands of dollars in computational infrastructure per year. Moreover, existing academic methodologies are focused on fair valuation of a single VA contract, exploiting ideas in option theory and regression. In most cases, the computational complexity of these methods surpasses the computational requirements of MC simulations. Therefore, academic methodologies cannot scale well to large portfolios of VA contracts. In this paper, we present a framework for valuing such portfolios based on spatial interpolation. We provide a comprehensive study of this framework and compare existing interpolation schemes. Our numerical results show superior performance, in terms of both computational efficiency and accuracy, for these methods compared to nested MC simulations. We also present insights into the challenge of finding an effective interpolation scheme in this framework, and suggest guidelines that help us build a fully automated scheme that is efficient and accurate.",A Spatial Interpolation Framework for Efficient Valuation of Large Portfolios of Variable Annuities,2017/7
945,Ken Jackson,"We develop a highly efficient MC method for computing plain vanilla European option prices and hedging parameters under a very general jump-diffusion option pricing model which includes stochastic variance and multi-factor Gaussian interest short rate(s). The focus of our MC approach is variance reduction via dimension reduction. More specifically, the option price is expressed as an expectation of a unique solution to a conditional Partial Integro-Differential Equation (PIDE), which is then solved using a Fourier transform technique. Important features of our approach are (1) the analytical tractability of the conditional PIDE is fully determined by that of the Black–Scholes–Merton model augmented with the same jump component as in our model, and (2) the variances associated with all the interest rate factors are completely removed when evaluating the expectation via iterated conditioning applied to only the ",A dimension and variance reduction Monte-Carlo method for option pricing under jump-diffusion models,2017-05-04 00:00:00
946,Ken Jackson,"As part of the new regulatory framework of Solvency II, introduced by the European Union, insurance companies are required to monitor their solvency by computing a key risk metric called the Solvency Capital Requirement (SCR). The official description of the SCR is not rigorous and has lead researchers to develop their own mathematical frameworks for calculation of the SCR. These frameworks are complex and are difficult to implement. Recently, Bauer et al. suggested a nested Monte Carlo (MC) simulation framework to calculate the SCR. But the proposed MC framework is computationally expensive even for a simple insurance product. In this paper, we propose incorporating a neural network approach into the nested simulation framework to significantly reduce the computational complexity in the calculation. We study the performance of our neural network approach in estimating the SCR for a large portfolio ",Efficient valuation of SCR via a neural network approach,2017-03-15 00:00:00
947,Ken Jackson,"Multivariate Poisson processes have many important applications in Insurance, Finance, and many other areas of Applied Probability. In this paper we study the backward simulation approach to modelling multivariate Poisson processes and analyze the connection to the extreme measures describing the joint distribution of the processes at the terminal simulation time. Multivariate Poisson processes have many important applications in Insurance, Finance, and many other areas of Applied Probability. In this paper we study the backward simulation approach to modelling multivariate Poisson processes and analyze the connection to the extreme measures describing the joint distribution of the processes at the terminal simulation time.",Correlated multivariate Poisson processes and extreme measures,2017-01-01 00:00:00
948,Ken Jackson,"Managing and hedging the risks associated with Variable Annuity (VA) products require intraday valuation of key risk metrics for these products. The complex structure of VA products and computational complexity of their accurate evaluation have compelled insurance companies to adopt Monte Carlo (MC) simulations to value their large portfolios of VA products. Because the MC simulations are computationally demanding, especially for intraday valuations, insurance companies need more efficient valuation techniques. Recently, a framework based on traditional spatial interpolation techniques has been proposed that can significantly decrease the computational complexity of MC simulation (Gan and Lin, 2015). However, traditional interpolation techniques require the definition of a distance function that can significantly impact their accuracy. Moreover, none of the traditional spatial interpolation techniques",A neural network approach to efficient valuation of large portfolios of variable annuities,2016-09-01 00:00:00
949,Ken Jackson,"The drMC method is a dimension reduction technique built upon (i) the conditional MC technique applied to one of the factors which does not depend on any other factors in the model, and (ii) the derivation of a closed-form solution to the conditional partial differential equation (PDE) that arises via Fourier transforms. In the drMC approach, the option price can be computed simply by taking the expectation of this closed-form solution. Hence, the approach results in a powerful dimension reduction from N to one, which often ",Dimension and variance reduction for Monte Carlo methods for high-dimensional models in finance,2015-11-02 00:00:00
950,Ken Jackson,"Pennation angle (PA) is an important property of human skeletal muscle that plays a significant role in determining the force contribution of fascicles to skeletal movement. Two-dimensional (2D) ultrasonography is the most common approach to measure PA. However, in principle, it is challenging to infer knowledge of three-dimensional (3D) architecture from 2D assessment. Furthermore, architectural complexity and variation impose more difficulties on reliable and consistent quantification of PA. Thus, the purpose of our study is to provide accurate insight into the correspondence between 2D assessment and the underlying 3D architecture. To this end, a 3D method was developed to directly quantify PA based on 3D architectural data that were acquired from cadaveric specimens through dissection and digitization. Those data were then assessed two-dimensionally by simulating ultrasound imaging. To achieve",A three-dimensional approach to pennation angle estimation for human skeletal muscle,2015-10-03 00:00:00
951,Ken Jackson,"Models based on stochastic differential equations are of high interest today due to their many important practical applications. Thus the need for efficient and accurate numerical methods to approximate their solution. In this paper, we propose several adaptive time-stepping strategies for the strong numerical solution of stochastic differential equations in Itô form, driven by multiple Wiener processes satisfying the commutativity condition. The adaptive schemes are based on I and PI control, and allow arbitrary values of the stepsize. The explicit Milstein method is applied to approximate the solution of the problem and the adaptive implementations are based on estimates of the local error obtained using Richardson extrapolation. Numerical tests on several models arising in applications show that our adaptive time-stepping schemes perform better than the fixed stepsize alternative and an adaptive Brownian tree time",Adaptive time-stepping for the strong numerical solution of stochastic differential equations,2015/4
952,Ken Jackson,we outline emerging opportunities and challenges to enhance the utility of ai for scientific discovery the distinct goals of ai for industry versus the goals of ai for science create tension between identifying patterns in data versus discovering patterns in the world from data if we address the fundamental challenges associated with bridging the gap between domaindriven scientific models and datadriven ai learning machines then we expect that these ai models can transform hypothesis generation scientific discovery and the scientific process itself less,Learning from learning machines: a new generation of AI technology to meet the needs of science,"26 November, 2021"
953,Ken Jackson,multivariate poisson processes have many important applications in insurance finance and many other areas of applied probability in this paper we study the backward simulation approach to modelling multivariate poisson processes and analyze the connection to the extreme measures describing the joint distribution of the processes at the terminal simulation time less,Correlated Multivariate Poisson Processes and Extreme Measures,"27 October, 2017"
954,Allan Jepson,in this paper we study the problem of procedure planning in instructional videos here an agent must produce a plausible sequence of actions that can transform the environment from a given start to a desired goal state when learning procedure planning from instructional videos most recent work leverages intermediate visual observations as supervision which requires expensive annotation efforts to localize precisely all the instructional steps in training videos in contrast we remove the need for expensive temporal video annotations and propose a weakly supervised approach by learning from natural language instructions our model is based on a transformer equipped with a memory module which maps the start and goal observations to a sequence of plausible actions furthermore we augment our model with a probabilistic generative module to capture the uncertainty inherent to procedure planning an aspect largely overlooked by previous work we evaluate our model on three datasets and show our weaklysupervised approach outperforms previous fully supervised stateoftheart models on multiple metrics less,P3IV: Probabilistic Procedure Planning from Instructional Videos with Weak Supervision,"4 May, 2022"
955,Allan Jepson,bisimulation metrics define a distance measure between states of a markov decision process mdp based on a comparison of reward sequences due to this property they provide theoretical guarantees in value function approximation in this work we first prove that bisimulation metrics can be defined via any pwasserstein metric for pgeq then we describe an approximate policy iteration api procedure that uses aggregation with bisimulation and prove performance bounds for continuous state spaces we bound the difference between bisimulation metrics in terms of the change in the policies themselves based on these theoretical results we design an api procedure that employs conservative policy updates and enjoys better performance bounds than the naive api approach in addition we propose a novel trust region approach which circumvents the requirement to explicitly solve a constrained optimization problem finally we provide experimental evidence of improved stability compared to nonconservative alternatives in simulated continuous control less,Trusted Approximate Policy Iteration with Bisimulation Metrics,"22 February, 2022"
956,Allan Jepson,modern generative adversarial networks gans predominantly use piecewise linear activation functions in discriminators or critics including relu and leakyrelu such models learn piecewise linear mappings where each piece handles a subset of the input space and the gradients per subset are piecewise constant under such a class of discriminator or critic functions we present gradient normalization gran a novel inputdependent normalization method which guarantees a piecewise klipschitz constraint in the input space in contrast to spectral normalization gran does not constrain processing at the individual network layers and unlike gradient penalties strictly enforces a piecewise lipschitz constraint almost everywhere empirically we demonstrate improved image generation performance across multiple datasets incl cifar stl lsun bedrooms and celeba gan loss functions and metrics further we analyze altering the often untuned lipschitz constant k in several standard gans not only attaining significant performance gains but also finding connections between k and training dynamics particularly in lowgradient loss plateaus with the common adam optimizer less,GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks,"4 November, 2021"
957,Allan Jepson,human beings even small children quickly become adept at figuring out how to use applications on their mobile devices learning to use a new app is often achieved via trialanderror accelerated by transfer of knowledge from past experiences with like apps the prospect of building a smarter smartphone one that can learn how to achieve tasks using mobile apps is tantalizing in this paper we explore the use of reinforcement learning rl with the goal of advancing this aspiration we introduce an rlbased framework for learning to accomplish tasks in mobile apps rl agents are provided with states derived from the underlying representation of onscreen elements and rewards that are based on progress made in the task agents can interact with screen elements by tapping or typing our experimental results over a number of mobile apps show that rl agents can learn to accomplish multistep tasks as well as achieve modest generalization across different apps more generally we develop a platform which addresses several engineering challenges to enable an effective rl training environment our appbuddy platform is compatible with openai gym and includes a suite of mobile apps and benchmark tasks that supports a diversity of rl research in the mobile app setting less,AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement Learning,"6 June, 2021"
958,Allan Jepson,a complete representation of d objects requires characterizing the space of deformations in an interpretable manner from articulations of a single instance to changes in shape across categories in this work we improve on a prior generative model of geometric disentanglement for d shapes wherein the space of object geometry is factorized into rigid orientation nonrigid pose and intrinsic shape the resulting model can be trained from raw d shapes without correspondences labels or even rigid alignment using a combination of classical spectral geometry and probabilistic disentanglement of a structured latent representation space our improvements include more sophisticated handling of rotational invariance and the use of a diffeomorphic flow network to bridge latent and spectral space the geometric structuring of the latent space imparts an interpretable characterization of the deformation space of an object furthermore it enables tasks like pose transfer and poseaware retrieval without requiring supervision we evaluate our model on its generative modelling representation learning and disentanglement performance showing improved rotation invariance and intrinsicextrinsic factorization quality over the prior model less,Disentangling Geometric Deformation Spaces in Generative Latent Shape Models,"27 February, 2021"
959,Allan Jepson,this paper reviews the aim challenge on efficient single image superresolution with focus on the proposed solutions and results the challenge task was to superresolve an input image with a magnification factor x based on a set of prior examples of low and corresponding high resolution images the goal is to devise a network that reduces one or several aspects such as runtime parameter count flops activations and memory consumption while at least maintaining psnr of msrresnet the track had registered participants and teams submitted the final results they gauge the stateoftheart in efficient single image superresolution less,AIM 2020 Challenge on Efficient Super-Resolution: Methods and Results,"15 September, 2020"
960,Allan Jepson,the computer vision community has witnessed recent advances in scene categorization from images with the stateofthe art systems now achieving impressive recognition rates on challenging benchmarks such as the places dataset such systems have been trained on photographs which include color texture and shading cues the geometry of shapes and surfaces as conveyed by scene contours is not explicitly considered for this task remarkably humans can accurately recognize natural scenes from line drawings which consist solely of contourbased shape cues here we report the first computer vision study on scene categorization of line drawings derived from popular databases including an artist scene database mit and places specifically we use offtheshelf pretrained cnns to perform scene classification given only contour information as input and find performance levels well above chance we also show that medialaxis based contour salience methods can be used to select more informative subsets of contour pixels and that the variation in cnn classification performance on various choices for these subsets is qualitatively similar to that observed in human performance moreover when the salience measures are used to weight the contours as opposed to pruning them we find that these weights boost our cnn performance above that for unweighted contour input that is the medial axis based salience weights appear to add useful information that is not available when cnns are trained to use contours alone less,Scene Categorization from Contours: Medial Axis Based Salience Measures,"26 November, 2018"
961,Hector Levesque,among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting the logical account proposed by bacchus halpern and levesque is perhaps the most expressive while their formalism is quite general it is restricted to fluents whose values are drawn from discrete finite domains as opposed to the continuous domains seen in many robotic applications in this work we show how this limitation in that approach can be lifted by dealing seamlessly with both discrete distributions and continuous densities within a rich theory of action we provide a very general logical specification of how belief should change after acting and sensing in complex noisy domains less,Reasoning about Discrete and Continuous Noisy Sensors and Effectors in Dynamical Systems,"14 September, 2018"
962,Hector Levesque,reasoning about degrees of belief in uncertain dynamic worlds is fundamental to many applications such as robotics and planning where actions modify state properties and sensors provide measurements both of which are prone to noise with the exception of limited cases such as gaussian processes over linear phenomena belief state evolution can be complex and hard to reason with in a general way this paper proposes a framework with new results that allows the reduction of subjective probabilities after sensing and acting to questions about the initial state only we build on an expressive probabilistic firstorder logical account by bacchus halpern and levesque resulting in a methodology that in principle can be coupled with a variety of existing inference solutions less,Reasoning about Probabilities in Dynamic Systems using Goal Regression,"26 September, 2013"
963,Renée Miller,"Dataset discovery can be performed using search (with a query or keywords) to find relevant data. However, the result of this discovery canbeoverwhelmingtoexplore.Existingnavigation techniques mostly focus on linkage graphs that enable navigation from one data set to another based on similarity or joinability of attributes. However, users often do not know which data set to start the navigation from. RONIN proposes an alternative way to navigate by building a hierarchical structure on a collection of data sets: the user navigates between groups of data sets in a hierarchical manner to narrow down to the data of interest. We demonstrate RONIN, a tool that enables user exploration of a data lake by seamlessly integrating the two common modalities of discovery: data set search and navigation of a hierarchical structure.",RONIN: Data Lake Exploration,2021
964,Renée Miller,"We discuss our experience in bringing data exchange to knowledge graphs. This experience includes the development of Kensho, a tool for generating mapping rules and performing knowledge exchange between two Knowledge Bases (KBs). We highlight the challenges addressed in Kensho, including managing the rich structural complexity of KBs and the need to handle incomplete correspondences between property paths. We use Kensho to highlight many open problems related to knowledge exchange including how knowledge translation can inform the task of KB integration and population.",Towards Knowledge Exchange: State-of-the-Art and Open Problems,11 January 2021
965,Renée Miller,"Modern data lakes are deeply heterogeneous in the vocabulary that is used to describe data. We study a problem of disambiguation in data lakes: how can we determine if a data value occurring more than once in the lake has different meanings and is therefore a homograph? While word and entity disambiguation have been well studied in computational linguistics, data management and data science, we show that data lakes provide a new opportunity for disambiguation of data values since they represent a massive network of interconnected values. We investigate to what extent this network can be used to disambiguate values. DomainNet uses network-centrality measures on a bipartite graph whose nodes represent values and attributes to determine, without supervision, if a value is a homograph. A thorough experimental evaluation demonstrates that state-of-the-art techniques in domain discovery cannot be re-purposed to compete with our method. Specifically, using a domain discovery method to identify homographs has a precision and a recall of 38% versus 69% with our method on a synthetic benchmark. By applying a network-centrality measure to our graph representation, DomainNet achieves a good separation between homographs and data values with a unique meaning. On a real data lake our top-200 precision is 89%.",DomainNet: Homograph Detection for Data Lake Disambiguation,17 Mar 2021 
966,Renée Miller,"CSV is a popular Open Data format widely used in a variety of domains for its simplicity and effectiveness in storing and disseminating data. Unfortunately, data published in this format often does not conform to strict specifications, making automated data extraction from CSV files a painful task. While table discovery from HTML pages or spreadsheets has been studied extensively, extracting tables from CSV files still poses a considerable challenge due to their loosely defined format and limited embedded metadata",Pytheas: Pattern-based Table Discovery in CSV Files,2020
967,Renée Miller,"We consider the problem of creating an effective navigation structure over a data lake. We define an organization as a navigation graph that contains nodes representing sets of attributes within a data lake and edges indicating subset relationships among nodes. We propose the data lake organization problem as the problem of finding an organization that allows a user to most effectively navigate a data lake. We present a new probabilistic model of how users interact with an organization and propose an approximate algorithm for the data lake organization problem. We show the effectiveness of the algorithm on both a real data lake containing data from open data portals and on a benchmark that contains rich metadata emulating the observed characteristics of real data lakes. Through a formal user study, we show that navigation can help users find relevant tables that cannot be found by keyword search.",Organizing Data Lakes for Navigation,11 June 2020
968,Renée Miller,"We introduce Kensho, a tool for generating mapping rules between two Knowledge Bases (KBs). To create the mapping rules, Kensho starts with a set of correspondences and enriches them with additional semantic information automatically identified from the structure and constraints of the KBs. Our approach works in two phases. In the first phase, semantic associations between resources of each KB are captured. In the second phase, mapping rules are generated by interpreting the correspondences in a way that respects the discovered semantic associations among elements of each KB. Kensho's mapping rules are expressed using SPARQL queries and can be used directly to exchange knowledge from source to target. Kensho is able to automatically rank the generated mapping rules using a set of heuristics. We present an experimental evaluation of Kensho and assess our mapping generation and ranking strategies using more than 50 synthesized and real world settings, chosen to showcase some of the most important applications of knowledge translation. In addition, we use three existing benchmarks to demonstrate Kensho's ability to deal with different mapping scenarios.",Knowledge Translation: Extended Technical Report, 3 Aug 2020
969,Renée Miller,"We present VISE, or Vehicle Image Search Engine, to support the fast search of similar vehicles from low-resolution traffic camera images. VISE can be used to trace and locate vehicles for applications such as police investigations when high-resolution footage is not available. Our system consists of three components: an interactive user-interface for querying and browsing identified vehicles; a scalable search engine for fast similarity search on millions of visual objects; and an image processing pipeline that extracts feature vectors of objects from video frames. W",VISE: Vehicle Image Search Engine with Traffic Camera,2019
970,Renée Miller,"Graphs have always been an important fundamental data structure in computer science. Graphs are also ubiquitous in daily life appearing throughout nature, science, economics, government, and our social structures. As our ability to store larger and larger graphs increases, so do the fundamental data management challenges in graph management. In this thematic special issue, we collect together five regular VLDB Journal papers that consider different classical data management issues related to graphs.",Thematic issue on data management for graphs,2019
971,Renée Miller,"As interest in supporting data exchange between heterogeneous knowledge bases (KBs) has increased, so has interest in benchmarking KB exchange systems. One important benchmark, LODIB (Linked Open Data Integration Benchmark), has been proposed that reflects the real and deep heterogeneity of knowledge bases in the Linked Open Data Cloud. In this position paper, we reflect on requirements for benchmarks of KB exchange systems and bring to bear important lessons learned from other data exchange systems. Specifically, we consider the importance, in data exchange, of explicitly modeling incompleteness, something that is at the heart of relational data exchange and that is solved using principled value invention methods. We also consider the incompleteness that arises naturally within knowledge bases and how that influences KB exchange. Instances of a single class in a KB may exhibit heterogeneity in their structure and modeling this is important in a KB exchange benchmark. Using these insights, we propose a set of new requirements for a KB exchange benchmark.",Towards a Benchmark for Knowledge Base Exchange,2019
972,Renée Miller,"We present a new solution for finding joinable tables in massive data lakes: given a table and one join column, find tables that can be joined with the given table on the largest number of distinct values. The problem can be formulated as an overlap set similarity search problem by considering columns as sets and matching values as intersection between sets. Although set similarity search is well-studied in the field of approximate string search (e.g., fuzzy keyword search), the solutions are designed for and evaluated over sets of relatively small size (average set size rarely much over 100 and maximum set size in the low thousands) with modest dictionary sizes (the total number of distinct values in all sets is only a few million). ",JOSIE: Overlap Set Similarity Search for Finding Joinable Tables in Data Lakes,June 2019
973,Renée Miller,"Open Data plays a major role in open government initiatives. Governments around the world are adopting Open Data Principles promising to make their Open Data complete, primary, and timely. These properties make this data tremendously valuable. Open Data poses interesting new challenges for data integration research and we take a look at one of those challenges, data discovery. How can we find new data sets within this ever expanding sea of Open Data. How do we make this sea transparent?",Making Open Data Transparent: Data Discovery on Open Data,2019
974,Renée Miller,we report on the first simultaneous nicer and nustar observations of the neutron star ns lowmass xray binary u obtained in august the source was at a luminosity of simd mathrmkpctimes ergs s in the kev band we account for the continuum emission with two different continuum descriptions that have been used to model the source previously despite the choice in continuum model the combined passband reveals a broad fe k line indicative of reflection in the spectrum in order to account for the reflection spectrum we utilize a modified version of the reflection model relxill that is tailored for thermal emission from accreting nss alternatively we also use the reflection convolution model of rfxconv to model the reflected emission that would arise from a comptonized thermal component for comparison we determine that the innermost region of the accretion disk extends close to the innermost stable circular orbit rmathrmisco at the confidence level regardless of reflection model moreover the current flux calibration of nicer is within of the nustarfpmab less,NICER-NuSTAR Observations of the Neutron Star Low-Mass X-ray Binary 4U 1735-44,"14 April, 2020"
975,Renée Miller,we report on xray and radio observations of the ultracompact xray binary u taken in august during an enhanced accretion episode we obtained nicer monitoring of the source over a sim day period during which targetofopportunity observations were also conducted with swift integral and atca emission lines were measured in the nicer xray spectrum at sim kev and sim kev that correspond to o and fe respectively by modeling these line components we are able to track changes in the accretion disk throughout this period the innermost accretion flow appears to move inwards from hundreds of gravitational radii rggmc at the beginning of the outburst to rg at peak intensity we do not detect the source in radio but are able to place a upper limit on the flux density at jy beam comparing the radio and xray luminosities we find that the source lies significantly away from the range typical of black holes in the lrlx plane suggesting a neutron star ns primary this adds to the evidence that nss do not follow a single track in the lrlx plane limiting its use in distinguishing between different classes of nss based on radio and xray observations alone less,"Observations of the Ultra-compact X-ray Binary 4U 1543-624 in Outburst with NICER, INTEGRAL, Swift, and ATCA","1 August, 2019"
976,Renée Miller,although the most luminous class of neutron star low mass xray binaries known as z sources have been well studied their behavior is not fully understood in particular what causes these sources to trace out the characteristic zshaped pattern on colorcolor or hardnessintensity diagrams is not well known by studying the physical properties of the different spectral states of these sources we may better understand such variability with that goal in mind we present a recent nustar observation of the z source gx which spans approximately days and covers all its spectral states by creating a hardnessintensity diagram we were able to extract five spectra and trace the change in spectral parameters throughout the ztrack gx shows a strong broad fe k line in all states regardless of the continuum model used through modeling of the reflection spectrum and fe k line we find that in most states the inner disk radius is consistent with remaining unchanged at an average radius of rg or km for a canonical modot neutron star during the brightest flaring branch however the inner disk radius from reflection is not well constrained less,A NuSTAR observation of the low mass X-ray binary GX 349+2 throughout the Z-track,"11 September, 2018"
977,Renée Miller,the linked clinical trials linkedct project aims at publishing the first open semantic web data source for clinical trials data the database exposed by linkedct is generated by transforming existing data sources of clinical trials into rdf and discovering semantic links between the records in the trials data and several other data sources in this paper we discuss several challenges involved in these two steps and present the methodology used in linkedct to overcome these challenges our approach for semantic link discovery involves using stateoftheart approximate string matching techniques combined with ontologybased semantic matching of the records all performed in a declarative and easytouse framework we present an evaluation of the performance of our proposed techniques in several link discovery scenarios in linkedct less,LinkedCT: A Linked Data Space for Clinical Trials,"4 August, 2009"
978,John Mylopoulos,nowadays most companies need to collect store and manage personal information in order to deliver their services accordingly privacy has emerged as a key concern for these companies since they need to comply with privacy laws and regulations to deal with them properly such privacy concerns should be considered since the early phases of system design ontologies have proven to be a key factor for elaborating highquality requirements models however most existing work deals with privacy as a special case of security requirements thereby missing essential traits of this family of requirements in this paper we introduce copri a core ontology for privacy requirements engineering that adopts and extends our previous work on privacy requirements engineering ontology that has been mined through a systematic literature review additionally we implement validate and then evaluate our ontology less,A Core Ontology for Privacy Requirements Engineering,"30 November, 2018"
979,John Mylopoulos,goal models have been widely used in computer science to represent software requirements business objectives and design qualities existing goal modelling techniques however have shown limitations of expressiveness andor tractability in coping with complex realworld problems in this work we exploit advances in automated reasoning technologies notably satisfiability and optimization modulo theories smtomt and we propose and formalize i an extended modelling language for goals namely the constrained goal model cgm which makes explicit the notion of goal refinement and of domain assumption allows for expressing preferences between goals and refinements and allows for associating numerical attributes to goals and refinements for defining constraints and optimization goals over multiple objective functions refinements and their numerical attributes ii a novel set of automated reasoning functionalities over cgms allowing for automatically generating suitable refinements of input cgms under userspecified assumptions and constraints that also maximize preferences and optimize given objective functions we have implemented these modelling and reasoning functionalities in a tool named cgmtool using the omt solver optimathsat as automated reasoning backend moreover we have conducted an experimental evaluation on large cgms to support the claim that our proposal scales well for goal models with thousands of elements less,Multi-Object Reasoning with Constrained Goal Models,"25 November, 2016"
980,John Mylopoulos,the requirements elicited from stakeholders are typically informal incomplete ambiguous and inconsistent it is the task of requirements engineering to transform them into an eligible formal sufficiently complete unambiguous consistent modifiable and traceable requirements specification of functions and qualities that the systemtobe needs to operationalize to address this requirements problem we have proposed desiree a requirements calculus for systematically transforming stakeholder requirements into an eligible specification in this paper we define the semantics of the concepts used to model requirements and that of the operators used to refine and operationalize requirements we present a graphical modeling tool that supports the entire framework including the nine concepts eight operators and the transformation methodology we use a meeting scheduler example to illustrate the kinds of reasoning tasks that we can perform based on the given semantics less,Desiree: a Refinement Calculus for Requirements Problems,"7 May, 2016"
981,John Mylopoulos,to fork a project is to copy the existing code base and move in a direction different than that of the erstwhile project leadership forking provides a rapid way to address new requirements by adapting an existing solution however it can also create a plethora of similar tools and fragment the developer community hence it is not always clear whether forking is the right strategy in this paper we describe a mixedmethods exploratory case study that investigated the process of forking a project the study concerned the forking of an opensource tool for managing software projects trac trac was forked to address differing requirements in an academic setting the paper makes two contributions to our understanding of code forking first our exploratory study generated several theories about code forking in open source projects for further research second we investigated one of these theories in depth via a quantitative study we conjectured that the features of the oss forking process would allow new requirements to be addressed we show that the forking process in this case was successful at fulfilling the new projects requirements less,Code forking in open-source software: a requirements perspective,"16 April, 2010"
982,John Mylopoulos,a requirements engineering artifact is valid relative to the stakeholders of the systemtobe if they agree on the content of that artifact checking relative validity involves a discussion between the stakeholders and the requirements engineer this paper proposes i a language for the representation of information exchanged in a discussion about the relative validity of an artifact ii the acceptability condition which when it verifies in a discussion captured in the proposed language signals that the relative validity holds for the discussed artifact and for the participants in the discussion and iii reasoning procedures to automatically check the acceptability condition in a discussions captured by the proposed language less,Towards a Theory of Requirements Elicitation: Acceptability Condition for the Relative Validity of Requirements,"5 February, 2009"
983,Radford Neal,"I show how it can be beneficial to express Metropolis accept/reject decisions in terms of comparison with a uniform [0,1] value, u, and to then update u non-reversibly, as part of the Markov chain state, rather than sampling it independently each iteration. This provides a small improvement for random walk Metropolis and Langevin updates in high dimensions. It produces a larger improvement when using Langevin updates with persistent momentum, giving performance comparable to that of Hamiltonian Monte Carlo (HMC) with long trajectories. This is of significance when some variables are updated by other methods, since if HMC is used, these updates can be done only between trajectories, whereas they can be done more often with Langevin updates. I demonstrate that for a problem with some continuous variables, updated by HMC or Langevin updates, and also discrete variables, updated by Gibbs sampling between updates of the continuous variables, Langevin with persistent momentum and non-reversible updates to u samples nearly a factor of two more efficiently than HMC. Benefits are also seen for a Bayesian neural network model in which hyperparameters are updated by Gibbs sampling.","Non-reversibly updating a uniform [0, 1] value for Metropolis accept/reject decisions",2020-01-31 00:00:00
984,Radford Neal,"We propose a new scheme for selecting pool states for the embedded Hidden Markov Model (HMM) Markov Chain Monte Carlo (MCMC) method. This new scheme allows the embedded HMM method to be used for efficient sampling in state space models where the state can be high-dimensional. Previously, embedded HMM methods were only applicable to low-dimensional state-space models. We demonstrate that using our proposed pool state selection scheme, an embedded HMM sampler can have similar performance to a well-tuned sampler that uses a combination of Particle Gibbs with Backward Sampling (PGBS) and Metropolis updates. The scaling to higher dimensions is made possible by selecting pool states locally near the current value of the state sequence. The proposed pool state selection scheme also allows each iteration of the embedded HMM sampler to take time linear in the number of the",Sampling latent states for high-dimensional non-linear state space models with the embedded HMM method,2018/9
985,Radford Neal,"I show how to run an N-time-step Markov chain simulation in a circular fashion, so that the state at time 0 follows the state at time N-1 in the same way as states at times t follow those at times t-1 for 0<t<N. This wrap-around of the chain is achieved using a coupling procedure, and produces states that all have close to the equilibrium distribution of the Markov chain, under the assumption that coupled chains are likely to coalesce in less than N/2 iterations. This procedure therefore automatically eliminates the initial portion of the chain that would otherwise need to be discarded to get good estimates of equilibrium averages. The assumption of rapid coalescence can be tested using auxiliary chains started at times spaced between 0 and N. When multiple processors are available, such auxiliary chains can be simulated in parallel, and pieced together to give the circularly-coupled chain, in less time than a sequential simulation would have taken, provided that coalescence is indeed rapid. The practical utility of these procedures is dependent on the development of good coupling schemes. I show how a specialized random-grid Metropolis algorithm can be used to produce the required exact coalescence. On its own, this method is not efficient in high dimensions, but it can be used to produce exact coalescence once other methods have brought the coupled chains close together. I investigate how well this combined scheme works with standard Metropolis, Langevin, and Gibbs sampling updates. Using such strategies, I show that circular coupling can work effectively in a Bayesian logistic regression problem.",Circularly-coupled Markov chain sampling,2017-11-13 00:00:00
986,Radford Neal,"I present two new methods for exactly summing a set of floating-point numbers, and then correctly rounding to the nearest floating-point number. Higher accuracy than simple summation (rounding after each addition) is important in many applications, such as finding the sample mean of data. Exact summation also guarantees identical results with parallel and serial implementations, since the exact sum is independent of order. The new methods use variations on the concept of a ""superaccumulator"" - a large fixed-point number that can exactly represent the sum of any reasonable number of floating-point values. One method uses a ""small"" superaccumulator with sixty-seven 64-bit chunks, each with 32-bit overlap with the next chunk, allowing carry propagation to be done infrequently. The small superaccumulator is used alone when summing a small number of terms. For big summations, a ""large"" superaccumulator is used as well. It consists of 4096 64-bit chunks, one for every possible combination of exponent bits and sign bit, plus counts of when each chunk needs to be transferred to the small superaccumulator. To add a term to the large superaccumulator, only a single chunk and its associated count need to be updated, which takes very few instructions if carefully implemented. On modern 64-bit processors, exactly summing a large array using this combination of large and small superaccumulators takes less than twice the time of simple, inexact, ordered summation, with a serial implementation. A parallel implementation using a small number of processor cores can be expected to perform exact summation of large arrays at a speed that ",Fast exact summation using small and large superaccumulators,2015-05-21 00:00:00
987,Radford Neal,"Data files often consist of numbers having only a few significant decimal digits, whose information content would allow storage in only 32 bits. However, we may require that arithmetic operations involving these numbers be done with 64-bit floating-point precision, which precludes simply representing the data as 32-bit floating-point values. Decimal floating point gives a compact and exact representation, but requires conversion with a slow division operation before it can be used. Here, I show that interesting subsets of 64-bit floating-point values can be compactly and exactly represented by the 32 bits consisting of the sign, exponent, and high-order part of the mantissa, with the lower-order 32 bits of the mantissa filled in by table lookup, indexed by bits from the part of the mantissa retained, and possibly from the exponent. For example, decimal data with 4 or fewer digits to the left of the decimal point and 2 or fewer digits to the right of the decimal point can be represented in this way using the lower-order 5 bits of the retained part of the mantissa as the index. Data consisting of 6 decimal digits with the decimal point in any of the 7 positions before or after one of the digits can also be represented this way, and decoded using 19 bits from the mantissa and exponent as the index. Encoding with such a scheme is a simple copy of half the 64-bit value, followed if necessary by verification that the value can be represented, by checking that it decodes correctly. Decoding requires only extraction of index bits and a table lookup. Lookup in a small table will usually reference cache; even with larger tables, decoding is still faster than conversion from decimal ",Representing numeric data in 32 bits while preserving 64-bit precision,2015-04-11 00:00:00
988,Radford Neal,"In this paper, we introduce efficient ensemble Markov Chain Monte Carlo (MCMC) sampling methods for Bayesian computations in the univariate stochastic volatility model. We compare the performance of our ensemble MCMC methods with an improved version of a recent sampler of Kastner and Fruwirth-Schnatter (2014). We show that ensemble samplers are more efficient than this state of the art sampler by a factor of about 3.1, on a data set simulated from the stochastic volatility model. This performance gain is achieved without the ensemble MCMC sampler relying on the assumption that the latent process is linear and Gaussian, unlike the sampler of Kastner and Fruwirth-Schnatter.",Efficient Bayesian inference for stochastic volatility models with ensemble MCMC methods,2014-12-09 00:00:00
989,Radford Neal,"We show how the Hamiltonian Monte Carlo algorithm can sometimes be speeded up by “splitting” the Hamiltonian in a way that allows much of the movement around the state space to be done at low computational cost. One context where this is possible is when the log density of the distribution of interest (the potential energy function) can be written as the log of a Gaussian density, which is a quadratic function, plus a slowly-varying function. Hamiltonian dynamics for quadratic energy functions can be analytically solved. With the splitting technique, only the slowly-varying part of the energy needs to be handled numerically, and this can be done with a larger stepsize (and hence fewer steps) than would be necessary with a direct simulation of the dynamics. Another context where splitting helps is when the most important terms of the potential energy function and its gradient can be evaluated quickly,",Split hamiltonian monte carlo,2014/5
990,Radford Neal,"We introduce an efficient MCMC sampling scheme to perform Bayesian inference in the M/G/1 queueing model given only observations of interdeparture times. Our MCMC scheme uses a combination of Gibbs sampling and simple Metropolis updates together with three novel ""shift"" and ""scale"" updates. We show that our novel updates improve the speed of sampling considerably, by factors of about 60 to about 180 on a variety of simulated data sets.",On Bayesian inference for the M/G/1 queue with efficient MCMC sampling,2014-01-22 00:00:00
991,Radford Neal,"Gaussian Process (GP) models are a powerful and flexible tool for non-parametric regression and classification. Computation for GP models is intensive, since computing the posterior density, , for covariance function parameters requires computation of the covariance matrix, C, a  operation, where p is the number of covariates and n is the number of training cases, and then inversion of C, an  operation. We introduce MCMC methods based on the ""temporary mapping and caching"" framework, using a fast approximation, , as the distribution needed to construct the temporary space. We propose two implementations under this scheme: ""mapping to a discretizing chain"", and ""mapping with tempered transitions"", both of which are exactly correct MCMC methods for sampling , even though their transitions are constructed using an approximation. These methods are equivalent when their tuning parameters are set at the simplest values, but differ in general. We compare how well these methods work when using several approximations, finding on synthetic datasets that a  based on the ""Subset of Data"" (SOD) method is almost always more efficient than standard MCMC using only . On some datasets, a more sophisticated  based on the ""Nystr\""om-Cholesky"" method works better than SOD",MCMC methods for Gaussian process models using fast approximations for the likelihood,2013-05-10 00:00:00
992,Radford Neal,"Non-linear state space models are a widely-used class of models for biological, economic, and physical processes. Fitting these models to observed data is a difficult inference problem that has no straightforward solution. We take a Bayesian approach to the inference of unknown parameters of a non-linear state model; this, in turn, requires the availability of efficient Markov Chain Monte Carlo (MCMC) sampling methods for the latent (hidden) variables and model parameters. Using the ensemble technique of Neal (2010) and the embedded HMM technique of Neal (2003), we introduce a new Markov Chain Monte Carlo method for non-linear state space models. The key idea is to perform parameter updates conditional on an enormously large ensemble of latent sequences, as opposed to a single sequence, as with existing methods. We look at the performance of this ensemble method when doing Bayesian inference in the Ricker model of population dynamics. We show that for this problem, the ensemble method is vastly more efficient than a simple Metropolis method, as well as 1.9 to 12.0 times more efficient than a single-sequence embedded HMM method, when all methods are tuned appropriately. We also introduce a way of speeding up the ensemble method by performing partial backward passes to discard poor proposals at low computational cost, resulting in a final efficiency gain of 3.4 to 20.4 times over the single-sequence method.",MCMC for non-linear state space models using ensembles of latent sequences,2013-05-02 00:00:00
993,Radford Neal,"Inference for belief networks using Gibbs sampling produces a distribution for unobserved variables that differs from the correct distribution by a (usually) unknown error, since convergence to the right distribution occurs only asymptotically. The method of ""coupling from the past"" samples from exactly the correct distribution by (conceptually) running dependent Gibbs sampling simulations from every possible starting state from a time far enough in the past that all runs reach the same state at time t=0. Explicitly considering every possible state is intractable for large networks, however. We propose a method for layered noisy-or networks that uses a compact, but often imprecise, summary of a set of states. This method samples from exactly the correct distribution, and requires only about twice the time per step as ordinary Gibbs sampling, but it may require more simulation steps than would be needed if chains were tracked exactly.",Inference for belief networks using coupling from the past,2013-01-16 00:00:00
994,Radford Neal,"Gaussian Process (GP) regression models typically assume that residuals are Gaussian and have the same variance for all observations. However, applications with input-dependent noise (heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a latent variable that serves as an additional unobserved covariate for the regression. This model (which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing partial derivative with respect to this unobserved covariate. With a suitable covariance function, our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) non-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance. We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV give better results (smaller mean squared error and negative log-probability density) than standard GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV.",Gaussian process regression with heteroscedastic or non-Gaussian residuals,2012-12-26 00:00:00
995,Radford Neal,bayesian classification and regression with high order interactions is largely infeasible because markov chain monte carlo mcmc would need to be applied with a great many parameters whose number increases rapidly with the order in this paper we show how to make it feasible by effectively reducing the number of parameters exploiting the fact that many interactions have the same values for all training cases our method uses a single compressed parameter to represent the sum of all parameters associated with a set of patterns that have the same value for all training cases using symmetric stable distributions as the priors of the original parameters we can easily find the priors of these compressed parameters we therefore need to deal only with a much smaller number of compressed parameters when training the model with mcmc the number of compressed parameters may have converged before considering the highest possible order after training the model we can split these compressed parameters into the original ones as needed to make predictions for test cases we show in detail how to compress parameters for logistic sequence prediction models experiments on both simulated and real data demonstrate that a huge number of parameters can indeed be reduced by our compression method less,A Method for Compressing Parameters in Bayesian Models with Application to Logistic Sequence Prediction Models,"30 November, 2007"
996,Radford Neal,i describe a new markov chain method for sampling from the distribution of the state sequences in a nonlinear state space model given the observation sequence this method updates all states in the sequence simultaneously using an embedded hidden markov model hmm an update begins with the creation of a pool of k states at each time by applying some markov chain update to the current state these pools define an embedded hmm whose states are indexes within this pool using the forwardbackward dynamic programming algorithm we can then efficiently choose a state sequence at random with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools i show empirically that when states at nearby times are strongly dependent embedded hmm sampling can perform better than metropolis methods that update one state at a time less,Markov Chain Sampling for Non-linear State Space Models Using Embedded Hidden Markov Models,"1 May, 2003"
997,David Wortman,we consider a participatory budgeting problem in which each voter submits a proposal for how to divide a single divisible resource such as money or time among several possible alternatives such as public projects or activities and these proposals must be aggregated into a single aggregate division under ell preferences for which a voters disutility is given by the ell distance between the aggregate division and the division he or she most prefers the social welfaremaximizing mechanism which minimizes the average ell distance between the outcome and each voters proposal is incentive compatible goel et al however it fails to satisfy the natural fairness notion of proportionality placing too much weight on majority preferences leveraging a connection between market prices and the generalized median rules of moulin we introduce the independent markets mechanism which is both incentive compatible and proportional we unify the social welfaremaximizing mechanism and the independent markets mechanism by defining a broad class of moving phantom mechanisms that includes both we show that every moving phantom mechanism is incentive compatible finally we characterize the social welfaremaximizing mechanism as the unique paretooptimal mechanism in this class suggesting an inherent tradeoff between pareto optimality and proportionality less,Truthful Aggregation of Budget Proposals,"21 January, 2022"
998,David Wortman,we initiate the study of incentivecompatible forecasting competitions in which multiple forecasters make predictions about one or more events and compete for a single prize we have two objectives to incentivize forecasters to report truthfully and to award the prize to the most accurate forecaster proper scoring rules incentivize truthful reporting if all forecasters are paid according to their scores however incentives become distorted if only the bestscoring forecaster wins a prize since forecasters can often increase their probability of having the highest score by reporting more extreme beliefs in this paper we introduce two novel forecasting competition mechanisms our first mechanism is incentive compatible and guaranteed to select the most accurate forecaster with probability higher than any other forecaster moreover we show that in the standard singleevent twoforecaster setting and under mild technical conditions no other incentivecompatible mechanism selects the most accurate forecaster with higher probability our second mechanism is incentive compatible when forecasters beliefs are such that information about one event does not lead to belief updates on other events and it selects the best forecaster with probability approaching as the number of events grows our notion of incentive compatibility is more general than previous definitions of dominant strategy incentive compatibility in that it allows for reports to be correlated with the event outcomes moreover our mechanisms are easy to implement and can be generalized to the related problems of outputting a ranking over forecasters and hiring a forecaster with high accuracy on future events less,Incentive-Compatible Forecasting Competitions,"7 September, 2021"
999,David Wortman,interpretability is an elusive but highly soughtafter characteristic of modern machine learning methods recent work has focused on interpretability via textitexplanations which justify individual model predictions in this work we take a step towards reconciling machine explanations with those that humans produce and prefer by taking inspiration from the study of explanation in philosophy cognitive science and the social sciences we identify key aspects in which these human explanations differ from current machine explanations distill them into a list of desiderata and formalize them into a framework via the notion of textitweight of evidence from information theory finally we instantiate this framework in two simple applications and show it produces intuitive and comprehensible explanations less,Weight of Evidence as a Basis for Human-Oriented Explanations,"29 October, 2019"
1000,David Wortman,we study voronoi diagrams for distance functions that add together two convex functions each taking as its argument the difference between cartesian coordinates of two planar points when the functions do not grow too quickly then the voronoi diagram has linear complexity and can be constructed in nearlinear randomized expected time additionally the level sets of the distances from the sites form a family of pseudocircles in the plane all cells in the voronoi diagram are connected and the set of bisectors separating any one cell in the diagram from each of the others forms an arrangement of pseudolines in the plane we apply these results to the smoothed distance or biotope transform metric a geometric analogue of the jaccard distance whose voronoi diagrams can be used to determine the dilation of a star network with a given hub for sufficiently closely spaced points in the plane the voronoi diagram of smoothed distance has linear complexity and can be computed efficiently we also experiment with a variant of lloyds algorithm adapted to smoothed distance to find uniformly spaced point samples with exponentially decreasing density around a given point less,"Dilation, smoothed distance, and minimization diagrams of convex functions","13 May, 2010"
1001,David Wortman,we present an on log ntime algorithm for the following problem given a finite metric space x create a startopology network with the points of x as its leaves such that the distances in the star are at least as large as in x with minimum dilation as part of our algorithm we solve in the same time bound the parametric negative cycle detection problem given a directed graph with edge weights that are increasing linear functions of a parameter lambda find the smallest value of lambda such that the graph contains no negativeweight cycles less,Optimal Embedding Into Star Metrics,"3 May, 2009"
1002,David Wortman,we provide an efficient reduction from the problem of querying approximate multiplicatively weighted farthest neighbors in a metric space to the unweighted problem combining our techniques with coresets for approximate unweighted farthest neighbors we show how to find epsilonapproximate farthest neighbors in time olog n per query in ddimensional euclidean space for any constants d and epsilon as an application we find an on log n expected time algorithm for choosing the center of a star topology network connecting a given set of points so as to approximately minimize the maximum dilation between any pair of points less,Approximate Weighted Farthest Neighbors and Minimum Dilation Stars,"7 February, 2006"
1003,Gary Bader,motivated by the fact that many relations cross the sentence boundary there has been increasing interest in documentlevel relation extraction docre docre requires integrating information within and across sentences capturing complex interactions between mentions of entities most existing methods are pipelinebased requiring entities as input however jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps in this paper we develop a sequencetosequence approach seqrel that can learn the subtasks of docre entity extraction coreference resolution and relation extraction endtoend replacing a pipeline of taskspecific components using a simple strategy we call entity hinting we compare our approach to existing pipelinebased methods on several popular biomedical datasets in some cases exceeding their performance we also report the first endtoend results on these datasets for future comparison finally we demonstrate that under our model an endtoend approach outperforms a pipelinebased approach our code data and trained models are available at urlhttpsgithubcomjohngiorgiseqrel an online demo is available at urlhttpssharestreamlitiojohngiorgiseqrelmaindemopy less,A sequence-to-sequence approach for document-level relation extraction,"10 April, 2022"
1004,Gary Bader,named entity recognition ner and relation extraction re are two important tasks in information extraction and retrieval ie ir recent work has demonstrated that it is beneficial to learn these tasks jointly which avoids the propagation of error inherent in pipelinebased systems and improves performance however stateoftheart joint models typically rely on external natural language processing nlp tools such as dependency parsers limiting their usefulness to domains eg news where those tools perform well the few neural endtoend models that have been proposed are trained almost completely from scratch in this paper we propose a neural endtoend model for jointly extracting entities and their relations which does not rely on external nlp tools and which integrates a large pretrained language model because the bulk of our models parameters are pretrained and we eschew recurrence for selfattention our model is fast to train on datasets across domains our model matches or exceeds stateoftheart performance sometimes by a large margin less,End-to-end Named Entity Recognition and Relation Extraction using Pre-trained Language Models,"20 December, 2019"
1005,Gary Bader,"Mitochondrial metabolites regulate leukaemic and normal stem cells by affecting epigenetic marks. How mitochondrial enzymes localize to the nucleus to control stem cell function is less understood. We discovered that the mitochondrial metabolic enzyme hexokinase 2 (HK2) localizes to the nucleus in leukaemic and normal haematopoietic stem cells. Overexpression of nuclear HK2 increases leukaemic stem cell properties and decreases differentiation, whereas selective nuclear HK2 knockdown promotes differentiation and decreases stem cell function. Nuclear HK2 localization is phosphorylation-dependent, requires active import and export, and regulates differentiation independently of its enzymatic activity. HK2 interacts with nuclear proteins regulating chromatin openness, increasing chromatin accessibilities at leukaemic stem cell-positive signature and DNA-repair sites. Nuclear HK2 overexpression ",The metabolic enzyme hexokinase 2 localizes to the nucleus in AML and normal haematopoietic stem and progenitor cells to maintain stemness,2022-06-06 00:00:00
1006,Gary Bader,"Hematopoietic stem cell (HSC) dormancy is understood as supportive of HSC function and their long-term integrity. While regulation of stress responses incurred as a result of HSC activation is recognized as important in maintaining stem cell function, little is understood of the preventative machinery present in human HSCs that may serve to resist their activation and promote HSC self-renewal. We demonstrate that the transcription factor PLAG1 is essential for long-term HSC function and when overexpressed endows a 15.6-fold enhancement in the frequency of functional HSC in stimulatory conditions. Genome-wide measures of chromatin occupancy and PLAG1-directed gene expression changes combined with functional measures reveal that PLAG1 dampens protein synthesis, restrains cell growth and division, and enhances survival, with the primitive cell advantages it imparts being attenuated by addition of ",PLAG1 dampens protein synthesis to promote human hematopoietic stem cell self-renewal,2022-06-02 00:00:00
1007,Gary Bader,"AML cells are arranged in a hierarchy with stem/progenitor cells giving rise to more differentiated bulk cells. Despite the importance of stem/progenitors in the pathogenesis of AML, the determinants of the AML stem/progenitor state are not fully understood. Through a comparison of genes that are significant for growth and viability of AML cells by way of a CRISPR screen, with genes that are differentially expressed in leukemia stem cells (LSC), we identified importin 11 (IPO11) as a novel target in AML. Importin 11 (IPO11) is a member of the importin β family of proteins that mediate transport of proteins across the nuclear membrane. In AML, knockdown of IPO11 decreased growth, reduced engraftment potential of LSC, and induced differentiation. Mechanistically, we identified the transcription factors BZW1 and BZW2 as novel cargo of IPO11. We further show that BZW1/2 mediate a transcriptional signature that",IPO11 regulates the nuclear import of BZW1/2 and is necessary for AML cells and stem cells,2022/5
1008,Gary Bader,"Knowing which proteins interact with each other is essential information for understanding how most biological processes at the cellular and organismal level operate and how their perturbation can cause disease. Continuous technical and methodological advances over the last two decades have led to many genome-wide systematically-generated protein–protein interaction (PPI) maps. To help store, visualize, analyze and disseminate these specialized experimental datasets via the web, we developed the freely-available Open-source Protein Interaction Platform (openPIP) as a customizable web portal designed to host experimental PPI maps. Such a portal is often required to accompany a paper describing the experimental data set, in addition to depositing the data in a standard repository. No coding skills are required to set up and customize the database and web portal. OpenPIP has been used to build the","OpenPIP: An Open-source Platform for Hosting, Visualizing and Analyzing Protein Interaction Data",2022-04-26 00:00:00
1009,Gary Bader,"Motivated by the fact that many relations cross the sentence boundary, there has been increasing interest in document-level relation extraction (DocRE). DocRE requires integrating information within and across sentences, capturing complex interactions between mentions of entities. Most existing methods are pipeline-based, requiring entities as input. However, jointly learning to extract entities and relations can improve performance and be more efficient due to shared parameters and training steps. In this paper, we develop a sequence-to-sequence approach, seq2rel, that can learn the subtasks of DocRE (entity extraction, coreference resolution and relation extraction) end-to-end, replacing a pipeline of task-specific components. Using a simple strategy we call entity hinting, we compare our approach to existing pipeline-based methods on several popular biomedical datasets, in some cases exceeding their performance. We also report the first end-to-end results on these datasets for future comparison. Finally, we demonstrate that, under our model, an end-to-end approach outperforms a pipeline-based approach. Our code, data and trained models are available at",A sequence-to-sequence approach for document-level relation extraction,2022-04-03 00:00:00
1010,Gary Bader,"The critical functions of the human liver are coordinated through the interactions of hepatic parenchymal and non‐parenchymal cells. Recent advances in single‐cell transcriptional approaches have enabled an examination of the human liver with unprecedented resolution. However, dissociation‐related cell perturbation can limit the ability to fully capture the human liver’s parenchymal cell fraction, which limits the ability to comprehensively profile this organ. Here, we report the transcriptional landscape of 73,295 cells from the human liver using matched single‐cell RNA sequencing (scRNA‐seq) and single‐nucleus RNA sequencing (snRNA‐seq). The addition of snRNA‐seq enabled the characterization of interzonal hepatocytes at a single‐cell resolution, revealed the presence of rare subtypes of liver mesenchymal cells, and facilitated the detection of cholangiocyte progenitors that had only been observed during ","Single‐cell, single‐nucleus, and spatial RNA sequencing of the human liver identifies cholangiocyte and mesenchymal heterogeneity",2022/4
1011,Gary Bader,"Gene expression profiling and proteome analysis of normal and malignant hematopoietic stem cells (HSCs) point to shared core stemness properties. However, discordance between mRNA and protein signatures highlights an important role for post-transcriptional regulation by microRNAs (miRNAs) in governing this critical nexus. Here, we identify miR-130a as a regulator of HSC self-renewal and differentiation. Enforced expression of miR-130a impairs B lymphoid differentiation and expands long-term HSCs. Integration of protein mass spectrometry and chimeric AGO2 crosslinking and immunoprecipitation (CLIP) identifies TBL1XR1 as a primary miR-130a target, whose loss of function phenocopies miR-130a overexpression. Moreover, we report that miR-130a is highly expressed in t(8;21) acute myeloid leukemia (AML), where it is critical for maintaining the oncogenic molecular program mediated by the AML1",Identification of the global miR-130a targetome reveals a role for TBL1XR1 in hematopoietic stem cell self-renewal and t (8; 21) AML,2022-03-08 00:00:00
1012,Gary Bader,"Despite extensive analysis of pRB phosphorylation in vitro, how this modification influences development and homeostasis in vivo is unclear. Here, we show that homozygous Rb∆K4 and Rb∆K7 knock‐in mice, in which either four or all seven phosphorylation sites in the C‐terminal region of pRb, respectively, have been abolished by Ser/Thr‐to‐Ala substitutions, undergo normal embryogenesis and early development, notwithstanding suppressed phosphorylation of additional upstream sites. Whereas Rb∆K4 mice exhibit telomere attrition but no other abnormalities, Rb∆K7 mice are smaller and display additional hallmarks of premature aging including infertility, kyphosis, and diabetes, indicating an accumulative effect of blocking pRb phosphorylation. Diabetes in Rb∆K7 mice is insulin‐sensitive and associated with failure of quiescent pancreatic β‐cells to re‐enter the cell cycle in response to mitogens, resulting in",Hypophosphorylated pRb knock‐in mice exhibit hallmarks of aging and vitamin C‐preventable diabetes,2022-02-15 00:00:00
1013,Gary Bader,"Muscle diseases share common pathological features suggesting common underlying mechanisms. We hypothesized there is a common set of genes dysregulated across muscle diseases compared to healthy muscle and that these genes correlate with severity of muscle disease. We performed meta-analysis of transcriptional pro les of muscle biopsies from human muscle diseases and healthy controls. Studies obtained from public microarray repositories ful lling quality criteria were divided into six categories: i) Immobility, ii) in ammatory myopathies, iii) ICU acquired weakness (ICUAW), iv) congenital muscle diseases, v) chronic systemic diseases, vi) motor neuron disease. Patient cohorts were separated in discovery and validation cohorts retaining roughly equal proportions of samples for the disease categories. To remove bias towards a speci c muscle disease category we repeated the meta-analysis ve times by removing data sets corresponding to one muscle disease class at a time in a “leave-onedisease-out” analysis. We used 636 muscle tissue samples from 30 independent cohorts to identify a 52 gene signature (36 upregulated and 16 down-regulated genes). We validated the discriminatory power of this signature in 657 muscle biopsies from 12 additional patient cohorts encompassing ve categories of muscle diseases with an area under the receiver operating characteristic curve of 0.91, 83% sensitivity, and 85.3% speci city. The expression score of the gene signature inversely correlated with quadriceps muscle mass (r=-0.50, p-value= 0.011) in ICUAW and shoulder abduction strength (r=-0.77, p-value= 0.014) in amyotrophic lateral",Comprehensive Multi-cohort Transcriptional Meta-analysis of Muscle Diseases Identifies a Signature of Disease Severity,2022-01-11 00:00:00
1014,Gary Bader,"The Reactome Knowledgebase (https://reactome.org), an Elixir core resource, provides manually curated molecular details across a broad range of physiological and pathological biological processes in humans, including both hereditary and acquired disease processes. The processes are annotated as an ordered network of molecular transformations in a single consistent data model. Reactome thus functions both as a digital archive of manually curated human biological processes and as a tool for discovering functional relationships in data such as gene expression profiles or somatic mutation catalogs from tumor cells. Recent curation work has expanded our annotations of normal and disease-associated signaling processes and of the drugs that target them, in particular infections caused by the SARS-CoV-1 and SARS-CoV-2 coronaviruses and the host response to infection. New tools support better ",The reactome pathway knowledgebase 2022,2022-01-07 00:00:00
1015,Gary Bader,integrating expression data with gene interactions in a network is essential for understanding the functional organization of the cells consequently knowledge of interaction types in biological networks is important for data interpretation signing of regulatory networks siren plugin for cytoscape is an opensource java tool for discrimination of interaction type activatory or inhibitory in gene regulatory networks utilizing an information theory based concept siren seeks to identify the interaction type of pairs of genes by examining their corresponding gene expression profiles we introduce siren a fast and memory efficient tool with low computational complexity that allows the user to easily consider it as a complementary approach for many network reconstruction methods siren allows biologists to use independent expression data to predict interaction types for known gene regulatory networks where reconstruction methods do not provide any information about the nature of their interaction types the siren cytoscape plugin is implemented in java and is freely available at httpbaderlaborgsoftwaresirenplugin and via the cytoscape app manager less,SIREN Cytoscape plugin: Interaction Type Discrimination in Gene Regulatory Networks,"16 December, 2015"
1016,Chris Beck,"In the last decade, decision diagrams (DDs) have been the basis for a large array of novel approaches for modeling and solving optimization problems. Many techniques now use DDs as a key tool to achieve state-of-the-art performance within other optimization paradigms, such as integer programming and constraint programming. This paper provides a survey of the use of DDs in discrete optimization, particularly focusing on recent developments. We classify these works into two groups based on the type of diagram (i.e., exact or approximate) and present a thorough description of their use. We discuss the main advantages of DDs, point out major challenges, and provide directions for future work.",Decision Diagrams for Discrete Optimization: A Survey of Recent Advances,2022-03-22 00:00:00
1017,Chris Beck,"We investigate the novel Two-stage Cutting Stock Problem with Flexible Length and Flexible Demand (2SCSP-FF): orders for rectangular items must be cut from rectangular stocks using guillotine cuts with the objective to minimize waste. Motivated by our industrial partner and different from problems in the literature, the 2SCSP-FF allows both the length of individual items and the total area of orders to vary within customer-specified intervals. We develop constraint programming (CP) and mixed-integer programming models, with the most successful coming from the adaptation of CP scheduling techniques. Numerical results show that this CP model has orders of magnitude smaller memory requirements and is the only model-based approach investigated that can solve industrial instances.",Packing by Scheduling: Using Constraint Programming to Solve a Complex 2D Cutting Stock Problem,2022
1018,Chris Beck,"Bipartite b-matching is a classical model that is used for utility maximization in various applications such as marketing, healthcare, education, and general resource allocation. Multi-attribute diverse weighted bipartite b-matching (MDWBM) balances the quality of the matching with its diversity. The recent paper by Ahmadi et al.(2020) introduced the MDWBM but presented an incorrect mixed integer quadra-tic program (MIQP) and a flawed local exchange algorithm. In this work, we develop two constraint programming (CP) models, a binary quadratic programming (BQP) model, and a quadratic unconstrained binary optimization (QUBO) model for both the unconstrained and constrained MDWBM. A thorough empirical evaluation using commercial solvers and specialized QUBO hardware shows that the hardware-based QUBO approach dominates, finding best-known solutions on all tested instances up to an order of",Model-Based Approaches to Multi-attribute Diverse Matching,2022
1019,Chris Beck,"The emergence of specialized hardware, such as quantum computers and Digital/CMOS annealers, and the slowing of performance growth of general-purpose hardware raises an important question for our community: how can the highperformance, specialized solvers be used for planning and scheduling problems? In this work, we focus on the job-shop scheduling problem (JSP) and Quadratic Unconstrained Binary Optimization (QUBO) models, the mathematical formulation shared by a number of novel hardware platforms. We study two direct QUBO models of JSP and propose a novel large neighborhood search (LNS) approach, that hybridizes a QUBO model with constraint programming (CP). Empirical results show that our LNS approach significantly outperforms classical CP-based LNS methods and a mixed integer programming model, while being competitive with CP for large problem instances. This work is the first approach that we are aware of that can solve non-trivial JSPs using QUBO hardware, albeit as part of a hybrid algorithm.",Solving Job-Shop Scheduling Problems with QUBO-Based Specialized Hardware,2022
1020,Chris Beck,"While numeric variables play an important, sometimes central, role in many planning problems arising from real world scenarios, most of the currently available heuristic search planners either do not support such variables or impose heavy restrictions on them. In particular, most admissible heuristics are restricted to domains where actions can only change numeric variables by predetermined constants. In this work, we consider the setting of optimal numeric planning with linear effects, where actions can have numeric effects that assign the result of the evaluation of a linear formula. We extend a recent formulation of Numeric LM-cut for simple effects by adding conditional effects and second-order simple effects, allowing the heuristic to produce admissible estimates for tasks with linear numeric effects. Empirical comparison shows that the proposed LM-cut heuristics favorably compete with the currently available state-of-the-art heuristics and achieve significant improvement in coverage in the domains with secondorder simple effects.",LM-Cut Heuristics for Optimal Linear Numeric Planning,2022
1021,Chris Beck,"Satisficing heuristic search such as greedy best-first search (GBFS) suffers from local minima, regions where heuristic values are inaccurate and a good node has a worse heuristic value than other nodes. Search algorithms that incorporate exploration mechanisms in GBFS empirically reduce the search effort to solve difficult problems. Although some of these methods entirely ignore the guidance of a heuristic during their exploration phase, intuitively, a good heuristic should have some bound on its inaccuracy, and exploration mechanisms should exploit this bound. In this paper, we theoretically analyze what a good node is for satisficing heuristic search algorithms and show that the heuristic value of a good node has an upper bound if a heuristic satisfies a certain property. Then, we propose biased exploration mechanisms which select lower heuristic values with higher probabilities. In the experiments using synthetic graph search problems and classical planning benchmarks, we show that the biased exploration mechanisms can be useful. In particular, one of our methods, Softmin-Type (h), significantly outperforms other GBFS variants in classical planning and improves the performance of Type-LAMA, a state-of-the-art classical planner.",Biased Exploration for Satisficing Heuristic Search,2022
1022,Chris Beck,"Decision-making in large-scale compute clouds relies on accurate workload modeling. Unfortunately, prior models have proven insufficient in capturing the complex correlations in real cloud workloads. We introduce the first model of large-scale cloud workloads that captures long-range inter-job correlations in arrival rates, resource requirements, and lifetimes. Our approach models workload as a three-stage generative process, with separate models for:(1) the number of batch arrivals over time,(2) the sequence of requested resources, and (3) the sequence of lifetimes. Our lifetime model is a novel extension of recent work in neural survival prediction. It represents and exploits inter-job correlations using a recurrent neural network. We validate our approach by showing it is able to accurately generate the production virtual machine workload of two real-world cloud providers.","Generating Complex, Realistic Cloud Workloads using Recurrent Neural Networks",2021-10-26 00:00:00
1023,Chris Beck,"Cut generation and lifting are key components for the performance of state-of-the-art mathematical programming solvers. This work proposes a new general cut-and-lift procedure that exploits the combinatorial structure of 0–1 problems via a binary decision diagram (BDD) encoding of their constraints. We present a general framework that can be applied to a wide range of binary optimization problems and show its applicability for second-order conic inequalities. We identify conditions for which our lifted inequalities are facet-defining and derive a new BDD-based cut generation linear program. Such a model serves as a basis for a max-flow combinatorial algorithm over the BDD that can be applied to derive valid cuts more efficiently. Our numerical results show encouraging performance when incorporated into a state-of-the-art mathematical programming solver, significantly reducing the root node gap, increasing ",A combinatorial cut-and-lift procedure with an application to 0–1 second-order conic programming,2021-08-11 00:00:00
1024,Chris Beck,"Recent work has demonstrated that neural sequence models can successfully solve combinatorial search problems such as program synthesis and routing problems. In these scenarios, the beam search algorithm is typically used to produce a set of high-likelihood candidate sequences that are evaluated to determine if they satisfy the goal criteria. If none of the candidates satisfy the criteria, the beam search can be restarted with a larger beam size until a satisfying solution is found. Inspired by works in combinatorial and heuristic search, we investigate whether heavy-tailed behavior can be observed in the search effort distribution of complete beam search in goal-oriented neural sequence decoding. We analyze four goal-oriented decoding tasks and find that the search effort of beam search exhibits fat- and heavy-tailed behavior. Following previous work on heavy-tailed behavior in search",Heavy-Tails and Randomized Restarting Beam Search in Goal-Oriented Neural Sequence Decoding,2021-07-05 00:00:00
1025,Chris Beck,we examine subhaloes and galaxies residing in a simulated lcdm galaxy cluster mrm crittimesmodoth produced by hydrodynamical codes ranging from classic smooth particle hydrodynamics sph newer sph codes adaptive and moving mesh codes these codes use subgrid models to capture galaxy formation physics we compare how well these codes reproduce the same subhaloesgalaxies in gravity only nonradiative hydrodynamics and full feedback physics runs by looking at the overall subhalogalaxy distribution and on an individual objects basis we find the subhalo population is reproduced to within lesssim for both dark matter only and nonradiative runs with individual objects showing codetocode scatter of lesssim dex although the gas in nonradiative simulations shows significant scatter including feedback physics significantly increases the diversity subhalo mass and vmax distributions vary by approx the galaxy populations also show striking codetocode variations although the tullyfisher relation is similar in almost all codes the number of galaxies with modothlesssim mlesssim modoth can differ by a factor of individual galaxies show codetocode scatter of sim dex in stellar mass moreover strong systematic differences exist with some codes producing galaxies smaller than others the diversity partially arises from the inclusionabsence of agn feedback our results combined with our companion papers demonstrate that subgrid physics is not just subject to finetuning but the complexity of building galaxies in all environments remains a challenge we argue even basic galaxy properties such as the stellar mass to halo mass should be treated with errors bars of sim dex less,nIFTY galaxy cluster simulations III: The Similarity & Diversity of Galaxies & Subhaloes,"10 February, 2016"
1026,Barend Beekhuizen,"Complement coercion interpretation is typically operationalized in terms of verb paraphrases (she finished the book > she finished reading the book), reflecting the construal of a specific event triggered by the direct object (cf. Pustejovsky & Bouillon). However, when given the option, participants frequently, and selectively, refrain from providing verb paraphrases for naturally occurring sentences. We present a corpus study of the pragmatic factors that affect the rate of refraining from giving a verb paraphrases, followed by two experiments. First, a fill-in-the-blank experiment shows that minimally different clause types containing complement coercion yield different rates of verb paraphrase. Second, we pair complement coercion sentences with possible verb paraphrases and obtain semantic (entailment) judgements from participants. Taken together, the results present a puzzle; participants’ likelihood to produce a verbal coercion interpretation is modulated by contextual factors, while their semantic comprehension of potential verb paraphrases for the same sentences is not.",Two measures for complement coercion interpretation: Interpretation vs production for complement coercion,2022
1027,Barend Beekhuizen,"Noun compounding is a productive word formation strategy in English. Low-frequency (hence: ‘novel’) noun compounds (NCs) like “animal teacher” elicit diverse responses when presented without a sentence context, suggesting that they are semantically underspecified. We explore exactly how much the role of context changes default interpretations of novel English NCs. We consider compounds with two plausible interpretations: OF-relations (teacher OF animals) and IS-relations (animal who IS a teacher). We predicted context would increase participants’ ratings of the plausibility of given compound interpretations. Corroborating our predictions, participant plausibility ratings showed OF-relations were more accessible out-of-context, and IS-relations were rated with less certainty. Mixed-effects modelling showed a significant interaction between context and relation type. The results suggest context increases interpretation certainty, particularly in OF-dominant compounds. However, several groups of cases notably defy this pattern. Finally, we use computational language models to identify aspects of context that are critical to novel NC interpretation.",Exploring context’s role in the interpretation of novel noun compounds,1905-07-14 00:00:00
1028,Barend Beekhuizen,"Existing (experimental and computational) linguistic work uses participant paraphrases as a stand-in for event interpretation in complement coercion sentences (eg she finished the coffee→ she finished drinking the coffee). We present crowdsourcing data and modelling that supports broadening this conception. In particular, our results suggest that sentences where many participants do not give a paraphrase, or where many different paraphrases are given are informative about to how complement coercion is interpreted in naturalistic contexts.",Remodelling complement coercion interpretation,1905-07-14 00:00:00
1029,Barend Beekhuizen,"In this paper, we examine a specific type of Relative Clause (RC). We look at the construction consisting of a ‘light noun’, that is, a noun with highly non-specific lexical content which does not do referential work, plus a relative clause. It has generally been assumed that the functional contribution of RCs is to narrow the set of referents of the head noun to only those for which the predicate of the RC holds true. However, the ‘Light Head RC construction’ (LHRC) has various interactional affordances that mostly revolve around characterizing referents with discourse-relevant properties rather than establishing reference. We argue that these various functions of LHRCs revolve around participants’ orientations to categoryhood. Data are in English and Dutch.",“Something that’s very American”: The Interactional Role of Light-Head Relative Clauses,2022
1030,Barend Beekhuizen,"While distributional semantic models (DSMs) can successfully capture the similarity structure within a semantic domain, less is known about their ability to represent abstract semantic properties that hold across domains. Such properties can form the basis for abstract semantic classes that are a crucial aspect of human semantic knowledge. For example, the abstract class of extreme adjectives (such as brilliant and freezing) spans a wide range of domains (here, INTELLIGENCE and TEMPERATURE). Using a model that compares query items to an aggregate DSM representation of a set of extreme adjectives, we show that novel adjectives can be classified accurately, supporting the insight that a cross-domain property like extremeness can be captured in a word’s DSM representation. We then use the extremeness classifier to model the emergence of intensifier meaning in adverbs, demonstrating, in a separate task, the effectiveness of detecting this abstract semantic property.",A formidable ability: detecting adjectival extremeness with DSMs,2021/8
1031,Barend Beekhuizen,"Lexical ambiguity—the phenomenon of a single word having multiple, distinguishable senses—is pervasive in language. Both the degree of ambiguity of a word (roughly, its number of senses) and the relatedness of those senses have been found to have widespread effects on language acquisition and processing. Recently, distributional approaches to semantics, in which a word's meaning is determined by its contexts, have led to successful research quantifying the degree of ambiguity, but these measures have not distinguished between the ambiguity of words with multiple related senses versus multiple unrelated meanings. In this work, we present the first assessment of whether distributional meaning representations can capture the ambiguity structure of a word, including both the number and relatedness of senses. On a very large sample of English words, we find that some, but not all, distributional semantic",Probing lexical ambiguity: word vectors encode number and relatedness of senses,2021/5
1032,Barend Beekhuizen,"Conversational interaction involves integrating the perspectives of multiple interlocutors with varying knowledge and beliefs. An issue that has received little attention in cognitive modeling of pragmatics is how speakers deal with the choice of words like come that are inherently perspectival. How do such lexical perspectival items fit into a speaker's overall integration of conversational perspective? We present new experimental results on production of perspectival words, in which speakers have varying degrees of certainty about their addressee's perspective. We show that the Multiple Perspectives Model closely fits the empirical data, lending support to the hypothesis that use of perspectival words can be naturally accommodated as a type of conversational perspective taking.",Come Together: Integrating Perspective Taking and Perspectival Expressions,2021
1033,Barend Beekhuizen,"Language is inherently flexible: people continually generalize over observed data to produce creative linguistic expressions. This process is constrained by a wide range of factors, whose interaction is not fully understood. We present a novel study of the creative use of verb constructions ``in the wild'', in a very large social media corpus. Our first experiment confirms on this large-scale data the important interaction of category variability and item similarity within creative extensions in actual language use. Our second experiment confirms the novel hypothesis that low-frequency exemplars may play a role in generalization by signaling the area of semantic space where creative coinages occur.",Coin it up: Generalization of creative constructions in the wild,2021
1034,Barend Beekhuizen,"Most words are polysemous, denoting related but distinct senses (eg, chicken referring to an ANIMAL or to FOOD). Jager, Green, and Clelland (2016, LCN) reported facilitatory effects of polysemy on lexical processing that interacted with word frequency and type of task. We undertook a broader investigation of interactions between polysemy and several sublexical, lexical, and semantic properties of words, to determine whether such interactions could explain inconsistent effects of polysemy reported in the literature. Estimating degree of polysemy using dictionary sense counts, we studied the interaction between polysemy and these other properties when predicting performance in lexical decision and semantic categorization mega-studies. We observed interactions between polysemy and both lexical and semantic, but not sublexical, variables. Our results, while not replicating the exact effects reported by Jager and ","Are Polysemy Effects Modulated by Sublexical, Lexical, and Semantic Factors?",2020
1035,Barend Beekhuizen,"A key challenge for children in language acquisition is to learn the mapping of words to mental categories, since this mapping varies greatly from language to language. The errors children make in this process are very informative regarding the development of lexical semantic categories; in particular, how children overextend a word to an inappropriate exemplar provides a window onto the mechanisms that underlie their categorization processes. We perform a large-scale quantitative analysis of the detailed patterns of children’s errors in the domain of color, finding evidence that these error patterns are driven by an interaction between domain general principles of categorization, and children’s developing knowledge of the semantics of color. Our results suggest that, while domain general processes play a role throughout development, their influence varies across ages according to their use of domain specific (conceptual) knowledge, which gradually increases over time.",Coloring Outside the Lines: Error Patterns in Children's Acquisition of Color Terms.,2020
1036,Barend Beekhuizen,"Distributional semantic models (DSMs) are substantially varied in the types of semantic similarity that they output. Despite this high variance, the different types of similarity are often conflated as a monolithic concept in models of behavioural data. We apply the insight that word2vec’s representations can be used for capturing both paradigmatic similarity (substitutability) and syntagmatic similarity (co-occurrence) to two sets of experimental findings (semantic priming and the effect of semantic neighbourhood density) that have previously been modeled with monolithic conceptions of DSM-based semantic similarity. Using paradigmatic and syntagmatic similarity based on word2vec, we show that for some tasks and types of items the two types of similarity play complementary explanatory roles, whereas for others, only syntagmatic similarity seems to matter. These findings remind us that it is important to develop more precise accounts of what we believe our DSMs represent, and provide us with novel perspectives on established behavioural patterns.",Untangling Semantic Similarity: Modeling Lexical Processing Experiments with Distributional Semantic Models.,2020
1037,Barend Beekhuizen,computational research on error detection in second language speakers has mainly addressed clear grammatical anomalies typical to learners at the beginnertointermediate level we focus instead on acquisition of subtle semantic nuances of english indefinite pronouns by nonnative speakers at varying levels of proficiency we first lay out theoretical linguistically motivated hypotheses and supporting empirical evidence on the nature of the challenges posed by indefinite pronouns to english learners we then suggest and evaluate an automatic approach for detection of atypical usage patterns demonstrating that deep learning architectures are promising for this task involving nuanced semantic anomalies less,Say Anything: Automatic Semantic Infelicity Detection in L2 English Indefinite Pronouns,"17 September, 2019"
1038,Kieran Campbell,"When a person is pregnant, a key question is how to establish the “date” of the pregnancy. Classically, the date was based on the last menstrual period (LMP). For the past 3 decades or more, in high-resource countries, this has been done using “hospital-grade” ultrasound machines, with testing performed by trained sonographers. In many parts of the world, neither the machines nor the trained sonographers are accessible. In an article published in NEJM Evidence, Pokaprakarn et al.1 asked whether a low-cost handheld ultrasound device combined with artificial intelligence (AI) could substitute for the expensive machines and trained sonographers.",The basics of machine learning,2022-04-26 00:00:00
1039,Kieran Campbell,"Many practical applications require optimization of multiple, computationally expensive, and possibly competing objectives that are well-suited for multi-objective Bayesian optimization (MOBO) procedures. However, for many types of biomedical data, measures of data analysis workflow success are often heuristic and therefore it is not known a priori which objectives are useful. Thus, MOBO methods that return the full Pareto front may be suboptimal in these cases. Here we propose a novel MOBO method that adaptively updates the scalarization function using properties of the posterior of a multi-output Gaussian process surrogate function. This approach selects useful objectives based on a flexible set of desirable criteria, allowing the functional form of each objective to guide optimization. We demonstrate the qualitative behaviour of our method on toy data and perform proof-of-concept analyses of single-cell RNA sequencing and highly multiplexed imaging datasets.",Multi-objective Bayesian Optimization with Heuristic Objectives for Biomedical and Molecular Data Analysis Workflows,2022-01-01 00:00:00
1040,Kieran Campbell,"Functionally characterizing the genetic alterations that drive pancreatic cancer progression is a prerequisite for Precision Medicine. Here, we developed a somatic CRISPR/Cas9 mutagenesis screen to assess the transforming potential of 125 recurrently mutated long-tail pancreatic cancer genes, which revealed USP15 and SCAF1 as novel and potent Pancreatic ductal adenocarcinoma PDAC tumor suppressors, with USP15 functioning in a haplo-insufficient manner. Mechanistically, we found that loss of USP15 leads to reduced inflammatory responses associated with TNFa, TGF-b and IL6 signaling and sensitizes pancreatic cancer cells to PARP inhibition and gemcitabine. Similarly, genetic ablation of SCAF1 reduced inflammatory responses linked to TNFa,TGF-b and mTOR signaling and increased sensitivity to PARP inhibition. Furthermore, we identified that loss of SCAF1 resulted in the formation of a truncated inactive USP15 isoform at the expense of full length USP15, functionally coupling SACF1 and USP15. Notably, USP15 and SCAF1 mutations or copy number losses are observed in 31% of PDAC patients. Together, our results demonstrate the utility of in vivo CRISPR to integrate human cancer genomics with mouse modeling to delineate novel cancer driver genes USP15 and SCAF1 such as with potential prognostic and therapeutic implications.",In vivo CRISPR screens reveal SCAF1 and USP15 as novel drivers of pancreatic cancer,2022-01-01 00:00:00
1041,Kieran Campbell,"We conducted a cross-sectional study of all completed or actively enrolling randomized, interventional, clinical trials for the treatment of COVID-19 in the United States registered on www.clinicaltrials.gov as of August 10, 2020. We excluded trials of vaccines and other interventions intended to prevent COVID-19.",The Landscape of COVID-19 Research in the United States: a Cross-sectional Study of Randomized Trials Registered on ClinicalTrials. Gov,2022/1
1042,Kieran Campbell,"A major challenge in the analysis of highly multiplexed imaging data is the assignment of cells to a priori known cell types. Existing approaches typically solve this by clustering cells followed by manual annotation. However, these often require several subjective choices and cannot explicitly assign cells to an uncharacterized type. To help address these issues we present Astir, a probabilistic model to assign cells to cell types by integrating prior knowledge of marker proteins. Astir uses deep recognition neural networks for fast inference, allowing for annotations at the million-cell scale in the absence of a previously annotated reference. We apply Astir to over 2.4 million cells from suspension and imaging datasets and demonstrate its scalability, robustness to sample composition, and interpretable uncertainty estimates. We envision deployment of Astir either for a first broad cell type assignment or to accurately",Automated assignment of cell identity from single-cell multiplexed imaging and proteomic data,2021-12-15 00:00:00
1043,Kieran Campbell,"Progress in defining genomic fitness landscapes in cancer, especially those defined by copy number alterations (CNAs), has been impeded by lack of time-series single-cell sampling of polyclonal populations and temporal statistical models–. Here we generated 42,000 genomes from multi-year time-series single-cell whole-genome sequencing of breast epithelium and primary triple-negative breast cancer (TNBC) patient-derived xenografts (PDXs), revealing the nature of CNA-defined clonal fitness dynamics induced by TP53 mutation and cisplatin chemotherapy. Using a new Wright–Fisher population genetics model, to infer clonal fitness, we found that TP53 mutation alters the fitness landscape, reproducibly distributing fitness over a larger number of clones associated with distinct CNAs. Furthermore, in TNBC PDX models with mutated TP53, inferred fitness coefficients from CNA-based genotypes accurately",Clonal fitness inferred from time-series modelling of single-cell cancer genomes,2021/7
1044,Kieran Campbell,"Hereditary diffuse gastric cancer (HDGC) is a cancer syndrome caused by germline variants in CDH1, the gene encoding the cell–cell adhesion molecule E‐cadherin. Loss of E‐cadherin in cancer is associated with cellular dedifferentiation and poor prognosis, but the mechanisms through which CDH1 loss initiates HDGC are not known. Using single‐cell RNA sequencing, we explored the transcriptional landscape of a murine organoid model of HDGC to characterize the impact of CDH1 loss in early tumourigenesis. Progenitor populations of stratified squamous and simple columnar epithelium, characteristic of the mouse stomach, showed lineage‐specific transcriptional programs. Cdh1 inactivation resulted in shifts along the squamous differentiation trajectory associated with aberrant expression of genes central to gastrointestinal epithelial differentiation. Cytokeratin 7 (CK7), encoded by the differentiation",Modelling hereditary diffuse gastric cancer initiation using transgenic mouse‐derived gastric organoids and single‐cell sequencing,2021/7
1045,Kieran Campbell,"The recent boom in microfluidics and combinatorial indexing strategies, combined with low sequencing costs, has empowered single-cell sequencing technology. Thousands—or even millions—of cells analyzed in a single experiment amount to a data revolution in single-cell biology and pose unique data science problems. Here, we outline eleven challenges that will be central to bringing this emerging field of single-cell data science forward. For each challenge, we highlight motivating research questions, review prior work, and formulate open problems. This compendium is for established researchers, newcomers, and students alike, highlighting interesting and rewarding problems for the coming years.",Eleven grand challenges in single-cell data science,2020/12
1046,Kieran Campbell,"Endometrial carcinoma, the most common gynaecological cancer, develops from endometrial epithelium which is composed of secretory and ciliated cells. Pathologic classification is unreliable and there is a need for prognostic tools. We used single cell sequencing to study organoid model systems derived from normal endometrial endometrium to discover novel markers specific for endometrial ciliated or secretory cells. A marker of secretory cells (MPST) and several markers of ciliated cells (FAM92B, WDR16, and DYDC2) were validated by immunohistochemistry on organoids and tissue sections. We performed single cell sequencing on endometrial and ovarian tumours and found both secretory‐like and ciliated‐like tumour cells. We found that ciliated cell markers (DYDC2, CTH, FOXJ1, and p73) and the secretory cell marker MPST were expressed in endometrial tumours and positively correlated with disease",Single cell transcriptomes of normal endometrial derived organoids uncover novel cell type markers and cryptic differentiation of primary tumours,2020/10
1047,Kieran Campbell,"Single-cell technologies have revolutionized biomedical research by enabling scalable measurement of the genome, transcriptome, proteome, and epigenome of multiple systems at single-cell resolution. Now widely applied to cancer models, these assays offer new insights into tumour heterogeneity, which underlies cancer initiation, progression, and relapse. However, the large quantities of high-dimensional, noisy data produced by single-cell assays can complicate data analysis, obscuring biological signals with technical artifacts. In this review article, we outline the major challenges in analyzing single-cell cancer genomics data and survey the current computational tools available to tackle these. We further outline unsolved problems that we consider major opportunities for future methods development to help interpret the vast quantities of data being generated.",Computational modelling in single-cell cancer genomics: methods and future directions,2020-09-16 00:00:00
1048,Kieran Campbell,"Ovarian cancers are the most common gynecologic malignancies. Low grade serous ovarian carcinoma (LGSOC) is a rare tumor, accounting for ~2000 cases diagnosed every year in North America. Most of LGSOCs are characterized by high fatality rates over the long term, with only 20% of women surviving 10 years after diagnosis, due suboptimal response to current chemotherapies. Understanding the molecular events is crucial for developing better early detection strategies and more informed therapeutic options. LGSOC harbors a relatively stable genome, with common activating mutations in BRAF, KRAS and NRAS. Recently, NRAS mutations (Q61R) were found to co-exist with EIF1AX mutations (G8E) in LGOSC, and the two mutated proteins functionally cooperate. Increasing histological and gene expression evidence suggest that the cell of origin of LGSOC is in the Fallopian tube.",Investigating mutation co-operativity in early tumorigenesis of low-grade serous ovarian carcinoma with organoid model system and single-cell RNA sequencing,2020-08-15 00:00:00
1049,Kieran Campbell,"Endometrial epithelium gives rise to both endometrial and ovarian cancers (of clear-cell and endometrioid subtypes), the latter arising from ectopic endometrium (endometriosis). Endometrial epithelium comprises mainly secretory cells, with a minor ciliated cell population. Due to their scarcity, little is known about the biology or function of endometrial ciliated cells. To understand the biology of endometrial epithelium, and by extension the cancers that arise from it, organoids derived from normal endometrial tissue were cultured. Notch signaling inhibitors were used to induce ciliated cell differentiation. Through single-cell RNA sequencing, distinct secretory and ciliated cell populations were observed, with the ciliated cell population increasing with Notch signaling inhibition. Many novel markers of ciliated cells were observed, but no highly specific markers of secretory cell differentiation. ",Abstract B09: Single-cell RNA sequencing of normal endometrial organoids uncovers novel cell-type markers for prognostication of primary tumor samples,2020-07-01 00:00:00
1050,Kieran Campbell,explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature we propose fit a framework that evaluates the importance of observations for a multivariate timeseries blackbox model by quantifying the shift in the predictive distribution over time fit defines the importance of an observation based on its contribution to the distributional shift under a kldivergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved we also demonstrate the need to control for timedependent distribution shifts we compare with stateoftheart baselines on simulated and realworld clinical data and demonstrate that our approach is superior in identifying important time points and observations throughout the time series less,What went wrong and when? Instance-wise Feature Importance for Time-series Models,"28 October, 2020"
1051,Mark Chignell,"Psychological resilience has emerged as a key factor in mental health during the global COVID-19 pandemic. However, no work to date has synthesised findings across review work or assessed the reliability of findings based on review work quality, so as to inform public health policy. We thus conducted a meta-review on all types of review work from the start of the pandemic (January 2020) until the last search date (June 2021). Of an initial 281 papers, 30 were included for review characteristic reporting and 15 were of sufficient review quality for further inclusion in strategy analyses. High-level strategies were identified at the individual, community, organisational, and governmental levels. Several specific training and/or intervention programmes were also identified. However, the quality of findings was insufficient for drawing conclusions. A major gap between measuring the psychological resilience of populations",A meta-review of psychological resilience during COVID-19,2022-07-05 00:00:00
1052,Mark Chignell,"Recently, there has been a growing demand to address failures in the fairness of artificial intelligence (AI) systems. Current techniques for improving fairness in AI systems are focused on broad changes to the norms, procedures and algorithms used by companies that implement those systems. However, some organizations may require detailed methods to identify which user groups are disproportionately impacted by failures in specific components of their systems. Failure mode and effects analysis (FMEA) is a popular safety engineering method and is proposed here as a vehicle to support the conducting of “AI fairness impact assessments” in organizations. An extension to FMEA called “FMEA-AI” is proposed as a modification to a familiar tool for engineers and manufacturers that can integrate moral sensitivity and ethical considerations into a company’s existing design process. Whereas current impact",FMEA-AI: AI fairness impact assessment using failure mode and effects analysis,2022-03-07 00:00:00
1053,Mark Chignell,"Serious games aim to provide cognitive assessments that are more enjoyable and easier to self-administer, potentially leading to more frequent assessments. We carried out two studies examining the relationship between game-playing enjoyment, game difficulty, and cognitive (game) performance. In the first study, 16 participants played three serious games once a week over four weeks as part of an undergraduate course, but with relatively minor motivation in terms of course credits. In the second study, 14 participants played serious games over six sessions in a period of three weeks. Participants included graduate students receiving credit for the course project (a major component of the grade) and friends and family that they recruited. Performance in the more difficult tasks tended to improve over time in the second study, but not in the first. Participants from the first study showed an overall negative sentiment",Longitudinal analysis of sustained performance on gamified cognitive assessment tasks,2022-02-23 00:00:00
1054,Mark Chignell,"Prospective observational study of people ≥65 years. We assessed delirium using the Confusion Assessment Method, then asked ED staff if the patient had delirium, confidence in their assessment, if the patient could be discharged, and contacted patients 1 week postdischarge. We report proportions and 95% confidence intervals (Cls).","Prevalence, management and outcomes of unrecognized delirium in a National Sample of 1,493 older emergency department patients: how many were sent home and what happened to them?",2022/2
1055,Mark Chignell,"The Hamilton Rating Scale for Depression (HAM-D) evaluates depression severity based on 17 symptoms of major depressive disorder (MDD). Understanding the clustering of symptoms within MDD is helpful for the identification of depression subgroups that might differ in how they respond to various treatment interventions. In the research reported in this paper, we employed hierarchical clustering to generate a symptom clustering hierarchy. We compare our findings with previously identified factor analytic HAM-D symptom groups. Our findings are comparatively assessed with previously identified HAM-D symptom groups using factor analysis.",Hierarchical Clustering of Multi-Study Depression Data Yields Four Symptom Clusters,2021-12-09 00:00:00
1056,Mark Chignell,"Major depressive disorder (MDD) is the most common mental health disorder and is one of the leading preventable causes of death in the United States (U.S.). It is also recognized as a global problem by the World Health organization (WHO). The persistence of MDD leads to many negative consequences including suicide and disability. The Hamilton Rating Scale for Depression (HAM-D) evaluates depression severity based on 17 risk factors (symptoms) of depression. Risk factor analysis is a process to identify and understand the risk factors contributing to a particular disease, and is an essential component in the development of efficient and effective prevention and intervention efforts. Most existing methods use a one-size-fits-all model to identify the risk factors at the population-level. However, this type of method fails to account for data heterogeneity within a population. ","Prioritization of Multi-level Risk Factors, and Predicting Changes in Depression Ratings after Treatment Using Multi-Task Learning",2021-12-09 00:00:00
1057,Mark Chignell,"Being able to accurately predict the time to event of interest, commonly known as survival analysis, is extremely beneficial in many real-world applications. Traditional commonly used statistical survival analysis methods, e.g., Cox proportional hazards model and parametric censored regressions, are based on strong and sometimes impractical assumptions and can only handle linearity relationship between features and target. Recently, deep learning based formulations have been proposed for survival analysis to handle non-linearity. However, these existing deep learning methods either inherit strong assumptions from their corresponding base models or tailor discrete-time survival analysis. To overcome the limitations within these existing models in the literature, we propose an objective function to guide the training of a deep learning model for continuous-time survival analysis. ",Combining Ranking and Point-wise Losses for Training Deep Survival Analysis Models,2021-12-07 00:00:00
1058,Mark Chignell,"In previous research, we developed a serious target acquisition game (with moles as targets) for assessing cognitive speed. Tong, Chignell, Tierney, and Lee demonstrated that performance on the game may be a useful screening tool for risk of delirium onset. In this study, we validate a version of the game where there are not only targets (moles) that should be hit but also distractors (butterflies, or moles with hats) that should not be hit. We hypothesized that performance on the game should be a measure of response inhibition ability, which has been implicated as a factor in many types of psychopathology. We carried out an experiment (with 30 healthy participants) to test whether the serious game does in fact measure response inhibition by comparing game performance with a standard response inhibition task (the Go/No-Go discrimination task). Our results show that, with the distractors, the game does ",Using a serious game to measure executive functioning: response inhibition ability,2021-11-02 00:00:00
1059,Mark Chignell,"This workshop explores socially expressive technology design using collaborative principles of theater, dance and music. Building on shared roots of making something together, technologies (including virtual agents, physical devices, storytelling multimedia) and artistic provocation can create playful, productive and healthy experiences for communities. How do we: Improve collaboration between performance art and creative technology? Design apps and artistic collaboration/learning for mental health? Use theater performativity and technology to enhance our lives? As smart devices become ubiquitous, yet solidarity and cooperation remain elusive in competitive/hierarchical societies, understanding how performance art and technology collaborate can improve how we work, play and care for our and each other's health. This workshop is a forum and creative space to discuss and play with the future of ",arttech: Performance and Embodiment in Technology for Resilience and Mental Health,2021-10-23 00:00:00
1060,Mark Chignell,"Human activity recognition is a key to a lot of applications such as healthcare and smart home. In this study, we provide a comprehensive survey on recent advances and challenges in human activity recognition (HAR) with deep learning. Although there are many surveys on HAR, they focused mainly on the taxonomy of HAR and reviewed the state-of-the-art HAR systems implemented with conventional machine learning methods. Recently, several works have also been done on reviewing studies that use deep models for HAR, whereas these works cover few deep models and their variants. There is still a need for a comprehensive and in-depth survey on HAR with recently developed deep learning methods.",A survey on deep learning for human activity recognition,2021-10-04 00:00:00
1061,Mark Chignell,"Cybersecurity is emerging as a major issue for many organizations and countries. Machine learning has been used to recognize threats, but it is difficult to predict future threats based on past events, since malicious attackers are constantly finding ways to circumvent defences and the algorithms that they rely on. Interactive Machine learning (iML) has been developed as a way to combine human and algorithmic expertise in a variety of domains and we are currently applying it to cybersecurity. In this application of iML, implicit knowledge about human behaviour, and about the changing nature of threats, can supplement the explicit knowledge encoded in algorithms to create more effective defences against cyber-attacks. In this paper we present the example problem of data exfiltration where insiders, or outsiders masquerading as insiders, who copy and transfer data maliciously, against the interests of an",Human Factors in Interactive Machine Learning: A Cybersecurity Case Study,2021/9
1062,Mark Chignell,while many machine learning methods have been used for medical prediction and risk factor analysis on healthcare data most prior research has involved singletask learning stl methods however healthcare research often involves multiple related tasks for instance implementation of disease scores prediction and risk factor analysis in multiple subgroups of patients simultaneously and risk factor analysis at multilevels synchronously in this paper we developed a new ensemble machine learning python package based on multitask learning mtl referred to as the medmultitask learning mdmtl package and applied it in predicting disease scores of patients and in carrying out risk factor analysis on multiple subgroups of patients simultaneously our experimental results on two datasets demonstrate the utility of the mdmtl package and show the advantage of mtl vs stl when analyzing data that is organized into different categories tasks which can be various age groups different levels of disease severity etc less,MD-MTL: An Ensemble Med-Multi-Task Learning Package for DiseaseScores Prediction and Multi-Level Risk Factor Analysis,"4 March, 2021"
1063,Natalie Enright Jerger,"NoCs are over-provisioned with large virtual channels to provide deadlock freedom and performance improvement. This use of virtual channels leads to considerable power and area overhead. In this paper, we introduce a novel flow control, called FastFlow, to enhance performance and avoid both protocol- and network-level deadlocks with an impressive reduction in number of virtual channels compared to the state-of-the-art NoCs. FastPass promotes a packet to traverse the network bufferlessly; the packet bypasses the routers to reach its destination. During the traversals, the packet is guaranteed to make forward progress every cycle. As a result, such a packet cannot be blocked by congestion nor deadlock. Promoting more packets to FastPass will provide higher throughput. To this end, FastPass allows multiple packets to be upgraded as FastPass packets simultaneously. ",Stay in your Lane: A NoC with Low-overhead Multi-packet Bypassing,2022-04-02 00:00:00
1064,Natalie Enright Jerger,"The computing world is witnessing a proverbial Cambrian explosion of emerging paradigms propelled by applications, such as artificial intelligence, big data, and cybersecurity. The recent advances in technology to store digital data inside a deoxyribonucleic acid (DNA) strand, manipulate quantum bits (qubits), perform logical operations with photons, and perform computations inside memory systems are ushering in the era of emerging paradigms of DNA computing, quantum computing, optical computing, and in-memory computing. In an orthogonal direction, research on interconnect design using advanced electro-optic, wireless, and microfluidic technologies has shown promising solutions to the architectural limitations of traditional von-Neumann computers. In this article, experts present their comments on the role of interconnects in the emerging computing paradigms, and discuss the potential use of chiplet ","Interconnects for DNA, Quantum, In-Memory, and Optical Computing: Insights From a Panel Discussion",2022-02-14 00:00:00
1065,Natalie Enright Jerger,"Allocating a free buffer before moving to the next router is a fundamental tenet for packet movement in NoCs. Often, to solve head of line blocking and avoid deadlock, NoCs are provisioned with significant buffer resources in the form of virtual channels (VC) which consume area and power. We introduce stochastic escape express channels (SEEC) to enhance performance and avoid deadlock with dramatically fewer buffers than state-of-the-art NoCs. The network interfaces in SEEC periodically send special tokens called seekers to find packets destined for them and upgrade them to use a novel flow control called Free-Flow (FF). FF-packets traverse the network minimally from link to link, bypassing routers (bufferlessly) to the destination. As a result, FF-packets bypass regions of congestion in the NoC without needing more buffers. Furthermore, any deadlock that a FF-packet was originally involved in is guaranteed",SEEC: Stochastic escape express channel,2021-11-14 00:00:00
1066,Natalie Enright Jerger,"Coherence induced cache misses are an important aspect limiting the scalability of shared memory parallel programs. Many coherence misses are avoidable, namely misses due to false sharing–when different threads write to different memory addresses that are contained within the same cache block causing unnecessary invalidations. Past work has proposed numerous ways to mitigate false sharing from coherence protocols optimized for certain sharing patterns, to software tools for false-sharing detection and repair. Our work leverages approximate computing and store value similarity in error-tolerant multi-threaded applications. We introduce a novel cache coherence protocol which implements an approximate store instruction and coherence states to allow some limited incoherence within approximatable shared data to mitigate both coherence misses and coherence traffic for various sharing patterns. ",Ghostwriter: A Cache Coherence Protocol for Error-Tolerant Applications,2021-08-09 00:00:00
1067,Natalie Enright Jerger,"The following paper,"" Simba: Scaling Deep-Learning Inference with Chiplet-Based Architecture,"" by Shao et al. presents a scalable deep learning accelerator architecture that tackles issues ranging from chip integration technology to workload partitioning and non-uniform latency effects on deep neural network performance. Through a hardware prototype, they present a timely study of cross-layer issues that will inform next-generation deep learning hardware, software, and neural network architectures.",Technical perspective: A chiplet prototype system for deep learning inference,2021-05-24 00:00:00
1068,Natalie Enright Jerger,"Maintaining correctness is of paramount importance in the design of a computer system. Within a multiprocessor interconnection network, correctness is guaranteed by having deadlock-free communication at both the protocol and network levels. Modern network-on-chip (NoC) designs use multiple virtual networks to maintain protocol-level deadlock freedom, at the expense of high power and area overheads. Other techniques involve complex detection and recovery mechanisms, or use misrouting which incurs additional packet latency. Considering that the probability of deadlocks occurring is low, the additional resources needed to avoid/resolve deadlocks should also be low. To this end, we propose Pitstop, a low-cost technique that guarantees correctness by resolving both protocol and network-level deadlocks without the use of virtual networks, complex hardware, or misrouting. ",Pitstop: Enabling a Virtual Network Free Network-on-Chip,2021/2
1069,Natalie Enright Jerger,"When a computational task tolerates a relaxation of its specification or when an algorithm tolerates the effects of noise in its execution, hardware, system software, and programming language compilers or their runtime systems can trade deviations from correct behavior for lower resource usage. We present, for the first time, a synthesis of research results on computing systems that only make as many errors as their end-to-end applications can tolerate. The results span the disciplines of computer-aided design of circuits, digital system design, computer architecture, programming languages, operating systems, and information theory. Rather than over-provisioning the resources controlled by each of these layers of abstraction to avoid errors, it can be more efficient to exploit the masking of errors occurring at one layer and thereby prevent those errors from propagating to a higher layer.",Exploiting errors for efficiency: A survey from circuits to applications,2020-06-12 00:00:00
1070,Natalie Enright Jerger,"Computation demands on mobile and edge devices are increasing dramatically. Mobile devices, such as smart phones, incorporate a large number of dedicated accelerators and fixed-function hardware blocks to deliver the required performance and power efficiency. Due to the heterogeneous nature of these devices, they feature vastly larger design spaces than traditional systems featuring only a CPU. Currently, academia struggles to fully evaluate such heterogeneous systems on chip due to the limited access and availability of proprietary workloads. To address these challenges, we propose Mocktails: a methodology to synthetically recreate the varying spatio-temporal memory access behaviour of proprietary heterogeneous compute devices. We focus on capturing the interspersed address streams of the workload and the burstiness of the injection process for proprietary compute devices commonly found in ",Mocktails: Capturing the Memory Behaviour of Proprietary Mobile Architectures,2020/6
1071,Natalie Enright Jerger,"There has been a lot of recent interest in applying machine learning (ML) to the design of systems, which purports to aid human experts in extracting new insights leading to better systems. In this work, we share our experiences with applying ML to improve one aspect of networks-on-chips (NoC) to uncover new ideas and approaches, which eventually led us to a new arbitration scheme that is effective for NoCs under heavy contention. However, a significant amount of human effort and creativity was still needed to optimize just one aspect (arbitration) of what is only one component (the NoC) of the overall processor. This leads us to conclude that much work (and opportunity!) remains to be done in the area of ML-driven architecture design.",Experiences with ML-Driven Design: A NoC Case Study,2020-02-22 00:00:00
1072,Natalie Enright Jerger,"Correctness is a first-order concern in the design of computer systems. For multiprocessors, a primary correctness concern is the deadlock-free operation of the network and its coherence protocol; furthermore, we must guarantee the continued correctness of the network in the face of increasing faults. Designing for deadlock freedom is expensive. Prior solutions either sacrifice performance or power efficiency to proactively avoid deadlocks or impose high hardware complexity to reactively resolve deadlocks as they occur. However, the precise confluence of events that lead to deadlocks is so rare that minimal resources and time should be spent to ensure deadlock freedom. To that end, we propose DRAIN, a subactive approach to remove potential deadlocks without needing to explicitly detect or avoid them. We simply let deadlocks happen and periodically drain (i.e., force the movement of) packets in the network ",DRAIN: Deadlock Removal for Arbitrary Irregular Networks,2020/2
1073,Natalie Enright Jerger,"Networks-on-Chip (NoCs) address many shortcomings of traditional interconnects. However, they consume a considerable portion of a chip's total power-particularly when the utilization is low. As transistor size continues to shrink, we expect NoCs to contribute even more, especially static power. A wide range of prior-art focuses on reducing the contribution of NoC power consumption. These can be categorized into two main groups:(1) power-gating, and (2) simplified router microarchitectures. Maintaining the performance and the flexibility of the network are key challenges that have not yet been addressed by these two groups of low-power architectures. In this paper, we propose UBERNoC, a simplified router microarchitecture, which reduces underutilized buffer space by leveraging an observation that for most switch traversals, only a single packet is present. We use a unified buffer with multiple virtual channels ",UBERNoC: unified buffer power-efficient router for network-on-chip,2019-10-17 00:00:00
1074,Natalie Enright Jerger,this work investigates how using reduced precision data in convolutional neural networks cnns affects network accuracy during classification more specifically this study considers networks where each layer may use different precision data our key result is the observation that the tolerance of cnns to reduced precision data not only varies across networks a well established observation but also within networks tuning precision per layer is appealing as it could enable energy and performance improvements in this paper we study how error tolerance across layers varies and propose a method for finding a low precision configuration for a network while maintaining high accuracy a diverse set of cnns is analyzed showing that compared to a conventional implementation using a bit floatingpoint representation for all layers and with less than loss in relative accuracy the data footprint required by these networks can be reduced by an average of and up to less,Reduced-Precision Strategies for Bounded Memory in Deep Neural Nets,"8 January, 2016"
1075,Mark Fox,this paper introduces ontological concepts required to evaluate and manage the coverage of social services in a smart city context here we focus on the perspective of key stakeholders namely social purpose organizations and the clients they serve the compass ontology presented here extends the common impact data standard by introducing new concepts related to key dimensions the who stakeholder the what need need satisfier outcome the how service event and the contributions tracking resources the paper first introduces key stakeholders services outcomes events needs and need satisfiers along with their definitions second a subset of competency questions are presented to illustrate the types of questions key stakeholders have posed third the extensions ability to answer questions is evaluated by presenting sparql queries executed on a compassbased knowledge graph and analysing their results less,An Ontological Approach to Analysing Social Service Provisioning,"24 June, 2022"
1076,Mark Fox,the strings for absorption length in water straw are the first in a series of pathfinders for the pacific ocean neutrino experiment pone a future largescale neutrino telescope in the northeastern pacific ocean straw consists of two m long mooring lines instrumented with optical emitters and detectors the pathfinder is designed to measure the attenuation length of the water and perform a longterm assessment of the optical background at the future pone site after two years of continuous operation measurements from straw show an optical attenuation length of about metres at nm additionally the data allow a study of the ambient undersea background the overall optical environment reported here is comparable to other deepwater neutrino telescopes and qualifies the site for the deployment of pone less,Two-Year Optical Site Characterization for the Pacific Ocean Neutrino Experiment P-ONE in the Cascadia Basin,"8 December, 2021"
1077,Mark Fox,topological photonic interfaces support topologically nontrivial optical modes with helical character when combined with an embedded quantum emitter that has a circularly polarised transition dipole moment a chiral quantum optical interface is formed due to spinmomentum locking here we experimentally realise such an interface by integrating semiconductor quantum dots into a valleyhall topological photonic crystal waveguide we harness the robust waveguide transport to create a ring resonator which supports helical modes chiral coupling of quantum dot transitions with directional contrast as high as is demonstrated the interface also supports a topologically trivial mode comparison with which allows us to clearly demonstrate the protection afforded by topology to the nontrivial mode less,Chiral topological photonics with an embedded quantum emitter,"28 October, 2020"
1078,Mark Fox,this white paper summarizes the workshop us cosmic visions new ideas in dark matter held at university of maryland on march less,US Cosmic Visions: New Ideas in Dark Matter 2017: Community Report,"14 July, 2017"
1079,Mark Fox,we present an ongoing systematic search for extragalactic infrared transients dubbed spirits spitzer infrared intensive transients survey in the first year using spitzerirac we searched nearby galaxies with cadence baselines of one month and six months we discovered over variables and transients here we describe the survey design and highlight unusual infrared transients with no optical counterparts to deep limits which we refer to as sprites especially red intermediate luminosity transient events sprites are in the infrared luminosity gap between novae and supernovae with absolute magnitudes between and vegamag and colors between mag and mag the photometric evolution of sprites is diverse ranging from magyr to magyr sprites occur in starforming galaxies we present an indepth study of one of them spirits ajc in messier which shows shockexcited molecular hydrogen emission this shock may have been triggered by the dynamic decay of a nonhierarchical system of massive stars that led to either the formation of a binary or a protostellar merger less,SPIRITS: Uncovering Unusual Infrared Transients With Spitzer,"4 January, 2017"
1080,Mark Fox,we present the temperature and polarization angular power spectra measured by the atacama cosmology telescope polarimeter actpol we analyze nighttime data collected during using two detector arrays at ghz from deg of sky on the celestial equator we use these spectra and the spectra measured with the mbac camera on act from in combination with planck and wmap data to estimate cosmological parameters from the temperature polarization and temperaturepolarization crosscorrelations we find the new actpol data to be consistent with the lcdm model the actpol temperaturepolarization crossspectrum now provides stronger constraints on multiple parameters than the actpol temperature spectrum including the baryon density the acoustic peak angular scale and the derived hubble constant adding the new data to planck temperature data tightens the limits on damping tail parameters for example reducing the joint uncertainty on the number of neutrino species and the primordial helium fraction by less,The Atacama Cosmology Telescope: Two-Season ACTPol Spectra and Parameters,"7 October, 2016"
1081,Mark Fox,we give a topological characterisation of alternating knot exteriors based on the presence of special spanning surfaces this shows that alternating is a topological property of the knot exterior and not just a property of diagrams answering an old question of fox we also give a characterisation of alternating link exteriors which have marked meridians we then describe a normal surface algorithm which can decide if a knot is prime and alternating given a triangulation of its exterior as input less,A characterisation of alternating knot exteriors,"16 November, 2015"
1082,Mark Fox,a scalable optical quantum information processor is likely to be a waveguide circuit with integrated sources detectors and either deterministic quantumlogic or quantum memory elements with microsecond coherence times ultrafast coherent control and lifetimelimited transitions semiconductor quantumdot spins are a natural choice for the static qubits however their integration with flying photonic qubits requires an onchip spinphoton interface which presents a fundamental problem the spinstate is measured and controlled via circularlypolarised photons but waveguides support only linear polarisation we demonstrate here a solution based on two orthogonal photonic nanowires in which the spinstate is mapped to a pathencoded photon thus providing a blueprint for a scalable spinphoton network furthermore for some devices we observe that the circular polarisation state is directly mapped to orthogonal nanowires this result which is physically surprising for a nonchiral structure is shown to be related to the nanopositioning of the quantumdot with respect to the photonic circuit less,Interfacing a quantum dot spin with a photonic circuit,"14 June, 2012"
1083,Brendan Frey,we propose generative neural network methods to generate dna sequences and tune them to have desired properties we present three approaches creating synthetic dna sequences using a generative adversarial network a dnabased variant of the activation maximization deep dream design method and a joint procedure which combines these two approaches together we show that these tools capture important structures of the data and when applied to designing probes for protein binding microarrays allow us to generate new sequences whose properties are estimated to be superior to those found in the training data we believe that these results open the door for applying deep generative models to advance genomics research less,Generating and designing DNA with deep generative models,"17 December, 2017"
1084,Brendan Frey,in this paper we propose the adversarial autoencoder aae which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks gan to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples as a result the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution we show how the adversarial autoencoder can be used in applications such as semisupervised classification disentangling style and content of images unsupervised clustering dimensionality reduction and data visualization we performed experiments on mnist street view house numbers and toronto face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semisupervised classification tasks less,Adversarial Autoencoders,"24 May, 2016"
1085,Brendan Frey,despite their success convolutional neural networks are computationally expensive because they must examine all image locations stochastic attentionbased models have been shown to improve computational efficiency at test time but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates borrowing techniques from the literature on training deep generative models we present the wakesleep recurrent attention model a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients we show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation less,Learning Wake-Sleep Recurrent Attention Models,"22 September, 2015"
1086,Brendan Frey,a new approach to maximum likelihood learning of discrete graphical models and rbm in particular is introduced our method perturb and descend pd is inspired by two ideas i perturb and map method for sampling ii learning by contrastive divergence minimization in contrast to perturb and map pd leverages training data to learn the models that do not allow efficient map estimation during the learning to produce a sample from the current model we start from a training data and descend in the energy landscape of the perturbed model for a fixed number of steps or until a local optima is reached for rbm this involves linear calculations and thresholding which can be very fast furthermore we show that the amount of perturbation is closely related to the temperature parameter and it can regularize the model by producing robust features resulting in sparse hidden layer activation less,Training Restricted Boltzmann Machine by Perturbation,"6 May, 2014"
1087,Brendan Frey,many realvalued stochastic timeseries are locally linear gassian but globally nonlinear for example the trajectory of a human hand gesture can be viewed as a linear dynamic system driven by a nonlinear dynamic system that represents muscle actions we present a mixedstate dynamic graphical model in which a hidden markov model drives a linear dynamic system this combination allows us to model both the discrete and continuous causes of trajectories such as human gestures the number of computations needed for exact inference is exponential in the sequence length so we derive an approximate variational inference technique that can also be used to learn the parameters of the discrete and continuous models we show how the mixedstate model and the variational technique can be used to classify human hand gestures made with a computer mouse less,Variational Learning in Mixed-State Dynamic Graphical Models,"23 January, 2013"
1088,Brendan Frey,some types of medical and topographic imaging device produce images in which the pixel values are phasewrapped ie measured modulus a known scalar phase unwrapping can be viewed as the problem of inferring the number of shifts between each and every pair of neighboring pixels subject to an a priori preference for smooth surfaces and subject to a zero curl constraint which requires that the shifts must sum to around every loop we formulate phase unwrapping as a mean field inference problem in a markov network where the prior favors the zero curl constraint we compare our mean field technique with the least squares method on a synthetic x image and give results on a x synthetic aperture radar image from sandia national laboratorieslong text less,A Factorized Variational Technique for Phase Unwrapping in Markov Random Fields,"10 January, 2013"
1089,Brendan Frey,the two most popular types of graphical model are directed models bayesian networks and undirected models markov random fields or mrfs directed and undirected models offer complementary properties in model construction expressing conditional independencies expressing arbitrary factorizations of joint distributions and formulating messagepassing inference algorithms we show that the strengths of these two representations can be combined in a single type of graphical model called a factor graph every bayesian network or mrf can be easily converted to a factor graph that expresses the same conditional independencies expresses the same factorization of the joint distribution and can be used for probabilistic inference through application of a single simple messagepassing algorithm in contrast to chain graphs where messagepassing is implemented on a hypergraph messagepassing can be directly implemented on the factor graph we describe a modified bayesball algorithm for establishing conditional independence in factor graphs and we show that factor graphs form a strict superset of bayesian networks and mrfs in particular we give an example of a commonlyused mixture of experts model fragment whose independencies cannot be represented in a bayesian network or an mrf but can be represented in a factor graph we finish by giving examples of realworld problems that are not well suited to representation in bayesian networks and mrfs but are wellsuited to representation in factor graphs less,Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models,"19 October, 2012"
1090,Brendan Frey,based on a recent development in the area of error control coding we introduce the notion of convolutional factor graphs cfgs as a new class of probabilistic graphical models in this context the conventional factor graphs are referred to as multiplicative factor graphs mfgs this paper shows that cfgs are natural models for probability functions when summation of independent latent random variables is involved in particular cfgs capture a large class of linear models where the linearity is in the sense that the observed variables are obtained as a linear ransformation of the latent variables taking arbitrary distributions we use gaussian models and independent factor models as examples to emonstrate the use of cfgs the requirement of a linear transformation between latent variables with certain independence restriction and the bserved variables to an extent limits the modelling flexibility of cfgs this structural restriction however provides a powerful analytic tool to the framework of cfgs that is upon taking the fourier transform of the function represented by the cfg the resulting function is represented by a fg with identical structure this fourier transform duality allows inference problems on a cfg to be solved on the corresponding dual mfg less,Convolutional Factor Graphs as Probabilistic Models,"11 July, 2012"
1091,Brendan Frey,exemplarbased clustering methods have been shown to produce stateoftheart results on a number of synthetic and realworld clustering problems they are appealing because they offer computational benefits over latentmean models and can handle arbitrary pairwise similarity measures between data points however when trying to recover underlying structure in clustering problems tailored similarity measures are often not enough we also desire control over the distribution of cluster sizes priors such as dirichlet process priors allow the number of clusters to be unspecified while expressing priors over data partitions to our knowledge they have not been applied to exemplarbased models we show how to incorporate priors including dirichlet process priors into the recently introduced affinity propagation algorithm we develop an efficient maxproduct belief propagation algorithm for our new model and demonstrate experimentally how the expanded range of clustering priors allows us to better recover true clusterings in situations where we have some information about the generating process less,Flexible Priors for Exemplar-based Clustering,"13 June, 2012"
1092,Brendan Frey,affinity propagation is an exemplarbased clustering algorithm that finds a set of datapoints that best exemplify the data and associates each datapoint with one exemplar we extend affinity propagation in a principled way to solve the hierarchical clustering problem which arises in a variety of domains including biology sensor networks and decision making in operational research we derive an inference algorithm that operates by propagating information up and down the hierarchy and is efficient despite the highorder potentials required for the graphical model formulationwe demonstrate that our method outperforms greedy techniques that cluster one layer at a time we show that on an artificial dataset designed to mimic the hivstrain mutation dynamics our method outperforms related methods for real hiv sequences where the ground truth is not available we show our method achieves better results in terms of the underlying objective function and show the results correspond meaningfully to geographical location and strain subtypes finally we report results on using the method for the analysis of mass spectra showing it performs favorably compared to stateoftheart methods less,Hierarchical Affinity Propagation,"14 February, 2012"
1093,Michael Gruninger,"Data science incorporates a variety of processes, concepts, techniques and domains, to transform data that is representative of real-world phenomena into meaningful insights and to inform decision-making. Data science relies on simple datatypes like strings and integers to represent complex real-world phenomena like time and geospatial regions. This reduction of semantically rich types to simplistic ones creates issues by ignoring common and significant relationships in data science including time, mereology, and provenance. Current solutions to this problem including documentation standards, provenance tracking, and knowledge model integration are opaque, lack standardization, and require manual intervention to validate. We introduce the meaningful type safety framework (MeTS) to ensure meaningful and correct data science through semantically-rich datatypes based on dependent types. ",What’s in a (Data) Type? Meaningful Type Safety for Data Science,2022
1094,Michael Gruninger,"Upper ontologies have traditionally arisen from the approach in which concepts that are common across a set of domains can be axiomatized at a general level. The rationale is that reuse across domains is to be supported through specialization of the general concepts from an upper ontology. Similarly, semantic integration between ontologies is to be achieved through the general concepts they specialize. The TUpper Ontology follows an alternative approach (referred to as the sideways approach) to the conventional upper ontology paradigm. Rather than think of an upper ontology as a monolithic axiomatization centred on a taxonomy, the sideways approach considers an upper ontology to be a modular ontology composed of generic ontologies that cover concepts including those related to time, process, and space. TUpper is therefore composed of a set of generic ontologies, and each generic ontology",TUpper: A top level ontology within standards,2022-01-01 00:00:00
1095,Michael Gruninger,"In recent years there has been a resurgence of interest in our community in the shape analysis of 3D objects represented by surface meshes, their voxelized interiors, or surface point clouds. In part, this interest has been stimulated by the increased availability of RGBD cameras, and by applications of computer vision to autonomous driving, medical imaging, and robotics. In these settings, spectral coordinates have shown promise for shape representation due to their ability to incorporate both local and global shape properties in a manner that is qualitatively invariant to isometric transformations. Yet, surprisingly, such coordinates have thus far typically considered only local surface positional or derivative information. In the present article, we propose to equip spectral coordinates with medial (object width) information, so as to enrich them. The key idea is to couple surface points that share a medial ball, via the weights of the adjacency matrix. We develop a spectral feature using this idea, and the algorithms to compute it. The incorporation of object width and medial coupling has direct benefits, as illustrated by our experiments on object classification, object part segmentation, and surface point correspondence.",Medial spectral coordinates for 3D shape analysis,2022
1096,Michael Gruninger,"We discuss a vision of the Semantic Web where ontologies, services, and devices can seamlessly interoperate across a multitude of protocols and languages. In particular, we discuss the importance of enabling interoperability for Semantic Web technologies in the knowledge representation layer, and give an overview of the Distributed Ontology Language DOL addressing this aspect, representing a first piece of a Rosetta Stone enabling overall interoperability.",The Babel of the Semantic Web Tongues–In Search of the Rosetta Stone of Interoperability,2022
1097,Michael Gruninger,"Although a variety of specialised formalisms have been proposed specifically for enterprise modelling, the use of existing modelling languages has not received as much attention. In this paper, we demonstrate that the systems modelling formalism SysML is in fact not sufficient to act as a standalone language for enterprise modelling. To demonstrate this claim, we show that there are four key enterprise modelling scenarios that cannot be addressed while adhering to SysML semantics: temporal representation, timing and scheduling, collaborations between two or more teams and decision trees .",Can SysML Be Used for Enterprise Modelling?,2021-11-24 00:00:00
1098,Michael Gruninger,"Humans are excellent at perceiving illusory outlines. We are readily able to complete contours, shapes, scenes, and even unseen objects when provided with images that contain broken fragments of a connected appearance. In vision science, this ability is largely explained by perceptual grouping: a foundational set of processes in human vision that describes how separated elements can be grouped. In this paper, we revisit an algorithm called Stochastic Completion Fields (SCFs) that mechanizes a set of such processes -- good continuity, closure, and proximity -- through contour completion. This paper implements a modernized model of the SCF algorithm, and uses it in an image editing framework where we propose novel methods to complete fragmented contours. We show how the SCF algorithm plausibly mimics results in human perception. We use the SCF completed contours as guides for inpainting, and show that our guides improve the performance of state-of-the-art models. Additionally, we show that the SCF aids in finding edges in high-noise environments. Overall, our described algorithms resemble an important mechanism in the human visual system, and offer a novel framework that modern computer vision models can benefit from.",Contour-guided Image Completion with Perceptual Grouping,2021-11-22 00:00:00
1099,Michael Gruninger,"Following Smith and Gasser’s work on embodied cognition, one can consider a robot as an intelligent agent that interacts with its external environment through sensorimotor activities, such as touching, lifting, standing, sitting, and walking. In this paper we explore the ontologies that are required to represent and reason about robot dynamics. We propose new ontologies for robotic components and poses, including a new nonclassical mereotopology for touch contact. The design of the ontologies is driven by semantic parsing of natural language instructions (eg “Lift the box that is beside the chair and place it on the table""), through which we identify the spatial and mereotopological relations among a robot’s components and the external world, as well as the activities that the robot can perform to change these relationships.",Qualitative Spatial Ontologies for Robot Dynamics.,2021
1100,Michael Gruninger,"Standards and Ontologies Page 1 Standards and Ontologies Michael Grüninger Semantic Technologies Lab University of Toronto Ontology Summit 2020 Standards for Knowledge Graphs June 3, 2020 (Standards for Knowledge Graphs) Standards and Ontologies Ontology Summit 2020 Standards for Knowledge Graphs Page 2 Challenge Our goal as the Applied Ontology community is the design and deployment of sharable and reusable ontologies. How do standards support/inhibit sharability and reusability of ontologies? (Standards for Knowledge Graphs) Standards and Ontologies Ontology Summit 2020 Standards for Knowledge Graphs Page 3 Workshop on Implemented Ontologies (1994) All papers had to be accompanied with the explicit encoding in an ontology representation language. The problem was not the diversity of ontology representation languages per se, but the lack of formal semantics for the",Standards and ontologies,2020-06-03 00:00:00
1101,Michael Gruninger,"Classical mereology is based on the assumption that any two underlapping elements have a sum, yet there are many domains (such as manufacturing assemblies, molecular structure, gene sequences, and convex time intervals) in which this assumption is not valid. In such domains, mereological sums must be connected objects. However, there has been little work in providing an axiomatization of such a mereology. Based on the observation that the underlying structures in these domains are represented by graphs, we propose a new mereotopology that axiomatizes the connected induced subgraph containment ordering for a graph, and then identify an axiomatization of the mereology that is a module of the mereotopology.",A mereology for connected structures,2020
1102,Michael Gruninger,"Gene sequences are a focal point of modern biological research, with applications ranging from diagnostics to gene-driven drug design. An ontology’s automated reasoning capability and traceable logic is sure to be an asset to these efforts. However, current biomedical ontologies fail to achieve this potential. This may partly be due to a lack of formal axiomatization of the underlying molecular structure and mereology of gene sequences, despite an otherwise richly defined vocabulary. In this paper, we propose a new BioSequence Ontology with explicit axiomatization of the underlying path graph structure.",A BioSequence Ontology from Molecular Structure,2020
1103,Michael Gruninger,"Quantities and units of measure provide an important means by which intelligent agents interact with the physical world. Although multiple ontologies for quantities and units of measure have been proposed within the Applied Ontology community, they often incorporate questionable ontological commitments. Quantities are combined using notions of dimensional analysis that often conflate the combination of units with algebraic operations on real numbers. In this paper, we present an alternative approach that shifts the focus to the connection between kinds of measurements associated with a unit and the physical objects and processes that are being measured. One of the key features of this approach is that it makes minimal ontological commitments with respect to the TUpper upper ontology–the only new classes that are introduced are the classes for kinds of measures and associated units. We propose","The FOUnt ontologies for quantities, units, and the physical world",2020-01-01 00:00:00
1104,Michael Gruninger,"The discipline of Applied Ontology is facing several challenges. Some of these challenges concern ontologies themselves, for instance the ability to appropriately and exhaustively represent certain domains, or to correctly and efficiently perform reasoning; others are more tied to their use, for example how they are used in applications and with which benefits, or whether and how it is possible to align them with other existing ontologies. What seems evident is that, in order to be widely and profitably used, they should demonstrate a high quality level and this is unfortunately not always the case. All of this is reflected in our experience with the Journal. Many papers are submitted on domainspecific ontologies, but few are accepted because they are not meeting quality standards. Very rarely the ontologies they present are aligned or even inspired by upper level ontologies and, worse, quite often the presentation of the ontology consists solely of a collection of Protégé screenshots. The ontology has a poor axiomatization, and even if axioms are presented, we find mere translation of them into natural language, without justification or explanation. ",Proposed guidelines for publishing ontology papers,2020-01-01 00:00:00
1105,Michael Gruninger,"Location ontologies axiomatize the relationship between physical bodies and the space that they occupy, and there is a rich literature on the philosophical underpinnings of these ontologies. Existing location ontologies have given primacy to the structure of what might be called abstract space. Consequently, the spatial properties and relations between physical bodies are extracted from the spatial properties and relations of the spatial regions that they occupy. In this paper, we take a different approach by beginning with the philosophical position of mereological pluralism, in which there are different mereologies for different kinds of entities. In particular, we make the fundamental ontological commitment that the mereology for spatial regions is different than the mereology for physical bodies. Different intuitions about location and different classes of physical bodies can be formally specified by imposing different ",Location ontologies based on mereotopological pluralism,2020-01-01 00:00:00
1106,Michael Gruninger,in recent years there has been a resurgence of interest in our community in the shape analysis of d objects represented by surface meshes their voxelized interiors or surface point clouds in part this interest has been stimulated by the increased availability of rgbd cameras and by applications of computer vision to autonomous driving medical imaging and robotics in these settings spectral coordinates have shown promise for shape representation due to their ability to incorporate both local and global shape properties in a manner that is qualitatively invariant to isometric transformations yet surprisingly such coordinates have thus far typically considered only local surface positional or derivative information in the present article we propose to equip spectral coordinates with medial object width information so as to enrich them the key idea is to couple surface points that share a medial ball via the weights of the adjacency matrix we develop a spectral feature using this idea and the algorithms to compute it the incorporation of object width and medial coupling has direct benefits as illustrated by our experiments on object classification object part segmentation and surface point correspondence less,Medial Spectral Coordinates for 3D Shape Analysis,"29 November, 2021"
1107,Michael Gruninger,the distributed ontology language dol is currently being standardized within the ontoiop ontology integration and interoperability activity of isotc sc it aims at providing a unified framework for ontologies formalized in heterogeneous logics modular ontologies links between ontologies and annotation of ontologies this paper presents the current state of dols standardization it focuses on use cases where distributed ontologies enable interoperability and reusability we demonstrate relevant features of the dol syntax and semantics and explain how these integrate into existing knowledge engineering environments less,"The Distributed Ontology Language (DOL): Use Cases, Syntax, and Extensibility","1 August, 2012"
1108,Philip Kim,whole slide image wsi classification is a fundamental task for the diagnosis and treatment of diseases but curation of accurate labels is timeconsuming and limits the application of fullysupervised methods to address this multiple instance learning mil is a popular method that poses classification as a weakly supervised learning task with slidelevel labels only while current mil methods apply variants of the attention mechanism to reweight instance features with stronger models scant attention is paid to the properties of the data distribution in this work we propose to recalibrate the distribution of a wsi bag instances by using the statistics of the maxinstance critical feature we assume that in binary mil positive bags have larger feature magnitudes than negatives thus we can enforce the model to maximize the discrepancy between bags with a metric feature loss that models positive bags as outofdistribution to achieve this unlike existing mil methods that use singlebatch training modes we propose balancedbatch sampling to effectively use the feature loss ie bags simultaneously further we employ a position encoding module pem to model spatialmorphological information and perform pooling by multihead selfattention psma with a transformer encoder experimental results on existing benchmark datasets show our approach is effective and improves over stateoftheart mil methods less,Feature Re-calibration based MIL for Whole Slide Image Classification,"22 June, 2022"
1109,Philip Kim,while privacy concerns entice connected and automated vehicles to incorporate onboard federated learning fl solutions an integrated vehicletoeverything communication with heterogeneous computation power aware learning platform is urgently necessary to make it a reality motivated by this we propose a novel mobility communication and computation aware online fl platform that uses onroad vehicles as learning agents thanks to the advanced features of modern vehicles the onboard sensors can collect data as vehicles travel along their trajectories while the onboard processors can train machine learning models using the collected data to take the high mobility of vehicles into account we consider the delay as a learning parameter and restrict it to be less than a tolerable threshold to satisfy this threshold the central server accepts partially trained models the distributed roadside units a perform downlink multicast beamforming to minimize global model distribution delay and b allocate optimal uplink radio resources to minimize local model offloading delay and the vehicle agents conduct heterogeneous local model training using realworld vehicle trace datasets we validate our fl solutions simulation shows that the proposed integrated fl platform is robust and outperforms baseline models with reasonable local training episodes it can effectively satisfy all constraints and deliver near ground truth multihorizon velocity and vehiclespecific power predictions less,"Mobility, Communication and Computation Aware Federated Learning for Internet of Vehicles","17 May, 2022"
1110,Philip Kim,many transit agencies operating paratransit and microtransit services have to respond to trip requests that arrive in realtime which entails solving hard combinatorial and sequential decisionmaking problems under uncertainty to avoid decisions that lead to significant inefficiency in the long term vehicles should be allocated to requests by optimizing a nonmyopic utility function or by batching requests together and optimizing a myopic utility function while the former approach is typically offline the latter can be performed online we point out two major issues with such approaches when applied to paratransit services in practice first it is difficult to batch paratransit requests together as they are temporally sparse second the environment in which transit agencies operate changes dynamically eg traffic conditions causing estimates that are learned offline to become stale to address these challenges we propose a fully online approach to solve the dynamic vehicle routing problem dvrp with time windows and stochastic trip requests that is robust to changing environmental dynamics by construction we focus on scenarios where requests are relatively sparse our problem is motivated by applications to paratransit services we formulate dvrp as a markov decision process and use monte carlo tree search to evaluate actions for any given state accounting for stochastic requests while optimizing a nonmyopic utility function is computationally challenging indeed the action space for such a problem is intractably large in practice to tackle the large action space we leverage the structure of the problem to design heuristics that can sample promising actions for the tree search our experiments using realworld data from our partner agency show that the proposed approach outperforms existing stateoftheart approaches both in terms of performance and robustness less,An Online Approach to Solve the Dynamic Vehicle Routing Problem with Stochastic Trip Requests for Paratransit Services,"31 March, 2022"
1111,Philip Kim,we present bvri and unfiltered clear light curves of strippedenvelope supernovae sesne observed between and from the lick observatory supernova search loss followup program our sesn sample consists of spectroscopically normal sneib two peculiar sne ib six sn ibn normal sne ic one peculiar sn ic ten sne icbl sne iib one ambiguous sn iibibc and two superluminous sne our followup photometry has on a persn basis a mean coverage of photometric points median of points and a mean cadence of d median of d from our full sample a subset of sne have premaximum coverage in at least one passband allowing for the peak brightness of each sn in this subset to be quantitatively determined we describe our data collection and processing techniques with emphasis toward our automated photometry pipeline from which we derive publicly available data products to enable and encourage further study by the community using these data products we derive hostgalaxy extinction values through the empirical colour evolution relationship and for the first time produce accurate risetime measurements for a large sample of sesne in both optical and infrared passbands by modeling multiband light curves we find that sne ic tend to have lower ejecta masses and lower ejecta velocities than sneib and iib but higher ni masses less,The Lick Observatory Supernova Search follow-up program: photometry data release of 70 stripped-envelope supernovae,"10 March, 2022"
1112,Philip Kim,we present the current state of models for the zsim carbon monoxide co lineintensity signal targeted by the co mapping array project comap pathfinder in the context of its early science results our fiducial model relating dark matter halo properties to co luminosities informs parameter priors with empirical models of the galaxyhalo connection and previous co observations the pathfinder early science data spanning wavenumbers kmpc represent the first direct d constraint on the clustering component of the co power spectrum our upper limit on the redshiftspace clustering amplitude arm clustlesssimk greatly improves on the indirect upper limit of k reported from the co power spectrum survey copss measurement at ksimmpc the comap limit excludes a subset of models from previous literature and constrains interpretation of the copss results demonstrating the complementary nature of comap and interferometric co surveys using line bias expectations from our priors we also constrain the squared mean line intensitybias product langletbranglelesssimk and the cosmic molecular gas density texthtimesmodotmpc upper limits based on early instrument performance and our current co signal estimates we forecast that the fiveyear pathfinder campaign will detect the co power spectrum with overall signaltonoise of between then and now we also expect to detect the cogalaxy crossspectrum using overlapping galaxy survey data enabling enhanced inferences of cosmic starformation and galaxyevolution history less,COMAP Early Science: V. Constraints and Forecasts at $z \sim 3$,"4 March, 2022"
1113,Philip Kim,electronic nematicity in iron pnictide materials has been extensively studied by various experimental techniques yet its heat capacity anomaly at the phase transition has not been examined quantitatively in this work we review the thermodynamic description of nematicity in bafexcoxas using the landau free energy which defines the behavior of three thermodynamic quantities the structural orthorhombicity that develops below the nematic transition the softening shear modulus above the transition and the discontinuous heat capacity at the transition we derive a quantitative relationship between these three quantities which is found to hold for a range of dopings this result shows that the nematic transition is exceedingly well described by a meanfield model in the underdoped regime of the phase diagram less,"Quantitative relationship between structural orthorhombicity, shear modulus and heat capacity anomaly of the nematic transition in iron-based superconductors","8 December, 2021"
1114,Baochun Li,federated learning fl is typically performed in a synchronous parallel manner where the involvement of a slow client delays a training iteration current fl systems employ a participant selection strategy to select fast clients with quality data in each iteration however this is not always possible in practice and the selection strategy often has to navigate an unpleasant tradeoff between the speed and the data quality of clients in this paper we present pisces an asynchronous fl system with intelligent participant selection and model aggregation for accelerated training to avoid incurring excessive resource cost and stale training computation pisces uses a novel scoring mechanism to identify suitable clients to participate in a training iteration it also adapts the pace of model aggregation to dynamically bound the progress gap between the selected clients and the server with a provable convergence guarantee in a smooth nonconvex setting we have implemented pisces in an opensource fl platform called plato and evaluated its performance in largescale experiments with popular vision and language models pisces outperforms the stateoftheart synchronous and asynchronous schemes accelerating the timetoaccuracy by up to x and x respectively less,Pisces: Efficient Federated Learning via Guided Asynchronous Training,"18 June, 2022"
1115,Baochun Li,this paper investigates lowlatency streaming codes for a threenode relay network the source transmits a sequence of messages streaming messages to the destination through the relay between them where the firsthop channel from the source to the relay and the secondhop channel from the relay to the destination are subject to packet erasures every source message must be recovered perfectly at the destination subject to a fixed decoding delay of t time slots in any sliding window of t time slots we assume no more than n and n erasures are introduced by the firsthop channel and secondhop channel respectively under this channel loss assumption we fully characterize the maximum achievable rate in terms of t n and n the achievability is proved by using a symbolwise decodeforward strategy where the source symbols within the same message are decoded by the relay with different delays the converse is proved by analyzing the maximum achievable rate for each channel when the erasures in the other channel are consecutive bursty in addition we show that traditional messagewise decodeforward strategies which require the source symbols within the same message to be decoded by the relay with the same delay are suboptimal in general less,Optimal Streaming Erasure Codes over the Three-Node Relay Network,"10 March, 2022"
1116,Baochun Li,the accelerated convergence of digital and realworld lifestyles has imposed unprecedented demands on todays wireless network architectures as it is highly desirable for such architectures to support wireless devices everywhere with high capacity and minimal signaling overhead conventional architectures such as cellular architectures are not able to satisfy these requirements simultaneously and are thus no longer suitable for the future era in this paper we propose a capacitycentric c architecture for future wireless communication networks it is designed based on the principles of maximizing the number of nonoverlapping clusters with the average cluster capacity guaranteed to be higher than a certain threshold and thus provides a flexible way to balance the capacity requirement against the signaling overhead our analytical results reveal that c has superior generality wherein both the cellular and the fully coordinated architectures can be viewed as its extreme cases simulation results show that the average capacity of c is at least three times higher compared to that of the cellular architecture more importantly different from the widely adopted conventional wisdom that basestation distributions dominate architecture designs we find that the c architecture is independent of basestation distributions and instead userside information should be the focus in future wireless network architecture designs less,What Should Future Wireless Network Architectures Be?,"12 October, 2021"
1117,Baochun Li,machine learning ml models are increasingly trained in clusters with nondedicated workers possessing heterogeneous resources in such scenarios model training efficiency can be negatively affected by stragglers workers that run much slower than others efficient model training requires eliminating such stragglers yet for modern ml workloads existing load balancing strategies are inefficient and even infeasible in this paper we propose a novel strategy called semidynamic load balancing to eliminate stragglers of distributed ml workloads the key insight is that ml workers shall be loadbalanced at iteration boundaries being nonintrusive to intraiteration execution we develop lbbsp based on such an insight which is an integrated worker coordination mechanism that adapts workers load to their instantaneous processing capabilities by rightsizing the sample batches at the synchronization barriers we have customdesigned the batch sizing algorithm respectively for cpu and gpu clusters based on their own characteristics lbbsp has been implemented as a python module for ml frameworks like tensorflow and pytorch our ec deployment confirms that lbbsp is practical effective and lightweight and is able to accelerating distributed training by up to less,Semi-Dynamic Load Balancing: Efficient Distributed Learning in Non-Dedicated Environments,"8 December, 2020"
1118,Baochun Li,as a certified defensive technique randomized smoothing has received considerable attention due to its scalability to large datasets and neural networks however several important questions remain unanswered such as i whether the gaussian mechanism is an appropriate option for certifying ellnorm robustness and ii whether there is an appropriate randomized smoothing mechanism to certify ellinftynorm robustness to shed light on these questions we argue that the main difficulty is how to assess the appropriateness of each randomized mechanism in this paper we propose a generic framework that connects the existing frameworks in citelecuyercertified licertified to assess randomized mechanisms under our framework for a randomized mechanism that can certify a certain extent of robustness we define the magnitude of its required additive noise as the metric for assessing its appropriateness we also prove lower bounds on this metric for the ellnorm and ellinftynorm cases as the criteria for assessment based on our framework we assess the gaussian and exponential mechanisms by comparing the magnitude of additive noise required by these mechanisms and the lower bounds criteria we first conclude that the gaussian mechanism is indeed an appropriate option to certify ellnorm robustness surprisingly we show that the gaussian mechanism is also an appropriate option for certifying ellinftynorm robustness instead of the exponential mechanism finally we generalize our framework to ellpnorm for any pgeq our theoretical findings are verified by evaluations on cifar and imagenet less,Towards Assessment of Randomized Smoothing Mechanisms for Certifying Adversarial Robustness,"7 June, 2020"
1119,Baochun Li,graphbased semisupervised learning has been shown to be one of the most effective approaches for classification tasks from a wide range of domains such as image classification and text classification as they can exploit the connectivity patterns between labeled and unlabeled samples to improve learning performance in this work we advance this effective learning paradigm towards a scenario where labeled data are severely limited more specifically we address the problem of graphbased semisupervised learning in the presence of severely limited labeled samples and propose a new framework called em shoestring that improves the learning performance through semantic transfer from these very few labeled samples to large numbers of unlabeled samples in particular our framework learns a metric space in which classification can be performed by computing the similarity to centroid embedding of each class em shoestring is trained in an endtoend fashion to learn to leverage the semantic knowledge of limited labeled samples as well as their connectivity patterns with large numbers of unlabeled samples simultaneously by combining em shoestring with graph convolutional networks label propagation and their recent labelefficient variations igcn and glp we are able to achieve stateoftheart node classification performance in the presence of very few labeled samples in addition we demonstrate the effectiveness of our framework on image classification tasks in the fewshot learning regime with significant gains on miniimagenet sim and tieredimagenet sim less,Shoestring: Graph-Based Semi-Supervised Learning with Severely Limited Labeled Data,"8 April, 2020"
1120,Baochun Li,in this paper an improved multisource thermal model is used to analyze the transverse momentum spectra in pp collisions at high energies ranging from sqrtmathrmit snn gev to tev we give a detailed comparison between the theoretical results and experimental data at rhic and lhc energies it is shown that the excitation factors of emission sources depend linearly on lnsqrtmathrmit snn in the framework based on the variation regularity of the sourceexcitation factors transverse momentum spectra are predicted in pp collisions at higher energies potential future pp colliders operating at sqrtmathrmit snn and tev less,Multisource thermal model to the transverse momentum spectra in pp collisions at RHIC and LHC energies,"13 November, 2019"
1121,Baochun Li,based on the datadriven analysis the midrapidity transverse momentum spectra of charged hadrons produced in central and peripheral goldgold auau collisions from the beam energy scan bes program at the relativistic heavy ion collider rhic are fitted by the blastwave model with boltzmanngibbs statistics the model result are in agreement with the experimental data measured by the star collaboration at the rhicbes energies we observe that the kinetic freezeout temperature transverse flow velocity mean transverse momentum and initial temperature increase with the collision energy and with the event centrality less,Kinetic freeze-out temperature and transverse flow velocity in Au-Au collisions at RHIC-BES energies,"25 September, 2019"
1122,Baochun Li,this paper considers transmitting a sequence of messages a streaming source over a packet erasure channel in each time slot the source constructs a packet based on the current and the previous messages and transmits the packet which may be erased when the packet travels from the source to the destination every source message must be recovered perfectly at the destination subject to a fixed decoding delay we assume that the channel loss model introduces either one burst erasure or multiple arbitrary erasures in any fixedsized sliding window under this channel loss assumption we fully characterize the maximum achievable rate by constructing streaming codes that achieve the optimal rate in addition our construction of optimal streaming codes implies the full characterization of the maximum achievable rate for convolutional codes with any given column distance column span and decoding delay numerical results demonstrate that the optimal streaming codes outperform existing streaming codes of comparable complexity over some instances of the gilbertelliott channel and the fritchman channel less,Optimal Streaming Codes for Channels with Burst and Arbitrary Erasures,"5 December, 2018"
1123,Baochun Li,photoproduction of eta mesons from nucleons can provide valuable information about the excitation spectrum of the nucleons the angular dependence of photoproduction in the photoninduced reaction is investigated in the multisource thermal model the results are compared with experimental data from the decay mode they are in good agreement with the experimental data it is shown that the movement factor increases linearly with the photon beam energies and the deformation and translation of emission sources are visually given in the formalism less,Angular dependence of eta photoproduction in photon-induced reaction,"3 May, 2018"
1124,Baochun Li,in this paper we introduce a unified framework for studying various cloud traffic management problems ranging from geographical load balancing to backbone traffic engineering we first abstract these realworld problems as a multifacility resource allocation problem and then present two distributed optimization algorithms by exploiting the special structure of the problem our algorithms are inspired by alternating direction method of multipliers admm enjoying a number of unique features compared to dual decomposition they converge with nonstrictly convex objective functions compared to other admmtype algorithms they not only achieve faster convergence under weaker assumptions but also have lower computational complexity and lower messagepassing overhead the simulation results not only confirm these desirable features of our algorithms but also highlight several additional advantages such as scalability and faulttolerance less,An Alternating Direction Method Approach to Cloud Traffic Management,"2 February, 2016"
1125,Baochun Li,we advocate to create a emphspot internet transit market where transit is sold using the underutilized backbone capacity at a lower price the providers can improve profit by capitalizing the perishable capacity and customers can buy transit ondemand without a minimum commitment level for elastic traffic and as a result improve its surplus ie utility gains we conduct a systematic study of the economical benefits of spot transit both theoretically and empirically we propose a simple analytical framework with a general demand function and solve the pricing problem of maximizing the expected profit taking into account the revenue loss of regular transit when spot transit traffic hikes we rigorously prove the price advantage of spot transit as well as profit and surplus improvements for tier isps and customers respectively using realworld price data and traffic statistics of ixps with more than isps we quantitatively evaluate spot transit and show that significant financial benefits can be achieved in both absolute and relative terms robust to parameter values less,Spot Transit: Cheaper Internet Transit for Elastic Traffic,"25 September, 2013"
1126,Baochun Li,we investigate the thermodynamic and transport properties of the real scalar field theory at weak as well as strong couplings in the hartree approximation of cornwalljackiwtomboulis cjt formalism to our surprise we find that near phase transition all the thermodynamic and transport properties of the simplest real scalar model at certain strong coupling agree well with the lattice results of the complex qcd system we also demonstrate that the system near phase transition is nonconformal and behaves as a nonperfect fluid less,Non-conformality and non-perfectness of fluid near phase transition,"20 November, 2008"
1127,David Lie,we define the notions of bngeneralized pseudohermitian and bngeneralized pseudokahler structures on an odd exact courant algebroid e when e is in the standard form or of type bn we express these notions in terms of classical tensor fields on the base of e this is analogous to the bihermitian viewpoint on generalized kahler structures on exact courant algebroids we describe leftinvariant bngeneralized pseudokahler structures on courant algebroids of type bn over lie groups of dimension two three and four less,B_n-generalized pseudo-Kahler structures,"21 June, 2022"
1128,David Lie,we study the lie group structure of asymptotic symmetry groups in general relativity from the viewpoint of infinitedimensional geometry to this end we review the geometric definition of asymptotic simplicity and emptiness due to penrose and the coordinatewise definition of asymptotic flatness due to bondi et al then we construct the lie group structure of the bondimetznersachs bms group and discuss its lie theoretic properties we find that the bms group is regular in the sense of milnor but not real analytic this motivates us to conjecture that it is not locally exponential finally we verify the trotter property as well as the commutator property as an outlook we comment on the situation of related asymptotic symmetry groups in particular the much more involved situation of the newmanunti group is highlighted which will be studied in future work less,Lie Theory for Asymptotic Symmetries in General Relativity: The BMS Group,"26 January, 2022"
1129,David Lie,we establish finiteness of lowdimensional actions of lattices in higherrank semisimple lie groups and establish zimmers conjecture for many such groups this builds on previous work of the authors handling the case of actions by cocompact lattices and of actions by mathrmslnmathbb z while the results are not sharp in all cases they do dramatically improve all known results the key difficulty overcome in this paper concerns escape of mass when taking limits of sequences of measures due to a need to control lyapunov exponents for unbounded cocycles when taking such limits quantitative controls on the concentration of mass at infinity are need and novel techniques are introduced to avoid escape of lyapunov exponent less,Zimmer's conjecture for non-uniform lattices and escape of mass,"24 January, 2022"
1130,Jörg Liebeherr,hierarchical link sharing addresses the demand for finegrain traffic control at multiple levels of aggregation at present packet schedulers that can support hierarchical link sharing are not suitable for an implementation at line rates and deployed schedulers perform poorly when distributing excess capacity to classes that need additional bandwidth we present hls a packet scheduler that ensures a hierarchical maxmin fair allocation of the link bandwidth hls supports minimum rate guarantees and isolation between classes since it is realized as a nonhierarchical round robin scheduler it is suitable to operate at high rates we implement hls in the linux kernel and evaluate it with respect to achieved rate allocations and overhead we compare the results with those obtained for cbq and htb the existing scheduling algorithms in linux for hierarchical link sharing we show that the overhead of hls is comparable to that of other classful packet schedulers less,A Round-Robin Packet Scheduler for Hierarchical Max-Min Fairness,"22 August, 2021"
1131,Jörg Liebeherr,generalized processor sharing gps which provides the theoretical underpinnings for fair packet scheduling algorithms has been studied extensively however a tight formulation of the available service of a flow only exists for traffic that is regulated by affine arrival envelopes and constantrate links in this paper we show that the universal service curve by parekh and gallager can be extended to concave arrival envelopes and links with timevariable capacity we also dispense with the previously existing assumption of a stable system less,A General Per-Flow Service Curve for GPS,"21 April, 2018"
1132,Jörg Liebeherr,we present an extension of the window flow control analysis by r agrawal etal reference cs chang reference and cs chang et al reference to a system with random service time and fixed feedback delay we consider two network service models in the first model the network service process itself has no time correlations the second model addresses a twostate markovmodulated service less,Window Flow Control Systems with Random Service,"16 July, 2015"
1133,Jörg Liebeherr,a fundamental problem in the delay and backlog analysis across multihop paths in wireless networks is how to account for the random properties of the wireless channel since the usual statistical models for radio signals in a propagation environment do not lend themselves easily to a description of the available service rate on a wireless link the performance analysis of wireless networks has resorted to higherlayer abstractions eg using markov chain models in this work we propose a network calculus that can incorporate common statistical models of fading channels and obtain statistical bounds on delay and backlog across multiple nodes we conduct the analysis in a transfer domain which we refer to as the snr domain where the service process at a link is characterized by the instantaneous signaltonoise ratio at the receiver we discover that in the transfer domain the network model is governed by a dioid algebra which we refer to as minxalgebra using this algebra we derive the desired delay and backlog bounds an application of the analysis is demonstrated for a simple multihop network with rayleigh fading channels and for a network with cross traffic less,A Network Calculus Approach for the Analysis of Multi-Hop Fading Channels,"27 July, 2012"
1134,Jörg Liebeherr,we develop a stochastic foundation for bandwidth estimation of networks with random service where bandwidth availability is expressed in terms of bounding functions with a defined violation probability exploiting properties of a stochastic maxplus algebra and system theory the task of bandwidth estimation is formulated as inferring an unknown bounding function from measurements of probing traffic we derive an estimation methodology that is based on iterative constant rate probes our solution provides evidence for the utility of packet trains for bandwidth estimation in the presence of variable cross traffic taking advantage of statistical methods we show how our estimation method can be realized in practice with adaptive train lengths of probe packets probing rates and replicated measurements required to achieve both high accuracy and confidence levels we evaluate our method in a controlled testbed network where we show the impact of cross traffic variability on the timescales of service availability and provide a comparison with existing bandwidth estimation tools less,A Foundation for Stochastic Bandwidth Estimation of Networks with Random Service,"31 July, 2010"
1135,Jörg Liebeherr,it is shown that bandwidth estimation in packet networks can be viewed in terms of minplus linear system theory the available bandwidth of a link or complete path is expressed in terms of a em service curve which is a function that appears in the network calculus to express the service available to a traffic flow the service curve is estimated based on measurements of a sequence of probing packets or passive measurements of a sample path of arrivals it is shown that existing bandwidth estimation methods can be derived in the minplus algebra of the network calculus thus providing further mathematical justification for these methods principal difficulties of estimating available bandwidth from measurement of network probes are related to potential nonlinearities of the underlying network when networks are viewed as systems that operate either in a linear or in a nonlinear regime it is argued that probing schemes extract the most information at a point when the network crosses from a linear to a nonlinear regime experiments on the emulab testbed at the university of utah evaluate the robustness of the system theoretic interpretation of networks in practice multinode experiments evaluate how well the convolution operation of the minplus algebra provides estimates for the available bandwidth of a path from estimates of individual links less,A System Theoretic Approach to Bandwidth Estimation,"2 January, 2008"
1136,Kelly Lyons,"Context: Recent studies on open source platforms, such as GitHub, provide insights into how developers engage with software artifacts such as ReadMe files. Since ReadMe files are usually the first item users interact with in a repository, it is important that ReadMe files provide users with the information needed to engage with the corresponding repository. Objective: We investigate and compare ReadMe files of open source Java projects on GitHub in order to (i) determine the degree to which ReadMe files are aligned with the official guidelines,(ii) identify the common patterns in the structure of ReadMe files, and (iii) characterize the relationship between ReadMe file structure and popularity of associated repositories. Method: We apply statistical analyzes and clustering methods on 14,901 Java repositories to identify structural patterns of ReadMe files and the relationship of ReadMe file structure to repository stars",How ReadMe files are structured in open source Java projects,2022-08-01 00:00:00
1137,Kelly Lyons,"User reviews that are posted on the Google Play Store provide app developers with important information such as bug reports, feature requests, and user experience. Developers should maintain their apps while taking user feedback into account to succeed in the competitive market of mobile apps. the Google Play Store provides a star-rating mechanism for users to rate apps on a scale of one to five. Apps that are ranked higher and have higher star ratings are more likely to be downloaded. In this paper, we investigate and compare men’s and women’s participation in user reviews that are posted on the Google Play Store. We analyze 438,707 user reviews of the top 156 Android apps over six months. We find that women give higher star ratings and use more positive sentiment in their reviews than men. Furthermore, women’s reviews receive more likes and are ranked higher in the top 10 by the Google Play Store",A study of gender in user reviews on the Google Play Store,2022/3
1138,Kelly Lyons,"The adoption of the digital technologies by firms across large components of the Canadian economy has the potential to create significant disruption in the labour market and dramatically impact skill requirements for the labour force. To add to the increased uncertainty, the onset of the COVID-19 pandemic has introduced previously unforeseen challenges to the adoption and further development of digital technologies-potentially altering the speed of diffusion and pace of technical change and the transition to the digital economy for organizations and workers. This shifting landscape will likely have both short run and long run impacts that will impact firms and workers alike. This workshop brought together panel and audience members drawn from academia, government agencies and different industries to discuss how COVID-19 has affected the deployment and development of data science, and its impact on the ","What will the transition to a digital economy look like? exploring future skills, jobs, and policies needed post COVID",2021-11-22 00:00:00
1139,Kelly Lyons,"Data science provides methods to understand and solve problems in an evidence-based manner by combining data and experience with scientific methods. When included with advances in robotic technologies, telecommunication technologies and internet coverage (digitally enabling infrastructure), and computer hardware, data science creates a host of “digital technologies” expected to bring value to business, government, and individuals. Recent adoption and development of these innovations have been affected by the pandemic, even as COVID-19 sped up the move to online activities from shopping to working at home. In this paper, we analyze trends and changes to understand the levels of disruption from COVID-19 on the data science innovation ecosystem. We consider three phases of innovation: early-stage research and development; late-stage research and development; and commercialization and diffusion. We show that the COVID-19 pandemic has had a negative impact on innovation in data science across the phases of innovation to varying degrees; however, our analysis suggests a return to previous growth may be expected in the short term should the economy continue to improve. These findings suggest that researchers and practitioners should be prepared to take advantage of this return in growth to invest in early stage R&D, to build research programs in data science, and to find ways to commercialize and adopt data science innovations. Our research also identifies the need for coordinated efforts to make current and up-to-date data for tracking innovation impacts available.",Toward understanding the COVID-19 impact on Data Science Innovation in Canada,2021-11-22 00:00:00
1140,Kelly Lyons,"Economists have long recognized that technological innovation is a key contributor to economic growth due to its impact on productivity. In this paper, we explore the impact of COVID-19 on innovation in artificial intelligence (AI) to better understand future effects on economic growth and productivity. Using patents as a measure of innovation and knowledge production, we analyze monthly patent application filing data from January 2015 to June 2021 to compare and assess trends. Past research has shown that growth in patents in the fields of AI have accelerated since 2012, with 6.5 times more annual filings occurring from 2006 to 2017. Here, we focus specifically on determining if the pandemic has had an impact on this acceleration in AI-related innovation. To accomplish this task we must confront the challenge in using up-to-date patent data for this kind of analysis due to the fact that there are considerable time",Evaluating the disruption of COVID-19 on AI innovation using patent filings,2021-10-28 00:00:00
1141,Kelly Lyons,"Data science provides methods to understand and solve problems in an evidence-based manner by combining data and experience, with scientific methods When combined with advances in robotic technologies, telecommunication technologies and internet coverage (digitally enabling infrastructure), and computer hardware, data science creates a host of “digital technologies” expected to bring value to business, government, and individuals This potential has been rising, in part, because of access to growing amounts of data from business applications that can be used in conjunction with the expanding set of digital innovations across a wide array of applications","Evaluating the Future of Skills, Jobs, and Policies for the Post COVID Digital Economy",2021-08-15 00:00:00
1142,Kelly Lyons,"The frequency at which new research documents are being published causes challenges for researchers who increasingly need access to relevant documents in order to conduct their research. Searching across a variety of databases and browsing millions of documents to find semantically relevant material is a time-consuming task. Recently, there has been a focus on recommendation algorithms that suggest relevant documents based on the current interests of the researchers. In this paper, we describe the implementation of seven commonly used algorithms and three aggregation algorithms. We evaluate the recommendation algorithms in a large-scale biomedical knowledge base with the goal of identifying relative weaknesses and strengths of each algorithm. We analyze the recommendations from each algorithm based on assessments of output as evaluated by 14 biomedical researchers",A qualitative study of large-scale recommendation algorithms for biomedical knowledge bases,2021/6
1143,Kelly Lyons,"We discuss our experience in bringing data exchange to knowledge graphs. This experience includes the development of Kensho, a tool for generating mapping rules and performing knowledge exchange between two Knowledge Bases (KBs). We highlight the challenges addressed in Kensho, including managing the rich structural complexity of KBs and the need to handle incomplete correspondences between property paths. We use Kensho to highlight many open problems related to knowledge exchange including how knowledge translation can inform the task of KB integration and population.",Towards Knowledge Exchange: State-of-the-Art and Open Problems,2021-01-25 00:00:00
1144,Kelly Lyons,"Since ancient times, individuals have recognized that innovation and adoption of new technologies is affected by demand (ie,"" necessity is the mother of all invention""). Advances in data science, an interdisciplinary scientific approach that combines computation methods with data to understand and solve problems in an evidence-based manner, is no exception. Prior to the COVID-19 outbreak, the speed of data science adoption within organisations faced barriers such as legal/regulatory challenges, available work-force skills and financial costs. The emergence of the pandemic has altered the incentives to invest in, and adopt these innovations. This shifting landscape, will likely have both short run and long run impacts.",How has COVID-19 changed the development and adoption of data science across firms and industries?,2020-11-10 00:00:00
1145,Kelly Lyons,"With the evolution of the world economy from manufacturing and goods to a services context, the focus of researchers and businesses alike has shifted to building an understanding of how to foster service innovation. Design techniques, such as design thinking, have been utilized to engage customers in service co-creation to shape their service engagement for maximum benefit and to introduce new ideas and innovative approaches. This paper focuses on research approaches to study service innovation in commercial organizations, and findings from those experiences. Insights related to data and methods, organizational ecosystems, customer interactions and employee engagement will be discussed, with particular focus on how these impact service innovations.",Research Approaches to Service Innovation: Organizational Perspectives,2020-07-16 00:00:00
1146,Kelly Lyons,"The benefits of adopting data science are increasingly clear in a variety of industries, yet adoption rates remain low. In this paper we examine the barriers faced by organizations in adopting data science approaches in the context of service innovation. We first characterize three types of barriers: legal framework, organizational challenges, and risks. The legal framework around data science is in a state of change, and certain aspects are outdated and fragmented. Organizational issues include recruitment and a lack of diversity. Finally, risk is inherent in any business, but data science investments may be especially uncertain due to the fundamental role that datasets play and the lack of familiarity that those making decisions may have with data science. We present results in which we identify and expand on the links between these barriers and service innovation using data science.",Barriers to Service Innovation Using Data Science,2020-07-16 00:00:00
1147,Kelly Lyons,"GitHub facilitates software development practices that encourage collaboration and communication. Part of GitHub's model includes forking, which enables users to make changes on a copy of the base repository. The process of forking opens avenues of communication between the users from the base repository and the users from the forked repositories. Since forking on GitHub is a common mechanism for initiating repositories, we are interested in how communication between a repository and its forks (forming a software family) relates to stars. In this paper, we study communications within 385 software families comprised of 13,431 software repositories. We find that the fork depth, the number of users who have contributed to multiple repositories in the same family, the number of followers from outside the family, familial pull requests, and reported issues share a statistically significant relationship with repository",We are family: analyzing communication in GitHub software repositories and their forks,2020-02-18 00:00:00
1148,Kelly Lyons,we introduce kensho a tool for generating mapping rules between two knowledge bases kbs to create the mapping rules kensho starts with a set of correspondences and enriches them with additional semantic information automatically identified from the structure and constraints of the kbs our approach works in two phases in the first phase semantic associations between resources of each kb are captured in the second phase mapping rules are generated by interpreting the correspondences in a way that respects the discovered semantic associations among elements of each kb kenshos mapping rules are expressed using sparql queries and can be used directly to exchange knowledge from source to target kensho is able to automatically rank the generated mapping rules using a set of heuristics we present an experimental evaluation of kensho and assess our mapping generation and ranking strategies using more than synthesized and real world settings chosen to showcase some of the most important applications of knowledge translation in addition we use three existing benchmarks to demonstrate kenshos ability to deal with different mapping scenarios less,Knowledge Translation: Extended Technical Report,"3 August, 2020"
1149,Chris McIntosh,"Refinement of radiomic results and methodologies is required to ensure progression of the field. In this work, we establish a set of safeguards designed to improve and support current radiomic methodologies through detailed analysis of a radiomic signature. A radiomic model (MW2018) was fitted and externally validated using features extracted from previously reported lung and head and neck (H&N) cancer datasets using gross-tumour-volume contours, as well as from images with randomly permuted voxel index values; i.e. images without meaningful texture. To determine MW2018’s added benefit, the prognostic accuracy of tumour volume alone was calculated as a baseline.",Vulnerabilities of radiomic signature development: the need for safeguards,2019-01-01 00:00:00
1150,Chris McIntosh,"Breakthroughs in artificial intelligence (AI) hold enormous potential as it can automate complex tasks and go even beyond human performance. In their study, McKinney et al. 1 showed the high potential of AI for breast cancer screening. However, the lack of details of the methods and algorithm code undermines its scientific value. Here, we identify obstacles that hinder transparent and reproducible AI research as faced by McKinney et al. 1, and provide solutions to these obstacles with implications for the broader field. The work by McKinney et al. 1 demonstrates the potential of AI in medical imaging, while highlighting the challenges of making such work reproducible. The authors assert that their system improves the speed and robustness of breast cancer screening, generalizes to populations beyond those used for training, and outperforms radiologists in specific settings. Upon successful prospective clinical ",Transparency and reproducibility in artificial intelligence,2020/10
1151,Chris McIntosh,"Recent works in automated radiotherapy treatment planning have used machine learning based on historical treatment plans to infer the spatial dose distribution for a novel patient directly from the planning image. We present a probabilistic, atlas-based approach which predicts the dose for novel patients using a set of automatically selected most similar patients (atlases). The output is a spatial dose objective, which specifies the desired dose-per-voxel, and therefore replaces the need to specify and tune dose-volume objectives. Voxel-based dose mimicking optimization then converts the predicted dose distribution to a complete treatment plan with dose calculation using a collapsed cone convolution dose engine. In this study, we investigated automated planning for right-sided oropharaynx head and neck patients treated with IMRT and VMAT. We compare four versions of our dose prediction pipeline using a",Fully automated treatment planning for head and neck radiotherapy using a voxel-based dose prediction and dose mimicking method,2017-07-06 00:00:00
1152,Chris McIntosh,"Segmenting anatomical structures from medical images is usually one of the most important initial steps in many applications, including visualization, computer-aided diagnosis, and morphometric analysis. Manual 2D segmentation suffers from operator variability and is tedious and time-consuming. These disadvantages are accentuated in 3D applications and, the additional requirement of producing intuitive displays to integrate 3D information for the user, makes manual segmentation even less approachable in 3D. Robust, automatic medical image segmentation in 2D to 3D remains an open problem caused particularly by sensitivity to low-level parameters of segmentation algorithms. Semi-automatic techniques present possible balanced solution where automation focuses on low-level computing-intensive tasks that can be hidden from the user, while manual inter- vention captures high-level expert knowledge ",3D live-wire-based semi-automatic segmentation of medical images,2005-04-29 00:00:00
1153,Chris McIntosh,"We present a novel approach to the segmentation and analysis of vasculature from volumetric medical image data. Our method is an adoption and significant extension of deformable organisms, an artificial life framework for medical image analysis that complements classical deformable models with high-level, anatomically-driven control mechanisms. We extend deformable organisms to 3D, model their bodies as tubular spring-mass systems, and equip them with a new repertoire of sensory modules, behavioral routines, and decision making strategies. The result is a new breed of robust deformable organisms, vessel crawlers, that crawl along vasculature in 3D images, accurately segmenting vessel boundaries, detecting and exploring bifurcations, and providing sophisticated, clinically-relevant structural analysis. We validate our method through the segmentation and analysis of vascular structures in both noisy ",Vessel crawlers: 3d physically-based deformable organisms for vasculature segmentation and analysis,2006-06-17 00:00:00
1154,Chris McIntosh,"Automating the radiotherapy treatment planning process is a technically challenging problem. The majority of automated approaches have focused on customizing and inferring dose volume objectives to be used in plan optimization. In this work we outline a multi-patient atlas-based dose prediction approach that learns to predict the dose-per-voxel for a novel patient directly from the computed tomography planning scan without the requirement of specifying any objectives. Our method learns to automatically select the most effective atlases for a novel patient, and then map the dose from those atlases onto the novel patient. We extend our previous work to include a conditional random field for the optimization of a joint distribution prior that matches the complementary goals of an accurately spatially distributed dose distribution while still adhering to the desired dose volume histograms. The resulting distribution can",Voxel-based dose prediction with multi-patient atlas selection for automated radiotherapy treatment planning,2016-12-20 00:00:00
1155,Chris McIntosh,"Spinal cord analysis is an important problem relating to the study of various neurological diseases. We present a novel approach to spinal cord segmentation in magnetic resonance images. Our method uses 3D “deformable organisms” (DefOrg) an artificial life framework for medical image analysis that complements classical deformable models (snakes and deformable meshes) with high-level, anatomically-driven control mechanisms. The DefOrg framework allows us to model the organism’s body as a growing generalized tubular spring-mass system with an adaptive and predominantly elliptical cross section, and to equip them with spinal cord specific sensory modules, behavioral routines and decision making strategies. The result is a new breed of robust DefOrgs, “spinal crawlers”, that crawl along spinal cords in 3D images, accurately segmenting boundaries, and providing sophisticated, clinically ",Spinal crawlers: Deformable organisms for spinal cord segmentation and analysis,2006
1156,Chris McIntosh,"We explore the application of genetic algorithms (GA) to deformable models through the proposition of a novel method for medical image segmentation that combines GA with nonconvex, localized, medial-based shape statistics. We replace the more typical gradient descent optimizer used in deformable models with GA, and the convex, implicit, global shape statistics with nonconvex, explicit, localized ones. Specifically, we propose GA to reduce typical deformable model weaknesses pertaining to model initialization, pose estimation and local minima, through the simultaneous evolution of a large number of models. Furthermore, we constrain the evolution, and thus reduce the size of the search-space, by using statistically-based deformable models whose deformations are intuitive (stretch, bulge, bend) and are driven in terms of localized principal modes of variation, instead of modes of variation across the entire",Medial-based deformable models in nonconvex shape-spaces for medical image segmentation,2011-07-21 00:00:00
1157,Chris McIntosh,"Radiation therapy is an integral part of cancer treatment, but to date it remains highly manual. Plans are created through optimization of dose volume objectives that specify intent to minimize, maximize, or achieve a prescribed dose level to clinical targets and organs. Optimization is NP-hard, requiring highly iterative and manual initialization procedures. We present a proof-of-concept for a method to automatically infer the radiation dose directly from the patient's treatment planning image based on a database of previous patients with corresponding clinical treatment plans. Our method uses regression forests augmented with density estimation over the most informative features to learn an automatic atlas-selection metric that is tailored to dose prediction. We validate our approach on 276 patients from 3 clinical treatment plan sites (whole breast, breast cavity, and prostate), with an overall dose prediction accuracies ",Contextual atlas regression forests: multiple-atlas-based automated dose prediction in radiation therapy,2015-12-03 00:00:00
1158,Chris McIntosh,"Radiation therapy is used to treat cancer patients around the world. High quality treatment plans maximally radiate the targets while minimally radiating healthy organs at risk. In order to judge plan quality and safety, segmentations of the targets and organs at risk are created, and the amount of radiation that will be delivered to each structure is estimated prior to treatment. If the targets or organs at risk are mislabelled, or the segmentations are of poor quality, the safety of the radiation doses will be erroneously reviewed and an unsafe plan could proceed. We propose a technique to automatically label groups of segmentations of different structures from a radiation therapy plan for the joint purposes of providing quality assurance and data mining. Given one or more segmentations and an associated image we seek to assign medically meaningful labels to each segmentation and report the confidence of that label. ",Groupwise conditional random forests for automatic shape classification and contour quality assessment in radiotherapy planning,2013-03-06 00:00:00
1159,Chris McIntosh,"Image segmentation is often performed via the minimization of an energy function over a domain of possible segmentations. The effectiveness and applicability of such methods depends greatly on the properties of the energy function and its domain, and on what information can be encoded by it. Here we propose an energy function that achieves several important goals. Specifically, our energy function is convex and incorporates shape prior information while simultaneously generating a probabilistic segmentation for multiple regions. Our energy function represents multi-region probabilistic segmentations as elements of a vector space using the isometric log-ratio (ILR) transformation. To our knowledge, these four goals (convex, with shape priors, multi-region, and probabilistic) do not exist together in any other method, and this is the first time ILR is used in an image segmentation method. We provide examples",Convex Multi-Region Probabilistic Segmentation with Shape Prior in the Isometric Log-Ratio Transformation Space,2011-07-27 00:00:00
1160,Chris McIntosh,"Energy functional minimization is an increasingly popular technique for image segmentation. However, it is far too commonly applied with hand-tuned parameters and initializations that have only been validated for a few images. Fixing these parameters over a set of images assumes the same parameters are ideal for each image. We highlight the effects of varying the parameters and initialization on segmentation accuracy and propose a framework for attaining improved results using image adaptive parameters and initializations. We provide an analytical definition of optimal weights for functional terms through an examination of segmentation in the context of image manifolds, where nearby images on the manifold require similar parameters and similar initializations. Our results validate that fixed parameters are insufficient in addressing the variability in real clinical data, that similar images require similar ",Is a single energy functional sufficient? Adaptive energy functionals and automatic initialization,2007
1161,Chris McIntosh,the international linear collider ilc is on the table now as a new global energyfrontier accelerator laboratory taking data in the s the ilc addresses key questions for our current understanding of particle physics it is based on a proven accelerator technology its experiments will challenge the standard model of particle physics and will provide a new window to look beyond it this document brings the story of the ilc up to date emphasizing its strong physics motivation its readiness for construction and the opportunity it presents to the us and the global particle physics community less,The International Linear Collider: Report to Snowmass 2021,"14 March, 2022"
1162,Chris McIntosh,accurate prognosis for an individual patient is a key component of precision oncology recent advances in machine learning have enabled the development of models using a wider range of data including imaging radiomics aims to extract quantitative predictive and prognostic biomarkers from routine medical imaging but evidence for computed tomography radiomics for prognosis remains inconclusive we have conducted an institutional machine learning challenge to develop an accurate model for overall survival prediction in head and neck cancer using clinical data etxracted from electronic medical records and pretreatment radiological images as well as to evaluate the true added benefit of radiomics for head and neck cancer prognosis using a large retrospective dataset of patients and a rigorous evaluation framework we compared different submissions using imaging and clinical data separately or in combination the winning approach used nonlinear multitask learning on clinical data and tumour volume achieving high prognostic accuracy for year and lifetime survival prediction and outperforming models relying on clinical data only engineered radiomics and deep learning combining all submissions in an ensemble model resulted in improved accuracy with the highest gain from a imagebased deep learning model our results show the potential of machine learning and simple informative prognostic factors in combination with large datasets as a tool to guide personalized cancer care less,A Machine Learning Challenge for Prognostic Modelling in Head and Neck Cancer Using Multi-modal Data,"28 January, 2021"
1163,Chris McIntosh,in their study mckinney et al showed the high potential of artificial intelligence for breast cancer screening however the lack of detailed methods and computer code undermines its scientific value we identify obstacles hindering transparent and reproducible ai research as faced by mckinney et al and provide solutions with implications for the broader field less,The importance of transparency and reproducibility in artificial intelligence research,"7 March, 2020"
1164,Alex Mihailidis,"Powered wheelchair use promotes participation in individuals with limited mobility, however training is required for safe and effective use. There is limited evidence on the task demands of powered wheelchair use to inform an evidence-based skills training programme. We used a two-phased think aloud process to conduct a task analysis of powered wheelchair use with experienced powered wheelchair users (n = 5) and expert clinicians (n = 5). Participants completed seven indoor driving tasks while speaking aloud (concurrent think aloud) and subsequently engaged in a structured qualitative interview to discuss skills, abilities, and knowledge used across each of the seven tasks (retrospective think aloud). ",Understanding the task demands for powered wheelchair driving: A think-aloud task analysis,2022-08-18 00:00:00
1165,Alex Mihailidis,"Falls are one of the leading cause of injury-related deaths among the elderly worldwide. Effective detection of falls can reduce the risk of complications and injuries. Fall detection can be performed using wearable devices or ambient sensors; these methods may struggle with user compliance issues or false alarms. Video cameras provide a passive alternative; however, regular RGB cameras are impacted by changing lighting conditions and privacy concerns. From a machine learning perspective, developing an effective fall detection system is challenging because of the rarity and variability of falls. Many existing fall detection datasets lack important real-world considerations, such as varied lighting, continuous activities of daily living (ADLs), and camera placement. The lack of these considerations makes it difficult to develop predictive models that can operate effectively in the real world. To address these limitations, we introduce a novel multi-modality dataset (MUVIM) that contains four visual modalities: infra-red, depth, RGB and thermal cameras. ",Multi Visual Modality Fall Detection Dataset,2022-06-25 00:00:00
1166,Alex Mihailidis,"We conducted a wide database search. All studies were independently screened by 2 reviewers (XW and YF), with a third reviewer (BY) involved in resolving discrepancies. The final included studies were rated according to their level of clinical evidence based on their correlation with clinical scales (with the same tasks or the same evaluation criteria). One reviewer (XW) extracted data on publication, demographic information, compensation types, sensors used for compensation assessment, compensation measurements, and statistical or artificial intelligence methods. Accuracy was checked by another reviewer (YF). Four research questions were presented. For each question, the data were synthesized and tabulated, and a descriptive summary of the findings was provided. The data were synthesized and tabulated based on each research question.",Technology-Based Compensation Assessment and Detection of Upper Extremity Activities of Stroke Survivors: Systematic Review,2022-06-13 00:00:00
1167,Alex Mihailidis,"The COVID-19 pandemic is having a major impact on the lives of everyone, but in particular on the health and well-being of older people. It has also disrupted the way that individuals access services and interact with one another, and physical distancing and “Stay at Home” orders have seen digital interaction become a necessity. While these restrictions have highlighted the importance of technology in everyday life, little is known about how older adults have responded to this change.",Older People’s Use of Digital Technology During the COVID-19 Pandemic,2022/6
1168,Alex Mihailidis,"Consensus methods have been used in health care for a long time to reach agreement among experts when there is a lack of information or conflicting information on a health topic. The Delphi and nominal group techniques are extensively used in health research. Although both consensus methods are transparent in developing health research agendas, their emphasis on clinical and academic experts is problematic in Indigenous research. Another consensus approach named Glaser’s state-of-the-art is being used in Indigenous research. In this approach, a panel of experts identifies additional experts who collectively engage in iterative rounds to develop a consensus statement based on current research. We will be using a modified Glaser’s state-of-the-art approach to develop an informant-based functional assessment tool to assess the instrumental activities of daily living in people living with dementia. In the first phase, we will form a core research team, set up an Indigenous community advisory group (CAG), and conduct a focus group with health professionals and in-depth interviews with caregivers to develop a draft functional assessment tool. ",Canadian Consortium on Neurodegeneration in Aging (CCNA) Partners Forum and Science Days 2021: Abstracts from the trainee poster competition: COLLABORATION FOR CONNECTIVITY ,2022-03-01 00:00:00
1169,Alex Mihailidis,"The present disclosure describes a system, device, and method for assisting a user to avoid contacting surfaces with their mobile device. An environment is sensed with one or more electronic sensors. The sensor readings are analyzed. Information is then provided to a user based on the analyzed sensor readings. The sensors may be configured so their sensor cones cross at a midpoint. Readings from the sensor (s) may be grouped according detection zone (s) corresponding to one or more areas about a mobile device. A computing module may control a feedback module according to detection zone readings. The feedback module may comprise an indicator for each detection zone. The indicator may be a vibration motor. The indicator may be a light. The computing module may set the colour of a light and/or control the vibrations based on the proximity of surfaces detected within the corresponding detection zone.","System, device and method for mobile device environment sensing and user feedback",2022-02-08 00:00:00
1170,Alex Mihailidis," Caregiving is highly stressful and is associated with poor mental and physical health. Various technologies, including mobile and eHealth apps, have been developed to address caregiver needs. However, there is still a paucity of research examining the technology perceptions of informal caregivers, especially from the perspectives of sex, gender, and diversity. Semistructured interviews were conducted with 16 informal caregivers of individuals with a range of chronic medical conditions and 8 technology researchers involved in caregiving technology projects.",Perceptions of Digital Technology Experiences and Development Among Family Caregivers and Technology Researchers: Qualitative Study,2022-01-28 00:00:00
1171,Alex Mihailidis,"Behavioural symptoms of dementia present a significant risk within Long Term Care (LTC) homes, which face difficulties supporting residents and monitoring their safety with limited staffing resources. Many LTC facilities have installed video surveillance systems in common areas that can help staff to observe residents; however, typically these video streams are not monitored. In this paper, we present the development of a computer vision algorithm to use these video streams to detect episodes of clinically important agitation in people with dementia. Given that episodes of agitation are rare in comparison to normal behaviours, we formulated this as an anomaly detection problem. This involves using the video camera to monitor the scene rather than tracking individuals. We developed a customized spatio-temporal convolution autoencoder that is trained on the normal behaviours and then identified agitation during",Unsupervised deep learning to detect agitation from videos in people with dementia,2022-01-18 00:00:00
1172,Alex Mihailidis,"Emotion dynamics, the continuous evaluation of emotion, is an understudied research field. However, understanding how emotions evolve over time could open new doors for emotion regulation interventions–as regulation strategies are deployed in relation to the temporal progression of emotional events (eg, strategies antecedent or consequent to outbursts of anger for individuals with a traumatic brain injury (TBI)). This study explores how physiology, specifically characteristics of photoplethysmography heart-rate variability (PPG-HRV), could be used to track dynamic valence emotion. Prior psychophysiology research on emotion-physiology relationships typically assume that these relationships exist in a co-occurring context. Meaning, if a trait of physiology is seen to change with an emotional phenomenon in one scenario, and then seen not to change with the same emotional phenomenon in another scenario, that this is evidence of a weak or null emotion-physiology relationship.",Exploring Dynamic Estimation of Valence for Emotion Regulation,2022
1173,Alex Mihailidis,"Anger dyscontrol is a common issue after traumatic brain injury (TBI). With the growth of wearable physiological sensors, there is new potential to facilitate the rehabilitation of such anger in the context of daily life. This potential, however, depends on how well physiological markers can distinguish changing emotional states and for such markers to generalize to real-world settings. Our study explores how wearable photoplethysmography (PPG), one of the most widely available physiological sensors, could be used detect anger within a heterogeneous population. This study collected the TRIEP (Toronto Rehabilitation Institute Emotion-Physiology) dataset, which comprised of 32 individuals (10 TBI), exposed to a variety of elicitation material (film, pictures, self-statements, personal recall), over two day sessions. This complex dataset allows for exploration into how the emotion-PPG relationship varies over changes in individuals, endogenous/exogenous drivers of emotion, and day-to-day differences. ",Towards PPG-Based Anger Detection for Emotion Regulation,2022
1174,Alex Mihailidis,"Backdriveable actuators with energy regeneration can improve the efficiency and extend the battery-powered operating times of robotic lower-limb exoskeletons by converting some of the otherwise dissipated energy during negative mechanical work into electrical energy. However, previous related studies have focused on steady-state level-ground walking. To better encompass real-world community mobility, here we developed a feedforward human-exoskeleton energy regeneration system model to simulate energy regeneration and storage during other daily locomotor activities. Data from inverse dynamics analyses of 10 healthy young adults walking at variable speeds and slopes were used to calculate the negative joint mechanical power and work (i.e., the mechanical energy theoretically available for electrical energy regeneration). ",Simulation of Energy Regeneration in Human Locomotion for Efficient Exoskeleton Actuation,2022-01-01 00:00:00
1175,Alex Mihailidis,"The COVID-19 pandemic is having a major impact on the lives of everyone, but in particular on the health and well-being of older people. It has also disrupted the way that individuals access services and interact with one another, and physical distancing and “Stay at Home” orders have seen digital interaction become a necessity. While these restrictions have highlighted the importance of technology in everyday life, little is known about how older adults have responded to this change. Two surveys, one in 2019 and another in 2020 collected data on a combined total of 1923 older adults aged 65 years and older in Canada. These looked at how older adults think about and use technology, with the 2020 survey additionally questioning how COVID-19 has impacted their use and attitudes towards technology.",Older People’s Use of Digital Technology During the COVID-19 Pandemic,2022
1176,Alex Mihailidis,falls are one of the leading cause of injuryrelated deaths among the elderly worldwide effective detection of falls can reduce the risk of complications and injuries fall detection can be performed using wearable devices or ambient sensors these methods may struggle with user compliance issues or false alarms video cameras provide a passive alternative however regular rgb cameras are impacted by changing lighting conditions and privacy concerns from a machine learning perspective developing an effective fall detection system is challenging because of the rarity and variability of falls many existing fall detection datasets lack important realworld considerations such as varied lighting continuous activities of daily living adls and camera placement the lack of these considerations makes it difficult to develop predictive models that can operate effectively in the real world to address these limitations we introduce a novel multimodality dataset muvim that contains four visual modalities infrared depth rgb and thermal cameras these modalities offer benefits such as obfuscated facial features and improved performance in lowlight conditions we formulated fall detection as an anomaly detection problem in which a customized spatiotemporal convolutional autoencoder was trained only on adls so that a fall would increase the reconstruction error our results showed that infrared cameras provided the highest level of performance auc roc followed by thermal auc roc depth auc roc and rgb auc roc this research provides a unique opportunity to analyze the utility of camera modalities in detecting falls in a home setting while balancing performance passiveness and privacy less,Multi Visual Modality Fall Detection Dataset,"25 June, 2022"
1177,Alex Mihailidis,human falls rarely occur however detecting falls is very important from the health and safety perspective due to the rarity of falls it is difficult to employ supervised classification techniques to detect them moreover in these highly skewed situations it is also difficult to extract domain specific features to identify falls in this paper we present a novel framework textitdeepfall which formulates the fall detection problem as an anomaly detection problem the textitdeepfall framework presents the novel use of deep spatiotemporal convolutional autoencoders to learn spatial and temporal features from normal activities using noninvasive sensing modalities we also present a new anomaly scoring method that combines the reconstruction score of frames across a temporal window to detect unseen falls we tested the textitdeepfall framework on three publicly available datasets collected through noninvasive sensing modalities thermal camera and depth cameras and show superior results in comparison to traditional autoencoder methods to identify unseen falls less,DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal Convolutional Autoencoders,"27 April, 2020"
1178,Alex Mihailidis,presence of missing values in a dataset can adversely affect the performance of a classifier single and multiple imputation are normally performed to fill in the missing values in this paper we present several variants of combining single and multiple imputation with bootstrapping to create ensembles that can model uncertainty and diversity in the data and that are robust to high missingness in the data we present three ensemble strategies bootstrapping on incomplete data followed by i single imputation and ii multiple imputation and iii multiple imputation ensemble without bootstrapping we perform an extensive evaluation of the performance of the these ensemble strategies on datasets by varying the missingness ratio our results show that bootstrapping followed by multiple imputation using expectation maximization is the most robust method even at high missingness ratio up to for small missingness ratio up to most of the ensemble methods perform quivalently but better than single imputation kappaerror plots suggest that accurate classifiers with reasonable diversity is the reason for this behaviour a consistent observation in all the datasets suggests that for small missingness up to bootstrapping on incomplete data without any imputation produces equivalent results to other ensemble methods less,Bootstrapping and Multiple Imputation Ensemble Approaches for Missing Data,"15 October, 2019"
1179,Alex Mihailidis,we present the development and evaluation of a hand tracking algorithm based on single depth images captured from an overhead perspective for use in the coach prompting system we train a random decision forest body part classifier using approximately manually labeled unbalanced partially labeled training images the classifier represents a random subset of pixels in each depth image with a learned probability density function across all trained body parts a local modefind approach is used to search for clusters present in the underlying feature space sampled by the classified pixels in each frame body part positions are chosen as the mode with the highest confidence user hand positions are translated into hand washing task actions based on proximity to environmental objects we validate the performance of the classifier and task action proposals on a large set of approximately manually labeled images less,"Depth image hand tracking from an overhead perspective using partially labeled, unbalanced data: Development and real-world testing","6 September, 2014"
1180,Quaid Morris,"Large chromosomal alterations are common in cancer and often show preferential gain or loss across many cancer types indicating their selective advantage. Triple negative breast cancer (TNBC) exhibits complex mutational spectrum without common oncogenic drivers yet displays consistent loss of large chromosomal regions. Here, we characterize selection pressures that maintain a recurrently deleted region of chromosome 4p in TNBC. We used single cell and bulk WGS phylogenetic analysis of TNCB PT/PDX panel to show that chr4p deletion is an early event in tumor evolution. We used scRNAseq gene expression and inferred copy number analysis to show that chr4p loss is associated with a proliferative state. This finding was confirmed by a combination of RNA in situ hybridization and immunofluorescence.",Evolution of large copy number variants in breast cancer through genetic network rewiring,2022-06-15 00:00:00
1181,Quaid Morris,"The DeepTumour algorithm predicts the tissue of origin of a tumor based on the pattern of passenger mutations identified by whole genome sequencing. ""Passengers"" are incidental mutations that accrue in the genome over time due to random mutational processes, and are functionally distinct from the ""driver"" mutations that are responsible for the cancer's malignant behavior. In adult cancers, passenger mutations typically outnumber drivers by a hundred or thousand-fold; critically, the vast majority of passengers arise in the normal cell lineage that precedes the malignant transformation event and hence reflects mutational processes existing in the cancer's precursor cell and its ancestors.",DeepTumour: Identify tumor origin from whole genome sequences,2022-06-15 00:00:00
1182,Quaid Morris,"The genetic architecture of blood is highly complex with germline polymorphisms, somatic point mutations, and larger chromosomal alterations playing a role in shaping the fitness of the immune cells. Many advances have been made in understanding how the age associated acquisition of point mutations and somatic structural variants (SSVs) in blood, termed Age-Related Clonal Hematopoiesis (ARCH), predispose individuals to hematological cancer or cardiovascular disease. Yet, ARCH is commonly observed in healthy individuals and our ability to predict who is at risk of progressing to disease remains limited. A previous study which integrated deep learning and population genetics methods to evaluate the complex interplay of selection on point mutations in deeply sequenced blood samples highlighted the role that negative selection plays in prevention progression to hematological cancer. ",A multi-omic perspective of how selection shapes blood cancer risk phenotypes in aging populations,2022-06-15 00:00:00
1183,Quaid Morris,"Cancers are composed of genetically distinct subpopulations of malignant cells. DNA-sequencing data can be used to determine the somatic point mutations specific to each population and build clone trees describing the evolutionary relationships between them. These clone trees can reveal critical points in disease development and inform treatment. Pairtree is a new method that constructs more accurate and detailed clone trees than previously possible using variant allele frequency data from one or more bulk cancer samples. It does so by first building a Pairs Tensor that captures the evolutionary relationships between pairs of subpopulations, and then it uses these relations to constrain clone trees and infer violations of the infinite sites assumption. Pairtree can accurately build clone trees using up to 100 samples per cancer that contain 30 or more subclonal populations.",Reconstructing complex cancer evolutionary histories from multiple bulk DNA samples using PairtreeReconstructing cancer evolutionary histories using Pairtree,2022-05-05 00:00:00
1184,Quaid Morris,"Understanding the regulatory interactions that control gene expression during the development of novel tissues is a key goal of evolutionary developmental biology. Here, we show that Mbnl3 has undergone a striking process of evolutionary specialization in eutherian mammals resulting in the emergence of a novel placental function for the gene. Mbnl3 belongs to a family of RNA-binding proteins whose members regulate multiple aspects of RNA metabolism. We find that, in eutherians, while both Mbnl3 and its paralog Mbnl2 are strongly expressed in placenta, Mbnl3 expression has been lost from nonplacental tissues in association with the evolution of a novel promoter. Moreover, Mbnl3 has undergone accelerated protein sequence evolution leading to changes in its RNA-binding specificities and cellular localization. While Mbnl2 and Mbnl3 share partially redundant roles in regulating alternative splicing, polyadenylation site usage and, in turn, placenta maturation, Mbnl3 has also acquired novel biological functions.",The X-linked splicing regulator MBNL3 has been co-opted to restrict placental growth in eutherians,2022-04-27 00:00:00
1185,Quaid Morris,"Tumours are dynamically evolving populations of cells. Subclonal reconstruction algorithms use bulk DNA sequencing data to quantify parameters of tumour evolution, allowing assessment of how cancers initiate, progress and respond to selective pressures. A plethora of subclonal reconstruction algorithms have been created, but their relative performance across the varying biological and technical features of real-world cancer genomic data is unclear. We therefore launched the ICGC-TCGA DREAM Somatic Mutation Calling -- Tumour Heterogeneity and Evolution Challenge. This seven-year community effort used cloud-computing to benchmark 31 containerized subclonal reconstruction algorithms on 51 simulated tumours. Each algorithm was scored for accuracy on seven independent tasks, leading to 12,061 total runs. Algorithm choice influenced performance significantly more than tumour features, but purity-adjusted read-depth, copy number state and read mappability were associated with performance of most algorithms on most tasks. ",Crowd-sourced benchmarking of single-sample tumour subclonal reconstruction,2022-01-01 00:00:00
1186,Quaid Morris,"Cancer genomes harbor a catalog of somatic mutations. The type and genomic context of these mutations depend on their causes, and allow their attribution to particular mutational signatures. Previous work has shown that mutational signature activities change over the course of tumor development, but investigations of genomic region variability in mutational signatures have been limited. Here, we expand upon this work by constructing regional profiles of mutational signature activities over 2,203 whole genomes across 25 tumor types, using data aggregated by the Pan-Cancer Analysis of Whole Genomes (PCAWG) consortium. We find that 426 genomes from 20 tumor types display at least one change in mutational signature activities (changepoint), and 174 genomes contain a recurrent changepoint shared by three or more genomes of the same tumor type. Twenty-three recurrent changepoint locations are shared by multiple tumor types. Within these regions, the particular signature changes are often consistent across samples of the same type, and some, but not all are characterized by signatures associated with subclonal expansion. ",Regional Mutational Signature Activities in Cancer Genomes,2022-01-01 00:00:00
1187,Quaid Morris,"Central nervous system (CNS) dissemination of B-precursor acute lymphoblastic leukemia (B-ALL) has poor prognosis and remains a therapeutic challenge. Here we performed targeted DNA sequencing as well as transcriptional and proteomic profiling of paired leukemia-infiltrating cells in the bone marrow (BM) and CNS of xenografts. Genes governing mRNA translation were upregulated in CNS leukemia, and subclonal genetic profiling confirmed this in both BM-concordant and BM-discordant CNS mutational populations. CNS leukemia cells were exquisitely sensitive to the translation inhibitor omacetaxine mepesuccinate, which reduced xenograft leptomeningeal disease burden. Proteomics demonstrated greater abundance of secreted proteins in CNS-infiltrating cells, including complement component 3 (C3), and drug targeting of C3 influenced CNS disease in xenografts. ",Multiomic Profiling of Central Nervous System Leukemia Identifies mRNA Translation as a Therapeutic TargetBlocking Translation to Target B-ALL CNS Disease,2022-01-01 00:00:00
1188,Quaid Morris,"Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classifiers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufficiently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. ",Learning optimal predictive checklists,2021-12-06 00:00:00
1189,Quaid Morris,"Recent literature has highlighted the role of the host in prognosis in oral squamous cell carcinoma (OSCC). Autoimmune (AI) disease represents a macroscopic depiction of host status. The goal of this study was to predict an AI “status” and to analyze the utility of this “status” as a prognostic indicator in OSCC. From a departmental database of OSCC patients (n = 1377), 125 patients with an AI disorder were identified. PBL values were obtained and standardized for analysis. A LASSO regression model was used to determine the best predictors of AI status and an AI score was developed. The score was then analyzed across various survival",Peripheral blood values as predictors of autoimmune status in oral cavity squamous cell carcinoma,2021-12-01 00:00:00
1190,Quaid Morris,"Age-related clonal hematopoiesis (ARCH) is characterized by age-associated accumulation of somatic mutations in hematopoietic stem cells (HSCs) or their pluripotent descendants. HSCs harboring driver mutations will be positively selected and cells carrying these mutations will rise in frequency. While ARCH is a known risk factor for blood malignancies, such as Acute Myeloid Leukemia (AML), why some people who harbor ARCH driver mutations do not progress to AML remains unclear. Here, we model the interaction of positive and negative selection in deeply sequenced blood samples from individuals who subsequently progressed to AML, compared to healthy controls, using deep learning and population genetics. Our modeling allows us to discriminate amongst evolutionary classes with high accuracy and captures signatures of purifying selection in most individuals. Purifying selection, acting on benign or ",Interacting evolutionary pressures drive mutation dynamics and health outcomes in aging blood,2021-08-13 00:00:00
1191,Quaid Morris,clinical machine learning models experience significantly degraded performance in datasets not seen during training eg new hospitals or populations recent developments in domain generalization offer a promising solution to this problem by creating models that learn invariances across environments in this work we benchmark the performance of eight domain generalization methods on multisite clinical time series and medical imaging data we introduce a framework to induce synthetic but realistic domain shifts and sampling bias to stresstest these methods over existing nonhealthcare benchmarks we find that current domain generalization methods do not consistently achieve significant gains in outofdistribution performance over empirical risk minimization on realworld medical imaging data in line with prior work on general imaging datasets however a subset of realistic inducedshift scenarios in clinical time series data do exhibit limited performance gains we characterize these scenarios in detail and recommend best practices for domain generalization in the clinical setting less,An Empirical Framework for Domain Generalization in Clinical Settings,"14 April, 2021"
1192,Quaid Morris,the standard interpretation of importanceweighted autoencoders is that they maximize a tighter lower bound on the marginal likelihood than the standard evidence lower bound we give an alternate interpretation of this procedure that it optimizes the standard variational lower bound but using a more complex distribution we formally derive this result present a tighter lower bound and visualize the implicit importanceweighted distribution less,Reinterpreting Importance-Weighted Autoencoders,"14 August, 2017"
1193,Quaid Morris,statistical machine learning methods especially nonparametric bayesian methods have become increasingly popular to infer clonal population structure of tumors here we describe the treecrp an extension of the chinese restaurant process crp a popular construction used in nonparametric mixture models to infer the phylogeny and genotype of major subclonal lineages represented in the population of cancer cells we also propose new splitmerge updates tailored to the subclonal reconstruction problem that improve the mixing time of markov chains in comparisons with the treestructured stick breaking prior used in phylosub we demonstrate superior mixing and running time using the treecrp with our new splitmerge procedures we also show that given the same number of samples tssb and treecrp have similar ability to recover the subclonal structure of a tumor less,Comparing Nonparametric Bayesian Tree Priors for Clonal Reconstruction of Tumors,"11 August, 2014"
1194,Alan Moses,"A major challenge to the characterization of intrinsically disordered regions (IDRs), which are widespread in the proteome, but relatively poorly understood, is the identification of molecular features that mediate functions of these regions, such as short motifs, amino acid repeats and physicochemical properties. Here, we introduce a proteome-scale feature discovery approach for IDRs. Our approach, which we call “reverse homology”, exploits the principle that important functional features are conserved over evolution. We use this as a contrastive learning signal for deep learning: given a set of homologous IDRs, the neural network has to correctly choose a held-out homolog from another set of IDRs sampled randomly from the proteome. We pair reverse homology with a simple architecture and standard interpretation techniques, and show that the network learns conserved features of IDRs that can be interpreted as motifs, repeats, or bulk features like charge or amino acid propensities.",Discovering molecular features of intrinsically disordered regions by using evolution for contrastive learning,2022-06-29 00:00:00
1195,Alan Moses,"Although the quantity and quality of single‐cell data have progressed rapidly, making quantitative predictions with single‐cell stochastic models remains challenging. The stochastic nature of cellular processes leads to at least three challenges in building models with single‐cell data: (a) because variability in single‐cell data can be attributed to multiple different sources, it is difficult to rule out conflicting mechanistic models that explain the same data equally well; (b) the distinction between interesting biological variability and experimental variability is sometimes ambiguous; (c) the nonstandard distributions of single‐cell data can lead to violations of the assumption of symmetric errors in least‐squares fitting. In this review, we first discuss recent studies that overcome some of the challenges or set up a promising direction and then introduce some powerful statistical approaches utilized in these studies.",Stochastic models for single‐cell data: Current challenges and the way forward,2022/2
1196,Alan Moses,"Deep learning-based approaches to protein structure prediction, such as AlphaFold2 and RoseTTAFold, can now define many protein structures with atomic-level accuracy. The AlphaFold Protein Structure Database (AFDB) contains a predicted structure for nearly every protein in the human proteome, including proteins that have intrinsically disordered regions (IDRs), which do not adopt a stable structure and rapidly interconvert between conformations. Although it is generally assumed that IDRs have very low AlphaFold2 confidence scores that reflect low-confidence structural predictions, we show here that AlphaFold2 assigns confident structures to nearly 15% of human IDRs. The amino-acid sequences of IDRs with high-confidence structures do not show significant similarity to the Protein Data Bank; instead, these IDR sequences exhibit a higher degree of positional amino-acid sequence conservation and are more enriched in charged and hydrophobic residues than IDRs with low-confidence structures. ",Systematic identification of conditionally folded intrinsically disordered regions by AlphaFold2,2022-01-01 00:00:00
1197,Alan Moses,"Motivation: In recent years, image-based biological assays have steadily become high-throughput, sparking a need for fast automated methods to extract biologically-meaningful information from hundreds of thousands of images. Taking inspiration from the success of ImageNet, we curate CytoImageNet, a large-scale dataset of openly-sourced and weakly-labeled microscopy images (890K images, 894 classes). Pretraining on CytoImageNet yields features that are competitive to ImageNet features on downstream microscopy classification tasks. We show evidence that CytoImageNet features capture information not available in ImageNet-trained features. The dataset is made available at https://www.kaggle.com/stanleyhua/cytoimagenet.",CytoImageNet: A large-scale pretraining dataset for bioimage transfer learning,2021-11-23 00:00:00
1198,Alan Moses,"Stochastic signaling dynamics expand living cells’ information processing capabilities. An increasing number of studies report that regulators encode information in their pulsatile dynamics. The evolutionary mechanisms that lead to complex signaling dynamics remain uncharacterized, perhaps because key interactions of signaling proteins are encoded in intrinsically disordered regions (IDRs), whose evolution is difficult to analyze. Here we focused on the IDR that controls the stochastic pulsing dynamics of Crz1, a transcription factor in fungi downstream of the widely conserved calcium signaling pathway. We find that Crz1 IDRs from anciently diverged fungi can all respond transiently to calcium stress; however, only Crz1 IDRs from the Saccharomyces clade support pulsatility, encode extra information, and rescue fitness in competition assays, while the Crz1 IDRs from distantly related fungi do none of the three.",A functionally divergent intrinsically disordered region underlying the conservation of stochastic signaling,2021-09-10 00:00:00
1199,Alan Moses,"Large self-supervised models pretrained on millions of protein sequences have recently gained popularity in generating embeddings of protein sequences for protein function prediction. However, the absence of random baselines makes it difficult to conclude whether pretraining has learned useful information for protein function prediction. Here we show that one-hot encoding and random embeddings, both of which do not require any pretraining, are strong baselines for protein function prediction across 14 diverse sequence-to-function tasks.",Random Embeddings and Linear Regression can Predict Protein Function,2021-04-25 00:00:00
1200,Alan Moses,"Transcriptional enhancers are critical for development and phenotype evolution and are often mutated in disease contexts; however, even in well-studied cell types, the sequence code conferring enhancer activity remains unknown. To examine the enhancer regulatory code for pluripotent stem cells, we identified genomic regions with conserved binding of multiple transcription factors in mouse and human embryonic stem cells (ESCs). Examination of these regions revealed that they contain on average 12.6 conserved transcription factor binding site (TFBS) sequences. Enriched TFBSs are a diverse repertoire of 70 different sequences representing the binding sequences of both known and novel ESC regulators. ",A flexible repertoire of transcription factor binding sites and a diversity threshold determines enhancer activity in embryonic stem cells,2021-04-01 00:00:00
1201,Alan Moses,"In previous work, we showed that intrinsically disordered regions (IDRs) of proteins contain sequence-distributed molecular features that are conserved over evolution, despite little sequence similarity that can be detected in alignments (Zarin et al., 2019). Here, we aim to use these molecular features to predict specific biological functions for individual IDRs and identify the molecular features within them that are associated with these functions. We find that the predictable functions are diverse. Examining the associated molecular features, we note some that are consistent with previous reports and identify others that were previously unknown. We experimentally confirm that elevated isoelectric point and hydrophobicity, features that are positively associated with mitochondrial localization, are necessary for mitochondrial targeting function.",Identifying molecular features that are associated with biological function of intrinsically disordered protein regions,2021-02-22 00:00:00
1202,Alan Moses,"The precise regulation of gene expression is fundamental to neurodevelopment, plasticity, and cognitive function. While several studies have deeply profiled mRNA dynamics in the developing human brain, there is a fundamental gap in our understanding of accompanying translational regulation. We perform ribosome profiling from more than 70 human prenatal and adult cortex samples across ontogeny and into adulthood, mapping translation events at nucleotide resolution. In addition to characterizing the translational regulation of annotated open reading frames (ORFs), we identify thousands of previously unknown translation events, including small open reading frames (sORFs) that give rise to human- and/or brain-specific microproteins, many of which we independently verify using size-selected proteomics. Ribosome profiling in stem cell-derived human neuronal cultures further corroborates these findings and shows that several neuronal activity-induced long non-coding RNAs (lncRNAs), including LINC00473, a primate-specific lncRNA implicated in depression, encode previously undescribed ",Developmental Dynamics of RNA Translation in the Human Brain,2021-01-01 00:00:00
1203,Alan Moses,"We created Architect, a pipeline for automatic metabolic model reconstruction from protein sequences. First, it performs enzyme annotation through an ensemble approach, whereby a likelihood score is computed for an EC prediction based on predictions from existing tools; for this step, our method shows both increased precision and recall compared to individual tools. Next, Architect uses these annotations to construct a high-quality metabolic network which is then gap-filled based on likelihood scores from the ensemble approach. The resulting metabolic model is output in SBML format, suitable for constraints-based analyses. Through comparisons of enzyme annotations and curated metabolic models, we demonstrate improved performance of Architect over other state-of-the-art tools.",Architect: a tool for producing high-quality metabolic models through improved enzyme annotation,2021-01-01 00:00:00
1204,Alan Moses,"It is unclear how disease mutations impact intrinsically disordered protein regions (IDRs), which lack a stable folded structure. These mutations, while prevalent in disease, are frequently neglected or annotated as variants of unknown significance. Biomolecular phase separation, a physical process often mediated by IDRs, has increasingly appreciated roles in cellular organization and regulation. We find that autism spectrum disorder (ASD)- and cancer-associated proteins are enriched for predicted phase separation propensities, suggesting that IDR mutations disrupt phase separation in key cellular processes. More generally, we hypothesize that combinations of small-effect IDR mutations perturb phase separation, potentially contributing to “missing heritability” in complex disease susceptibility.",Phase separation as a missing mechanism for interpretation of disease mutations,2020-12-23 00:00:00
1205,Alan Moses,large selfsupervised models pretrained on millions of protein sequences have recently gained popularity in generating embeddings of protein sequences for protein function prediction however the absence of random baselines makes it difficult to conclude whether pretraining has learned useful information for protein function prediction here we show that onehot encoding and random embeddings both of which do not require any pretraining are strong baselines for protein function prediction across diverse sequencetofunction tasks less,Random Embeddings and Linear Regression can Predict Protein Function,"25 April, 2021"
1206,Alan Moses,ionization cooling is the preferred method for producing bright muon beams this cooling technique requires the operation of normal conducting radiofrequency rf accelerating cavities within the multitesla fields of dc solenoid magnets under these conditions cavities exhibit increased susceptibility to rf breakdown which can damage channel components and imposes limits on channel length and transmission efficiency we present a solution to the problem of breakdown in strong magnetic fields we report for the first time stable highvacuum copper cavity operation at gradients above mvm and in an external magnetic field of three tesla this eliminates a significant technical risk that has previously been inherent in ionization cooling channel designs less,Operation of normal-conducting RF cavities in multi-tesla magnetic fields for muon ionization cooling: a feasibility demonstration,"10 July, 2018"
1207,Cosmin Munteanu,"Speech and voice interaction is often hailed as a natural form of interaction and thus more inclusive for a larger portion of users. But, how accurate is this claim? In this panel, we challenge existing assumptions that voice and speech interaction is inclusive of diverse users. The goal of this panel is to bring together the broad HCI community to discuss the state of voice interaction for marginalized and vulnerable populations, how inclusive design is considered (or neglected) in current voice interaction design practice, and how to move forward when it comes to designing voice interaction for inclusion and diversity. In particular, we plan to center the discussion on older adults as a representative group of digitally-marginalized populations, especially given that voice interfaces are marketed towards this group, yet often fail to properly include this population in the design of such interfaces.","Alexa, Tell Me a Joke!:"" Voice Interfaces are Truly Inclusive""",2022-04-27 00:00:00
1208,Cosmin Munteanu,"Voice interaction has long been envisioned as enabling users to transform physical interaction into hands-free, such as allowing fine-grained control of instructional videos without physically disengaging from the task at hand. While significant engineering advances have brought us closer to this ideal, we do not fully understand the user requirements for voice interactions that should be supported in such contexts. This paper presents an ecologically-valid wizard-of-oz elicitation study exploring realistic user requirements for an ideal instructional video playback control while cooking. Through the analysis of the issued commands and performed actions during this non-linear and complex task, we identify (1) patterns of command formulation,(2) challenges for design, and (3) how task and voice-based commands are interwoven in real-life. We discuss implications for the design and research of voice interactions for ",“Rewind to the Jiggling Meat Part”: Understanding Voice Control of Instructional Videos in Everyday Tasks,2022-04-27 00:00:00
1209,Cosmin Munteanu,"Interactions with our personal and family pictures are essential to continued social reminiscence, leading to long-term benefits, including reduced social isolation. Previous research has identified how designs of digital picture tools fall short of physical options specifically in terms of reminiscence. However, the relative prompting abilities of different digital interactions, including the types of memories prompted like external facts or person-centred memories, have not yet been explored. To investigate this, we present a controlled study of the memories prompted by three digital picture interactions (slideshow, gallery, and tabletop) on personal touchscreen devices. We find differences in how these tools and the interactions they support prompt reminiscence. In particular, gallery views prompt significantly fewer memories than either the tabletop or slideshow. Slideshows prompt significantly more external, factual",Design is Worth a Thousand Words: The Effect of Digital Interaction Design on Picture-Prompted Reminiscence,2022-04-27 00:00:00
1210,Cosmin Munteanu,"Building on the prior workshops on conversational user interfaces (CUIs)[2, 40], we tackle the topic of ethics of CUIs at CHI 2022. Though commercial CUI developments continue to rapidly advance, our scholarly dialogue on ethics of CUIs is underwhelming. The CUI community has implicitly been concerned with ethics, yet making it central to the growing body of work thus far has not been adequately done. Since ethics is a far-reaching topic, perspectives from philosophy, design, and engineering domains are integral to our CUI research community. For instance, philosophical traditions, eg, deontology or virtue ethics, can guide ethical concepts that are relevant for CUIs, eg, autonomy or trust. The practice of design through approaches like value sensitive design can inform how CUIs should be developed. Ethics comes into play with technical contributions, eg, privacy-preserving data sharing between",Ethics of Conversational User Interfaces,2022-04-27 00:00:00
1211,Cosmin Munteanu,"Older adults frequently collaborate with their spouses in daily tasks and problem solving. Despite information seeking being an important aspect of collaboration, the information seeking behaviour of older adults and in particular couples remains under investigated. To address this gap, in this paper we present a qualitative investigation of older adults’ collaborative information seeking. Through in-depth interviews and demonstrations of real-life search tasks with eleven older couples, we show that older couples frequently engage in collaborative information seeking in daily tasks, interests, and to satisfy curiosity. Our research suggests that collaborative information seeking is a relationship maintenance behaviour among older couples, and that their long-term relationships may play a key role in how they communicate, make decisions, and develop divide and conquer strategies by taking on various roles during their ",Partners in life and online search: Investigating older couples’ collaborative information seeking,2022-03-14 00:00:00
1212,Cosmin Munteanu,"There is a growing interest to investigate the feasibility of using voice user interfaces as a platform for digital therapeutics in chronic disease management. While mostly deployed as smartphone applications, some demographics struggle when using touch screens and often cannot complete tasks independently. This research aimed to evaluate how heart failure patients interacted with a voice app version of an already existing digital therapeutic, Medly , using a mixed-methods concurrent triangulation approach. The objective was to determine the acceptability and feasibility of the voice app by better understanding who this platform is be best suited for. Quantitative data included engagement levels and accuracy rates. Participants (n=20) used the voice app over a four week period and completed questionnaires and semi-structured interviews relating to acceptability, ease of use, and workload. The average engagement level was 73%, with a 14% decline between week one and four. ",A Voice App Design for Heart Failure Self-Management: A Pilot Study,2022-01-01 00:00:00
1213,Cosmin Munteanu,"300,000 immigrants move to Canada each year in search of better economic opportunities, and many have limited English language skills. Improving written literacy of newcomers can enhance education, employment, or social integration opportunities. However, frequent, timely, and personalized feedback is not always possible for immigrants. Online writing support tools can scaffold writing development by providing this feedback, but existing systems provide inadequate support when instructors are inaccessible. In this paper, we show how feedback system design can leverage peer and automated feedback to support mature English Language Learners’ (ELL) needs and practices. We identify strong associations between epistemic beliefs and learning strategies, highlighting the importance of tasks that activate productive epistemic beliefs. We find learners accurately assessed high-level issues in a peer",Collaborating with mature English language learners to combine peer and automated feedback: A user-centered approach to designing writing support,2021/12
1214,Cosmin Munteanu,"Since 2016, the SIGCHI Research Ethics Committee has been in place to advise CSCW and other SIGCHI conferences and communities on ethical issues that arise in the course of our research. This town hall style panel provides an annual opportunity to connect with the committee, which has a remit to report back to the HCI research community on issues that arise as our methods, technologies, and best practices evolve.",SIGCHI Research Ethics Committee Town Hall,2021-10-23 00:00:00
1215,Cosmin Munteanu,"Hand-tracking allows users to engage with a virtual environment with their own hands, rather than the more traditional method of using accompanying controllers in order to operate the device they are using and interact with the virtual world. We seek to explore the range of low-level interaction actions and high-level interaction tasks and domains can be associated with the multimodal hand-tracking and voice input in VR. Thus, we created Let's Go There, which explores this joint-input method. So far, we have identified four low-level interaction actions which are exemplified by this demo: positioning oneself, positioning others, selection, and information assignment. We anticipate potential high-level interaction tasks and domains to include customer service training, social skills training, and cultural competency training (eg when interacting with older adults). Let's Go There, the system described in this paper, had ",Low-level Voice and Hand-Tracking Interaction Actions: Explorations with Let's Go There,2021-09-27 00:00:00
1216,Cosmin Munteanu,"We are underusing chatbots. Mainly, we seem to employ chatbots to a degree of use below this technology's current potential, both in its engineering capabilities and in terms of application areas. This may be due to our envisioned use of chatbots as replacing humans in a variety of service-oriented conversational tasks. Yet, even within this context of use, the decision to implement chatbots may be driven by financial or economic arguments, and their use is fairly conservative. In this provocation paper, we are arguing for a less conventional use of chatbots–that of intelligence-gathering agents operating on behalf of law enforcement on the dark web. This proposed use challenges both the current accepted uses of chatbots and their utilization that is not pushing the boundaries of technological capabilities. We discuss how chatbots may be used on the dark web and what sociotechnical challenges that may pose ",Towards a chatbot for evidence gathering on the dark web,2021-07-27 00:00:00
1217,Cosmin Munteanu,"As Voice User Interfaces (VUIs) become widely popular, designers must handle new usability challenges. However, compared to other established domains such as Graphical User Interfaces (GUIs), VUI designers have fewer resources (training support, usability heuristics, design patterns) to guide them. On the other hand, GUI-trained designers may also be solicited upon to design VUIs given the increased demand for such interfaces. This raises the question: how can we best support such designers as they transition from GUI to VUI design? To answer this, we focus on usability heuristics as a key resource, and conduct several workshops with GUI design experts, exploring how they map their design experience onto VUI design. Based on this, we suggest that the “path of least resistance” to transitioning designers from GUI to VUI may be the adaptation of familiar resources and concepts (such as GUI heuristics) to ",Finding a New Voice: Transitioning Designers from GUI to VUI Design,2021-07-27 00:00:00
1218,Cosmin Munteanu,"Voice User Interfaces (VUIs) such as smart speakers hold promise for older adults (OAs) in terms of usability and convenience. However, their adoption and the extent of their benefits to OAs may be influenced by mass media, as this is a primary source of technology education for OAs. Thus, we aim to obtain a better understanding of how VUIs’ value and utility for OAs are portrayed in the media. We conducted a systematic review and thematic analysis of articles published in ten popular digital news outlets that focus on VUIs and older adults. The analysis reveals several design and engineering factors that are portrayed in media as being relevant or encouraging to older adults’ adoption of VUIs. Given the media's influence of the consumer adoption of new technologies, this analysis brings to light several sociotechnical aspects that are dominant threads within the media discourse related to VUIs.",VUI Influencers: How the Media Portrays Voice User Interfaces for Older Adults,2021-07-27 00:00:00
1219,Cosmin Munteanu,"As CUIs become more prevalent in both academic research and the commercial market, it becomes more essential to design usable and adoptable CUIs. Though research on the usability and design of CUIs has been growing greatly over the past decade, we see that many usability issues are still prevalent in current conversational voice interfaces, from issues in feedback and visibility, to learnability, to error correction, and more. These issues still exist in the most current conversational interfaces in the commercial market, like the Google Assistant, Amazon Alexa, and Siri. The aim of this workshop therefore is to bring both academics and industry practitioners together to bridge the gaps of knowledge in regards to the tools, practices, and methods used in the design of CUIs. This workshop will bring together both the research performed by academics in the field, and the practical experience and needs from industry",Let’s Talk About CUIs: Putting Conversational User Interface Design Into Practice,2021-05-08 00:00:00
1220,Cosmin Munteanu,"HCI research has for long been dedicated to better and more naturally facilitating information transfer between humans and machines. Unfortunately, humans' most natural form of communication, speech, is also one of the most difficult modalities to be understood by machines–despite, and perhaps, because it is the highest-bandwidth communication channel we possess. While significant research efforts, from engineering, to linguistic, and to cognitive sciences, have been spent on improving machines' ability to understand speech, the CHI community (and the HCI field at large) has only recently started embracing this modality as a central focus of research. This can be attributed in part to the unexpected variations in error rates when processing speech, in contrast with often-unfounded claims of success from industry, but also to the intrinsic difficulty of designing and especially evaluating speech and natural ",Conversational Voice User Interfaces: Connecting Engineering Fundamentals to Design Considerations,2021-05-08 00:00:00
1221,Cosmin Munteanu,conversational agents promise conversational interaction but fail to deliver efforts often emulate functional rules from human speech without considering key characteristics that conversation must encapsulate given its potential in supporting longterm humanagent relationships it is paramount that hci focuses efforts on delivering this promise we aim to understand what people value in conversation and how this should manifest in agents findings from a series of semistructured interviews show people make a clear dichotomy between social and functional roles of conversation emphasising the longterm dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have people fundamentally questioned the need for bond and common ground in agent communication shifting to more utilitarian definitions of conversational qualities drawing on these findings we discuss key challenges for conversational agent design most notably the need to redefine the design parameters for conversational agent interaction less,What Makes a Good Conversation? Challenges in Designing Truly Conversational Agents,"19 January, 2019"
1222,Radford Neal,"I show how it can be beneficial to express Metropolis accept/reject decisions in terms of comparison with a uniform [0,1] value, u, and to then update u non-reversibly, as part of the Markov chain state, rather than sampling it independently each iteration. This provides a small improvement for random walk Metropolis and Langevin updates in high dimensions. It produces a larger improvement when using Langevin updates with persistent momentum, giving performance comparable to that of Hamiltonian Monte Carlo (HMC) with long trajectories. This is of significance when some variables are updated by other methods, since if HMC is used, these updates can be done only between trajectories, whereas they can be done more often with Langevin updates. I demonstrate that for a problem with some continuous variables, updated by HMC or Langevin updates, and also discrete variables, updated by Gibbs sampling between updates of the continuous variables, Langevin with persistent momentum and non-reversible updates to u samples nearly a factor of two more efficiently than HMC. Benefits are also seen for a Bayesian neural network model in which hyperparameters are updated by Gibbs sampling.","Non-reversibly updating a uniform [0, 1] value for Metropolis accept/reject decisions",2020-01-31 00:00:00
1223,Radford Neal,"We propose a new scheme for selecting pool states for the embedded Hidden Markov Model (HMM) Markov Chain Monte Carlo (MCMC) method. This new scheme allows the embedded HMM method to be used for efficient sampling in state space models where the state can be high-dimensional. Previously, embedded HMM methods were only applicable to low-dimensional state-space models. We demonstrate that using our proposed pool state selection scheme, an embedded HMM sampler can have similar performance to a well-tuned sampler that uses a combination of Particle Gibbs with Backward Sampling (PGBS) and Metropolis updates. The scaling to higher dimensions is made possible by selecting pool states locally near the current value of the state sequence. The proposed pool state selection scheme also allows each iteration of the embedded HMM sampler to take time linear in the number of the ",Sampling latent states for high-dimensional non-linear state space models with the embedded HMM method,2018/9
1224,Radford Neal,"I show how to run an N-time-step Markov chain simulation in a circular fashion, so that the state at time 0 follows the state at time N-1 in the same way as states at times t follow those at times t-1 for 0<t<N. This wrap-around of the chain is achieved using a coupling procedure, and produces states that all have close to the equilibrium distribution of the Markov chain, under the assumption that coupled chains are likely to coalesce in less than N/2 iterations. This procedure therefore automatically eliminates the initial portion of the chain that would otherwise need to be discarded to get good estimates of equilibrium averages. The assumption of rapid coalescence can be tested using auxiliary chains started at times spaced between 0 and N. When multiple processors are available, such auxiliary chains can be simulated in parallel, and pieced together to give the circularly-coupled chain, in less time than a sequential simulation would have taken, provided that coalescence is indeed rapid. ",Circularly-coupled Markov chain sampling,2017-11-13 00:00:00
1225,Radford Neal,"I present two new methods for exactly summing a set of floating-point numbers, and then correctly rounding to the nearest floating-point number. Higher accuracy than simple summation (rounding after each addition) is important in many applications, such as finding the sample mean of data. Exact summation also guarantees identical results with parallel and serial implementations, since the exact sum is independent of order. The new methods use variations on the concept of a ""superaccumulator"" - a large fixed-point number that can exactly represent the sum of any reasonable number of floating-point values. One method uses a ""small"" superaccumulator with sixty-seven 64-bit chunks, each with 32-bit overlap with the next chunk, allowing carry propagation to be done infrequently. The small superaccumulator is used alone when summing a small number of terms. ",Fast exact summation using small and large superaccumulators,2015-05-21 00:00:00
1226,Radford Neal,"Data files often consist of numbers having only a few significant decimal digits, whose information content would allow storage in only 32 bits. However, we may require that arithmetic operations involving these numbers be done with 64-bit floating-point precision, which precludes simply representing the data as 32-bit floating-point values. Decimal floating point gives a compact and exact representation, but requires conversion with a slow division operation before it can be used. Here, I show that interesting subsets of 64-bit floating-point values can be compactly and exactly represented by the 32 bits consisting of the sign, exponent, and high-order part of the mantissa, with the lower-order 32 bits of the mantissa filled in by table lookup, indexed by bits from the part of the mantissa retained, and possibly from the exponent. For example, decimal data with 4 or fewer digits to the left of the decimal point and 2 or fewer digits to the right of the decimal point can be represented in this way using the lower-order 5 bits of the retained part of the mantissa as the index.",Representing numeric data in 32 bits while preserving 64-bit precision,2015-04-11 00:00:00
1227,Radford Neal,"Gaussian Process (GP) models are a powerful and flexible tool for non-parametric regression and classification. Computation for GP models is intensive, since computing the posterior density, , for covariance function parameters requires computation of the covariance matrix, C, a  operation, where p is the number of covariates and n is the number of training cases, and then inversion of C, an  operation. We introduce MCMC methods based on the ""temporary mapping and caching"" framework, using a fast approximation, , as the distribution needed to construct the temporary space. We propose two implementations under this scheme: ""mapping to a discretizing chain"", and ""mapping with tempered transitions"", both of which are exactly correct MCMC methods for sampling , even though their transitions are constructed using an approximation. ",MCMC methods for Gaussian process models using fast approximations for the likelihood,2013-05-10 00:00:00
1228,Radford Neal,"Consider a Markov chain defined on a finite state space, X, that leaves invariant the uniform distribution on X, and whose transition probabilities are integer multiples of 1/Q, for some integer Q. I show how a simulation of n transitions of this chain starting at x_0 can be viewed as applying a random permutation on the space XxU, where U={0,1,...,Q-1}, to the start state (x_0,u_0), with u_0 drawn uniformly from U. This result can be applied to a non-uniform distribution with probabilities that are integer multiples of 1/P, for some integer P, by representing it as the marginal distribution for X from the uniform distribution on a suitably-defined subset of XxY, where Y={0,1,...,P-1}. By letting Q, P, and the cardinality of X go to infinity, this result can be generalized to non-rational probabilities and to continuous state spaces, with permutations on a finite space replaced by volume-preserving one-to-one maps from a continuous space to itself. These constructions can be efficiently implemented for chains commonly used in Markov chain Monte Carlo (MCMC) simulations. ","How to view an MCMC simulation as a permutation, with applications to parallel simulation and improved importance sampling",2012-05-01 00:00:00
1229,Radford Neal,"Past deglacial ice sheet reconstructions have generally relied upon discipline-specific constraints with no attention given to the determination of objective confidence intervals. Reconstructions based on geophysical inversion of relative sea level (RSL) data have the advantage of large sets of proxy data but lack ice-mechanical constraints. Conversely, reconstructions based on dynamical ice sheet models are glaciologically self-consistent, but depend on poorly constrained climate forcings and sub-glacial processes.",A data-calibrated distribution of deglacial chronologies for the North American ice complex from glaciological modeling,2012-01-15 00:00:00
1230,Radford Neal,bayesian classification and regression with high order interactions is largely infeasible because markov chain monte carlo mcmc would need to be applied with a great many parameters whose number increases rapidly with the order in this paper we show how to make it feasible by effectively reducing the number of parameters exploiting the fact that many interactions have the same values for all training cases our method uses a single compressed parameter to represent the sum of all parameters associated with a set of patterns that have the same value for all training cases using symmetric stable distributions as the priors of the original parameters we can easily find the priors of these compressed parameters we therefore need to deal only with a much smaller number of compressed parameters when training the model with mcmc the number of compressed parameters may have converged before considering the highest possible order after training the model we can split these compressed parameters into the original ones as needed to make predictions for test cases we show in detail how to compress parameters for logistic sequence prediction models experiments on both simulated and real data demonstrate that a huge number of parameters can indeed be reduced by our compression method less,A Method for Compressing Parameters in Bayesian Models with Application to Logistic Sequence Prediction Models,"30 November, 2007"
1231,Radford Neal,i describe a new markov chain method for sampling from the distribution of the state sequences in a nonlinear state space model given the observation sequence this method updates all states in the sequence simultaneously using an embedded hidden markov model hmm an update begins with the creation of a pool of k states at each time by applying some markov chain update to the current state these pools define an embedded hmm whose states are indexes within this pool using the forwardbackward dynamic programming algorithm we can then efficiently choose a state sequence at random with the appropriate probabilities from the exponentially large number of state sequences that pass through states in these pools i show empirically that when states at nearby times are strongly dependent embedded hmm sampling can perform better than metropolis methods that update one state at a time less,Markov Chain Sampling for Non-linear State Space Models Using Embedded Hidden Markov Models,"1 May, 2003"
1232,Nicolas Papernot,neural networks are susceptible to adversarial examplessmall input perturbations that cause models to fail adversarial training is one of the solutions that stops adversarial examples models are exposed to attacks during training and learn to be resilient to them yet such a procedure is currently expensiveit takes a long time to produce and train models with adversarial samples and what is worse it occasionally fails in this paper we demonstrate data pruninga method for increasing adversarial training efficiency through data subsamplingwe empirically show that data pruning leads to improvements in convergence and reliability of adversarial training albeit with different levels of utility degradation for example we observe that using random subsampling of cifar to drop of data we lose adversarial accuracy against the strongest attackers while by using only of data we lose adversarial accuracy and reduce runtime by a factor of interestingly we discover that in some settings data pruning brings benefits from both worldsit both improves adversarial accuracy and training time less,Efficient Adversarial Training With Data Pruning,"1 July, 2022"
1233,Nicolas Papernot,selfsupervised learning ssl is an increasingly popular ml paradigm that trains models to transform complex inputs into representations without relying on explicit labels these representations encode similarity structures that enable efficient learning of multiple downstream tasks recently mlasaservice providers have commenced offering trained ssl models over inference apis which transform user inputs into useful representations for a fee however the high cost involved to train these models and their exposure over apis both make blackbox extraction a realistic security threat we thus explore model stealing attacks against ssl unlike traditional model extraction on classifiers that output labels the victim models here output representations these representations are of significantly higher dimensionality compared to the lowdimensional prediction scores output by classifiers we construct several novel attacks and find that approaches that train directly on a victims stolen representations are query efficient and enable high accuracy for downstream models we then show that existing defenses against model extraction are inadequate and not easily retrofitted to the specificities of ssl less,On the Difficulty of Defending Self-Supervised Learning against Model Extraction,"29 June, 2022"
1234,Nicolas Papernot,machine learning models trained on private datasets have been shown to leak their private data while recent work has found that the average data point is rarely leaked the outlier samples are frequently subject to memorization and consequently privacy leakage we demonstrate and analyse an onion effect of memorization removing the layer of outlier points that are most vulnerable to a privacy attack exposes a new layer of previouslysafe points to the same attack we perform several experiments to study this effect and understand why it occurs the existence of this effect has various consequences for example it suggests that proposals to defend against memorization without training with rigorous privacy guarantees are unlikely to be effective further it suggests that privacyenhancing technologies such as machine unlearning could actually harm the privacy of other users less,The Privacy Onion Effect: Memorization is Relative,"22 June, 2022"
1235,Nicolas Papernot,machine learning is vulnerable to adversarial manipulation previous literature has demonstrated that at the training stage attackers can manipulate data and data sampling procedures to control model behaviour a common attack goal is to plant backdoors ie force the victim model to learn to recognise a trigger known only by the adversary in this paper we introduce a new class of backdoor attacks that hide inside model architectures ie in the inductive bias of the functions used to train these backdoors are simple to implement for instance by publishing opensource code for a backdoored model architecture that others will reuse unknowingly we demonstrate that model architectural backdoors represent a real threat and unlike other approaches can survive a complete retraining from scratch we formalise the main construction principles behind architectural backdoors such as a link between the input and the output and describe some possible protections against them we evaluate our attacks on computer vision benchmarks of different scales and demonstrate the underlying vulnerability is pervasive in a variety of training settings less,Architectural Backdoors in Neural Networks,"15 June, 2022"
1236,Nicolas Papernot,selective classification is the task of rejecting inputs a model would predict incorrectly on through a tradeoff between input space coverage and model accuracy current methods for selective classification impose constraints on either the model architecture or the loss function this inhibits their usage in practice in contrast to prior work we show that stateoftheart selective classification performance can be attained solely from studying the discretized training dynamics of a model we propose a general framework that for a given test input monitors metrics capturing the disagreement with the final predicted label over intermediate models obtained during training we then reject data points exhibiting too much disagreement at late stages in training in particular we instantiate a method that tracks when the label predicted during training stops disagreeing with the final predicted label our experimental evaluation shows that our method achieves stateoftheart accuracycoverage tradeoffs on typical selective classification benchmarks for example we improve coverage on cifarsvhn by respectively at a fixed target error of less,Selective Classification Via Neural Network Training Dynamics,"26 May, 2022"
1237,Nicolas Papernot,for many differentially private algorithms such as the prominent noisy stochastic gradient descent dpsgd the analysis needed to bound the privacy leakage of a single training run is well understood however few studies have reasoned about the privacy leakage resulting from the multiple training runs needed to fine tune the value of the training algorithms hyperparameters in this work we first illustrate how simply setting hyperparameters based on nonprivate training runs can leak private information motivated by this observation we then provide privacy guarantees for hyperparameter search procedures within the framework of renyi differential privacy our results improve and extend the work of liu and talwar stoc our analysis supports our previous observation that tuning hyperparameters does indeed leak private information but we prove that under certain assumptions this leakage is modest as long as each candidate training run needed to select hyperparameters is itself differentially private less,Hyperparameter Tuning with Renyi Differential Privacy,"14 March, 2022"
1238,Nicolas Papernot,sharing realworld speech utterances is key to the training and deployment of voicebased services however it also raises privacy risks as speech contains a wealth of personal data speaker anonymization aims to remove speaker information from a speech utterance while leaving its linguistic and prosodic attributes intact stateoftheart techniques operate by disentangling the speaker information represented via a speaker embedding from these attributes and resynthesizing speech based on the speaker embedding of another speaker prior research in the privacy community has shown that anonymization often provides brittle privacy protection even less so any provable guarantee in this work we show that disentanglement is indeed not perfect linguistic and prosodic attributes still contain speaker information we remove speaker information from these attributes by introducing differentially private feature extractors based on an autoencoder and an automatic speech recognizer respectively trained using noise layers we plug these extractors in the stateoftheart anonymization pipeline and generate for the first time differentially private utterances with a provable upper bound on the speaker information they contain we evaluate empirically the privacy and utility resulting from our differentially private speaker anonymization approach on the librispeech data set experimental results show that the generated utterances retain very high utility for automatic speech recognition training and inference while being much better protected against strong adversaries who leverage the full knowledge of the anonymization process to try to infer the speaker identity less,Differentially Private Speaker Anonymization,"23 February, 2022"
1239,Nicolas Papernot,machine unlearning ie having a model forget about some of its training data has become increasingly more important as privacy legislation promotes variants of the righttobeforgotten in the context of deep learning approaches for machine unlearning are broadly categorized into two classes exact unlearning methods where an entity has formally removed the data points impact on the model by retraining the model from scratch and approximate unlearning where an entity approximates the model parameters one would obtain by exact unlearning to save on compute costs in this paper we first show that the definition that underlies approximate unlearning which seeks to prove the approximately unlearned model is close to an exactly retrained model is incorrect because one can obtain the same model using different datasets thus one could unlearn without modifying the model at all we then turn to exact unlearning approaches and ask how to verify their claims of unlearning our results show that even for a given training trajectory one cannot formally prove the absence of certain data points used during training we thus conclude that unlearning is only welldefined at the algorithmic level where an entitys only possible auditable claim to unlearning is that they used a particular algorithm designed to allow for external scrutiny during an audit less,On the Necessity of Auditable Algorithmic Definitions for Machine Unlearning,"19 February, 2022"
1240,Nicolas Papernot,in model extraction attacks adversaries can steal a machine learning model exposed via a public api by repeatedly querying it and adjusting their own model based on obtained predictions to prevent model stealing existing defenses focus on detecting malicious queries truncating or distorting outputs thus necessarily introducing a tradeoff between robustness and model utility for legitimate users instead we propose to impede model extraction by requiring users to complete a proofofwork before they can read the models predictions this deters attackers by greatly increasing even up to x the computational effort needed to leverage query access for model extraction since we calibrate the effort required to complete the proofofwork to each query this only introduces a slight overhead for regular users up to x to achieve this our calibration applies tools from differential privacy to measure the information revealed by a query our method requires no modification of the victim model and can be applied by machine learning practitioners to guard their publicly exposed models against being easily stolen less,Increasing the Cost of Model Extraction with Calibrated Proof of Work,"23 January, 2022"
1241,Nicolas Papernot,in federated learning fl data does not leave personal devices when they are jointly training a machine learning model instead these devices share gradients with a central party eg a company because data never leaves personal devices fl is presented as privacypreserving yet recently it was shown that this protection is but a thin facade as even a passive attacker observing gradients can reconstruct data of individual users in this paper we argue that prior work still largely underestimates the vulnerability of fl this is because prior efforts exclusively consider passive attackers that are honestbutcurious instead we introduce an active and dishonest attacker acting as the central party who is able to modify the shared models weights before users compute model gradients we call the modified weights trap weights our active attacker is able to recover user data perfectly and at near zero costs the attack requires no complex optimization objectives instead it exploits inherent data leakage from model gradients and amplifies this effect by maliciously altering the weights of the shared model these specificities enable our attack to scale to models trained with large minibatches of data where attackers from prior work require hours to recover a single data point our method needs milliseconds to capture the full minibatch of data from both fullyconnected and convolutional deep neural networks finally we consider mitigations we observe that current implementations of differential privacy dp in fl are flawed as they explicitly trust the central party with the crucial task of adding dp noise and thus provide no protection against a malicious central party we also consider other defenses and explain why they are similarly inadequate a significant redesign of fl is required for it to provide any meaningful form of data privacy to users less,When the Curious Abandon Honesty: Federated Learning Is Not Private,"6 December, 2021"
1242,Nicolas Papernot,sophisticated machine learning ml models to inform trading in the financial sector create problems of interpretability and risk management seemingly robust forecasting models may behave erroneously in out of distribution settings in some of the worlds most sophisticated quant hedge funds suffered losses as their ml models were first underhedged and then overcompensated we implement a gradientbased approach for precisely stresstesting how a trading models forecasts can be manipulated and their effects on downstream tasks at the trading execution level we construct inputs whether in changes to sentiment or market variables that efficiently affect changes in the return distribution in an industrystandard trading pipeline we perturb model inputs for eight sp stocks we find our approach discovers seemingly insample input settings that result in large negative shifts in return distributions we provide the financial community with mechanisms to interpret ml forecasts in trading systems for the security community we provide a compelling application where studying ml robustness necessitates that one capture an endtoend systems performance rather than study a ml model in isolation indeed we show in our evaluation that errors in the forecasting models predictions alone are not sufficient for trading decisions made based on these forecasts to yield a negative return less,Interpretability in Safety-Critical FinancialTrading Systems,"24 September, 2021"
1243,Nicolas Papernot,machine learning ml models are known to be vulnerable to adversarial examples applications of ml to voice biometrics authentication are no exception yet the implications of audio adversarial examples on these realworld systems remain poorly understood given that most research targets limited defenders who can only listen to the audio samples conflating detectability of an attack with human perceptibility research has focused on methods that aim to produce imperceptible adversarial examples which humans cannot distinguish from the corresponding benign samples we argue that this perspective is coarse for two reasons imperceptibility is impossible to verify it would require an experimental process that encompasses variations in listener training equipment volume ear sensitivity types of background noise etc and it disregards pipelinebased detection clues that realistic defenders leverage this results in adversarial examples that are ineffective in the presence of knowledgeable defenders thus an adversary only needs an audio sample to be plausible to a human we thus introduce surreptitious adversarial examples a new class of attacks that evades both human and pipeline controls in the whitebox setting we instantiate this class with a joint multistage optimization attack using an amazon mechanical turk user study we show that this attack produces audio samples that are more surreptitious than previous attacks that aim solely for imperceptibility lastly we show that surreptitious adversarial examples are challenging to develop in the blackbox setting less,On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples,"3 August, 2021"
1244,Nicolas Papernot,inpainting is a learned interpolation technique that is based on generative modeling and used to populate masked or missing pieces in an image it has wide applications in picture editing and retouching recently inpainting started being used for watermark removal raising concerns in this paper we study how to manipulate it using our markpainting technique first we show how an image owner with access to an inpainting model can augment their image in such a way that any attempt to edit it using that model will add arbitrary visible information we find that we can target multiple different models simultaneously with our technique this can be designed to reconstitute a watermark if the editor had been trying to remove it second we show that our markpainting technique is transferable to models that have different architectures or were trained on different datasets so watermarks created using it are difficult for adversaries to remove markpainting is novel and can be used as a manipulation alarm that becomes visible in the event of inpainting less,Markpainting: Adversarial Machine Learning meets Inpainting,"1 June, 2021"
1245,Nicolas Papernot,with increasingly more data and computation involved in their training machine learning models constitute valuable intellectual property this has spurred interest in model stealing which is made more practical by advances in learning with partial little or no supervision existing defenses focus on inserting unique watermarks in a models decision surface but this is insufficient the watermarks are not sampled from the training distribution and thus are not always preserved during model stealing in this paper we make the key observation that knowledge contained in the stolen models training set is what is common to all stolen copies the adversarys goal irrespective of the attack employed is always to extract this knowledge or its byproducts this gives the original models owner a strong advantage over the adversary model owners have access to the original training data we thus introduce dataset inference the process of identifying whether a suspected model copy has private knowledge from the original models dataset as a defense against model stealing we develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary our experiments on cifar svhn cifar and imagenet show that model owners can claim with confidence greater than that their model or dataset as a matter of fact was stolen despite only exposing of the stolen models training points dataset inference defends against stateoftheart attacks even when the adversary is adaptive unlike prior work it does not require retraining or overfitting the defended model less,Dataset Inference: Ownership Resolution in Machine Learning,"21 April, 2021"
1246,Nicolas Papernot,machine learning benefits from large training datasets which may not always be possible to collect by any single entity especially when using privacysensitive data in many contexts such as healthcare and finance separate parties may wish to collaborate and learn from each others data but are prevented from doing so due to privacy regulations some regulations prevent explicit sharing of data between parties by joining datasets in a central location confidentiality others also limit implicit sharing of data eg through model predictions privacy there is currently no method that enables machine learning in such a setting where both confidentiality and privacy need to be preserved to prevent both explicit and implicit sharing of data federated learning only provides confidentiality not privacy since gradients shared still contain private information differentially private learning assumes unreasonably large datasets furthermore both of these learning paradigms produce a central model whose architecture was previously agreed upon by all parties rather than enabling collaborative learning where each party learns and improves their own local model we introduce confidential and private collaborative capc learning the first method provably achieving both confidentiality and privacy in a collaborative setting we leverage secure multiparty computation mpc homomorphic encryption he and other techniques in combination with privately aggregated teacher models we demonstrate how capc allows participants to collaborate without having to explicitly join their training sets or train a central model each party is able to improve the accuracy and fairness of their model even in settings where each party has a model that performs well on their own dataset or when datasets are not iid and model architectures are heterogeneous across parties less,CaPC Learning: Confidential and Private Collaborative Learning,"19 March, 2021"
1247,Nicolas Papernot,progress in generative modelling especially generative adversarial networks have made it possible to efficiently synthesize and alter media at scale malicious individuals now rely on these machinegenerated media or deepfakes to manipulate social discourse in order to ensure media authenticity existing research is focused on deepfake detection yet the adversarial nature of frameworks used for generative modeling suggests that progress towards detecting deepfakes will enable more realistic deepfake generation therefore it comes at no surprise that developers of generative models are under the scrutiny of stakeholders dealing with misinformation campaigns at the same time generative models have a lot of positive applications as such there is a clear need to develop tools that ensure the transparent use of generative modeling while minimizing the harm caused by malicious applications our technique optimizes over the source of entropy of each generative model to probabilistically attribute a deepfake to one of the models we evaluate our method on the seminal example of face synthesis demonstrating that our approach achieves attribution accuracy and is less sensitive to perturbations and adversarial examples we discuss the ethical implications of our work identify where our technique can be used and highlight that a more meaningful legislative framework is required for a more transparent and ethical use of generative modeling finally we argue that model developers should be capable of claiming plausible deniability and propose a second framework to do so this allows a model developer to produce evidence that they did not produce media that they are being accused of having produced less,On Attribution of Deepfakes,"3 March, 2021"
1248,Nicolas Papernot,adversarial training is a common approach to improving the robustness of deep neural networks against adversarial examples in this work we propose a novel regularization approach as an alternative to derive the regularizer we formulate the adversarial robustness problem under the robust optimization framework and approximate the loss function using a secondorder taylor series expansion our proposed secondorder adversarial regularizer soar is an upper bound based on the taylor approximation of the innermax in the robust optimization objective we empirically show that the proposed method significantly improves the robustness of networks against the ellinfty and ell bounded perturbations generated using crossentropybased pgd on cifar and svhn less,SOAR: Second-Order Adversarial Regularization,"7 February, 2021"
1249,Nicolas Papernot,once users have shared their data online it is generally difficult for them to revoke access and ask for the data to be deleted machine learning ml exacerbates this problem because any model trained with said data may have memorized it putting users at risk of a successful privacy attack exposing their information yet having models unlearn is notoriously difficult we introduce sisa training a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure while our framework is applicable to any learning algorithm it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks sisa training reduces the computational overhead associated with unlearning even in the worstcase setting where unlearning requests are made uniformly across the training set in some cases the service provider may have a prior on the distribution of unlearning requests that will be issued by users we may take this prior into account to partition and order data accordingly and further decrease overhead from unlearning our evaluation spans several datasets from different domains with corresponding motivations for unlearning under no distributional assumptions for simple learning tasks we observe that sisa training improves time to unlearn points from the purchase dataset by x and x for the svhn dataset over retraining from scratch sisa training also provides a speedup of x in retraining for complex learning tasks such as imagenet classification aided by transfer learning this results in a small degradation in accuracy our work contributes to practical data governance in machine unlearning less,Machine Unlearning,"15 December, 2020"
1250,Nicolas Papernot,machine learning models in health care are often deployed in settings where it is important to protect patient privacy in such settings methods for differentially private dp learning provide a generalpurpose approach to learn models with privacy guarantees modern methods for dp learning ensure privacy through mechanisms that censor information judged as too unique the resulting privacypreserving models therefore neglect information from the tails of a data distribution resulting in a loss of accuracy that can disproportionately affect small groups in this paper we study the effects of dp learning in health care we use stateoftheart methods for dp learning to train privacypreserving models in clinical prediction tasks including xray classification of images and mortality prediction in time series data we use these models to perform a comprehensive empirical investigation of the tradeoffs between privacy utility robustness to dataset shift and fairness our results highlight lesserknown limitations of methods for dp learning in health care models that exhibit steep tradeoffs between privacy and utility and models whose predictions are disproportionately influenced by large demographic groups in the training data we discuss the costs and benefits of differentially private learning in health care less,Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings,"13 October, 2020"
1251,Nicolas Papernot,adversarial examples are malicious inputs crafted to induce misclassification commonly studied sensitivitybased adversarial examples introduce semanticallysmall changes to an input that result in a different model prediction this paper studies a complementary failure mode invariancebased adversarial examples that introduce minimal semantic changes that modify an inputs true label yet preserve the models prediction we demonstrate fundamental tradeoffs between these two types of adversarial examples we show that defenses against sensitivitybased attacks actively harm a models accuracy on invariancebased attacks and that new approaches are needed to resist both attack types in particular we break stateoftheart adversariallytrained and certifiablyrobust models by generating small perturbations that the models are provably robust to yet that change an inputs class according to human labelers finally we formally show that the existence of excessively invariant classifiers arises from the presence of overlyrobust predictive features in standard datasets less,Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations,"4 August, 2020"
1252,Nicolas Papernot,speech and speaker recognition systems are employed in a variety of applications from personal assistants to telephony surveillance and biometric authentication the wide deployment of these systems has been made possible by the improved accuracy in neural networks like other systems based on neural networks recent research has demonstrated that speech and speaker recognition systems are vulnerable to attacks using manipulated inputs however as we demonstrate in this paper the endtoend architecture of speech and speaker systems and the nature of their inputs make attacks and defenses against them substantially different than those in the image space we demonstrate this first by systematizing existing research in this space and providing a taxonomy through which the community can evaluate future work we then demonstrate experimentally that attacks against these models almost universally fail to transfer in so doing we argue that substantial additional work is required to provide adequate mitigations in this space less,SoK: The Faults in our ASRs: An Overview of Attacks against Automatic Speech Recognition and Speaker Identification Systems,"21 July, 2020"
1253,Nicolas Papernot,in cooperative multiagent reinforcement learning cmarl agents learn to cooperatively take actions as a team to maximize a total team reward we analyze the robustness of cmarl to adversaries capable of attacking one of the agents on a team through the ability to manipulate this agents observations the adversary seeks to decrease the total team reward attacking cmarl is challenging for three reasons first it is difficult to estimate team rewards or how they are impacted by an agent mispredicting second models are nondifferentiable and third the feature space is lowdimensional thus we introduce a novel attack the attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take then the adversary uses targeted adversarial examples to force the victim to take this action our results on the startcraft ii multiagent benchmark demonstrate that cmarl teams are highly vulnerable to perturbations applied to one of their agents observations by attacking a single agent our attack method has highly negative impact on the overall team reward reducing it from to this results in the teams winning rate to go down from to less,On the Robustness of Cooperative Multi-Agent Reinforcement Learning,"8 March, 2020"
1254,Nicolas Papernot,machine learning algorithms are vulnerable to data poisoning attacks prior taxonomies that focus on specific scenarios eg indiscriminate or targeted have enabled defenses for the corresponding subset of known attacks yet this introduces an inevitable arms race between adversaries and defenders in this work we study the feasibility of an attackagnostic defense relying on artifacts that are common to all poisoning attacks specifically we focus on a common element between all attacks they modify gradients computed to train the model we identify two main artifacts of gradients computed in the presence of poison their ell norms have significantly higher magnitudes than those of clean gradients and their orientation differs from clean gradients based on these observations we propose the prerequisite for a generic poisoning defense it must bound gradient magnitudes and minimize differences in orientation we call this gradient shaping as an exemplar tool to evaluate the feasibility of gradient shaping we use differentially private stochastic gradient descent dpsgd which clips and perturbs individual gradients during training to obtain privacy guarantees we find that dpsgd even in configurations that do not result in meaningful privacy guarantees increases the models robustness to indiscriminate attacks it also mitigates worstcase targeted attacks and increases the adversarys cost in multipoison scenarios the only attack we find dpsgd to be ineffective against is a strong yet unrealistic indiscriminate attack our results suggest that while we currently lack a generic poisoning defense gradient shaping is a promising direction for future research less,On the Effectiveness of Mitigating Data Poisoning Attacks with Gradient Shaping,"27 February, 2020"
1255,Nicolas Papernot,we develop techniques to quantify the degree to which a given training or testing example is an outlier in the underlying distribution we evaluate five methods to score examples in a dataset by how wellrepresented the examples are for different plausible definitions of wellrepresented and apply these to four common datasets mnist fashionmnist cifar and imagenet despite being independent approaches we find all five are highly correlated suggesting that the notion of being wellrepresented can be quantified among other uses we find these methods can be combined to identify a prototypical examples that match human expectations b memorized training examples and c uncommon submodes of the dataset further we show how we can utilize our metrics to determine an improved ordering for curriculum learning and impact adversarial robustness we release all metric values on training and test sets we studied less,"Distribution Density, Tails, and Outliers in Machine Learning: Metrics and Applications","29 October, 2019"
1256,Nicolas Papernot,broad adoption of machine learning techniques has increased privacy concerns for models trained on sensitive data such as medical records existing techniques for training differentially private dp models give rigorous privacy guarantees but applying these techniques to neural networks can severely degrade model performance this performance reduction is an obstacle to deploying private models in the real world in this work we improve the performance of dp models by finetuning them through active learning on public data we introduce two new techniques diversepublic and nearprivate for doing this finetuning in a privacyaware way for the mnist and svhn datasets these techniques improve stateoftheart accuracy for dp models while retaining privacy guarantees less,Improving Differentially Private Models with Active Learning,"2 October, 2019"
1257,Vardan Papyan,the recently discovered neural collapse nc phenomenon occurs pervasively in todays deep net training paradigm of driving crossentropy ce loss towards zero during nc lastlayer features collapse to their classmeans both classifiers and classmeans collapse to the same simplex equiangular tight frame and classifier behavior collapses to the nearestclassmean decision rule recent works demonstrated that deep nets trained with mean squared error mse loss perform comparably to those trained with ce as a preliminary we empirically establish that nc emerges in such msetrained deep nets as well through experiments on three canonical networks and five benchmark datasets we provide in a google colab notebook pytorch code for reproducing msenc and cenc at httpscolabresearchgooglecomgithubneuralcollapseneuralcollapseblobmainneuralcollapseipynb the analyticallytractable mse loss offers more mathematical opportunities than the hardtoanalyze ce loss inspiring us to leverage mse loss towards the theoretical investigation of nc we develop three main contributions i we show a new decomposition of the mse loss into a terms directly interpretable through the lens of nc and which assume the lastlayer classifier is exactly the leastsquares classifier and b a term capturing the deviation from this leastsquares classifier ii we exhibit experiments on canonical datasets and networks demonstrating that termb is negligible during training this motivates us to introduce a new theoretical construct the central path where the linear classifier stays mseoptimal for feature activations throughout the dynamics iii by studying renormalized gradient flow along the central path we derive exact dynamics that predict nc less,Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path,"9 May, 2022"
1258,Vardan Papyan,modern practice for training classification deepnets involves a terminal phase of training tpt which begins at the epoch where training error first vanishes during tpt the training error stays effectively zero while training loss is pushed towards zero direct measurements of tpt for three prototypical deepnet architectures and across seven canonical classification datasets expose a pervasive inductive bias we call neural collapse involving four deeply interconnected phenomena nc crossexample withinclass variability of lastlayer training activations collapses to zero as the individual activations themselves collapse to their classmeans nc the classmeans collapse to the vertices of a simplex equiangular tight frame etf nc up to rescaling the lastlayer classifiers collapse to the classmeans or in other words to the simplex etf ie to a selfdual configuration nc for a given activation the classifiers decision collapses to simply choosing whichever class has the closest train classmean ie the nearest class center ncc decision rule the symmetric and very simple geometry induced by the tpt confers important benefits including better generalization performance better robustness and better interpretability less,Prevalence of Neural Collapse during the terminal phase of deep learning training,"21 August, 2020"
1259,Vardan Papyan,we apply stateoftheart tools in modern highdimensional numerical linear algebra to approximate efficiently the spectrum of the hessian of modern deepnets with tens of millions of parameters trained on real data our results corroborate previous findings based on smallscale networks that the hessian exhibits spiked behavior with several outliers isolated from a continuous bulk we decompose the hessian into different components and study the dynamics with training and sample size of each term individually less,The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size,"2 June, 2019"
1260,Vardan Papyan,the recently proposed multilayer convolutional sparse coding mlcsc model consisting of a cascade of convolutional sparse layers provides a new interpretation of convolutional neural networks cnns under this framework the computation of the forward pass in a cnn is equivalent to a pursuit algorithm aiming to estimate the nested sparse representation vectors or feature maps from a given input signal despite having served as a pivotal connection between cnns and sparse modeling a deeper understanding of the mlcsc is still lacking there are no pursuit algorithms that can serve this model exactly nor are there conditions to guarantee a nonempty model while one can easily obtain signals that approximately satisfy the mlcsc constraints it remains unclear how to simply sample from the model and more importantly how one can train the convolutional filters from real data in this work we propose a sound pursuit algorithm for the mlcsc model by adopting a projection approach we provide new and improved bounds on the stability of the solution of such pursuit and we analyze different practical alternatives to implement this in practice we show that the training of the filters is essential to allow for nontrivial signals in the model and we derive an online algorithm to learn the dictionaries from real data effectively resulting in cascaded sparse convolutional layers last but not least we demonstrate the applicability of the mlcsc model for several applications in an unsupervised setting providing competitive results our work represents a bridge between matrix factorization sparse dictionary learning and sparse autoencoders and we analyze these connections in detail less,Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary Learning,"30 June, 2018"
1261,Vardan Papyan,recovering images from undersampled linear measurements typically leads to an illposed linear inverse problem that asks for proper statistical priors building effective priors is however challenged by the low train and test overhead dictated by realtime tasks and the need for retrieving visually plausible and physically feasible images with minimal hallucination to cope with these challenges we design a cascaded network architecture that unrolls the proximal gradient iterations by permeating benefits from generative residual networks resnet to modeling the proximal operator a mixture of pixelwise and perceptual costs is then deployed to train proximals the overall architecture resembles backandforth projection onto the intersection of feasible and plausible images extensive computational experiments are examined for a global task of reconstructing mr images of pediatric patients and a more local task of superresolving celeba faces that are insightful to design efficient architectures our observations indicate that for mri reconstruction a recurrent resnet with a single residual block effectively learns the proximal this simple architecture appears to significantly outperform the alternative deep resnet architecture by db snr and the conventional compressedsensing mri by db snr with x faster inference for image superresolution our preliminary results indicate that modeling the denoising proximal demands deep resnets less,Recurrent Generative Adversarial Networks for Proximal Learning and Automated Compressive Image Recovery,"27 November, 2017"
1262,Vardan Papyan,convolutional sparse coding csc is an increasingly popular model in the signal and image processing communities tackling some of the limitations of traditional patchbased sparse representations although several works have addressed the dictionary learning problem under this model these relied on an admm formulation in the fourier domain losing the sense of locality and the relation to the traditional patchbased sparse pursuit a recent work suggested a novel theoretical analysis of this global model providing guarantees that rely on a localized sparsity measure herein we extend this localglobal relation by showing how one can efficiently solve the convolutional sparse pursuit problem and train the filters involved while operating locally on image patches our approach provides an intuitive algorithm that can leverage standard techniques from the sparse representations field the proposed method is fast to train simple to implement and flexible enough that it can be easily deployed in a variety of applications we demonstrate the proposed training scheme for image inpainting and image separation while achieving stateoftheart results less,Convolutional Dictionary Learning via Local Processing,"9 May, 2017"
1263,Vardan Papyan,the celebrated sparse representation model has led to remarkable results in various signal processing tasks in the last decade however despite its initial purpose of serving as a global prior for entire signals it has been commonly used for modeling low dimensional patches due to the computational constraints it entails when deployed with learned dictionaries a way around this problem has been proposed recently adopting a convolutional sparse representation model this approach assumes that the global dictionary is a concatenation of banded circulant matrices although several works have presented algorithmic solutions to the global pursuit problem under this new model very few trulyeffective guarantees are known for the success of such methods in the first of this twopart work we address the theoretical aspects of the sparse convolutional model providing the first meaningful answers to corresponding questions of uniqueness of solutions and success of pursuit algorithms to this end we generalize mathematical quantities such as the ell norm the mutual coherence and the spark to their counterparts in the convolutional setting which intrinsically capture local measures of the global model in a companion paper we extend the analysis to a noisy regime addressing the stability of the sparsest solutions and pursuit algorithms and demonstrate practical approaches for solving the global pursuit problem via simple local processing less,Working Locally Thinking Globally - Part I: Theoretical Guarantees for Convolutional Sparse Coding,"22 February, 2017"
1264,Vardan Papyan,convolutional neural networks cnn have led to many stateoftheart results spanning through various fields however a clear and profound theoretical understanding of the forward pass the core algorithm of cnn is still lacking in parallel within the wide field of sparse approximation convolutional sparse coding csc has gained increasing attention in recent years a theoretical study of this model was recently conducted establishing it as a reliable and stable alternative to the commonly practiced patchbased processing herein we propose a novel multilayer model mlcsc in which signals are assumed to emerge from a cascade of csc layers this is shown to be tightly connected to cnn so much so that the forward pass of the cnn is in fact the thresholding pursuit serving the mlcsc model this connection brings a fresh view to cnn as we are able to attribute to this architecture theoretical claims such as uniqueness of the representations throughout the network and their stable estimation all guaranteed under simple local sparsity conditions lastly identifying the weaknesses in the above pursuit scheme we propose an alternative to the forward pass which is connected to deconvolutional recurrent and residual networks and has better theoretical guarantees less,Convolutional Neural Networks Analyzed via Convolutional Sparse Coding,"10 October, 2016"
1265,Hannes Rӧst,"Proteome analysis revealed signatures of co-expressed upregulated metabolism proteins highly conserved between primary and non-small cell lung cancer (NSCLC) patient-derived xenograft tumors (Li et al. 2014, Nat. Communications 5:5469). The C10 signature is encoded by seven genes (ADSS, ATP2A2, CTPS1, IMPDH2, PKM2, PTGES3, SGPL1) and DNA alterations in C10-encoding genes are associated with longer survival in a subset of NSCLC. To explore the C10 signature as an oncogenic driver and address potential mechanisms of action, C10 protein expression and protein-protein interactions were determined. In independent NSCLC cohorts, the coordinated expression of C10 proteins was significant and mutations in C10 genes were associated with better outcome. Affinity purification-mass spectrometry and in vivo proximity-based biotin identification defined a C10 interactome involving 667",Proteomic characterization of a candidate polygenic driver of metabolism in non-small cell lung cancer,2022-05-18 00:00:00
1266,Hannes Rӧst,"The extraction of meaningful biological knowledge from high-throughput mass spectrometry data relies on limiting false discoveries to a manageable amount. For targeted approaches in metabolomics a main challenge is the detection of false positive metabolic features in the low signal-to-noise ranges of data-independent acquisition results and their filtering. Another factor is that the creation of assay libraries for data-independent acquisition analysis and the processing of extracted ion chromatograms have not been automated in metabolomics. Here we present a fully automated open-source workflow for high-throughput metabolomics that combines data-dependent and data-independent acquisition for library generation, analysis, and statistical validation, with rigorous control of the false-discovery rate while matching manual analysis regarding quantification accuracy. ",DIAMetAlyzer allows automated false-discovery rate-controlled analysis for data-independent acquisition in metabolomics,2022-03-15 00:00:00
1267,Hannes Rӧst,"Data-independent acquisition (DIA) has become an important approach in global, mass spectrometric proteomic studies because it provides in-depth insights into the molecular variety of biological systems. However, DIA data analysis remains challenging owing to the high complexity and large data and sample size, which require specialized software and vast computing infrastructures. Most available open-source DIA software necessitates basic programming skills and covers only a fraction of a complete DIA data analysis. In consequence, DIA data analysis often requires usage of multiple software tools and compatibility thereof, severely limiting the usability and reproducibility.",Democratizing data-independent acquisition proteomics analysis on public cloud infrastructures via the Galaxy framework,2022-02-16 00:00:00
1268,Hannes Rӧst,"SWATH-MS has become a mainstream method for quantitative proteomics, however consistent quantification across multiple LC-MS/MS instruments remains a bottleneck in parallelizing the data-acquisition. To produce a highly consistent and quantitatively accurate data matrix, we have developed DIAlignR which uses raw fragment-ion chromatograms for cross-run alignment. Its performance on a gold standard annotated dataset, demonstrates a threefold reduction in the identification error-rate when compared to standard non-aligned SWATH-MS results. A similar performance is achieved for a dataset of 229 runs acquired using 11 different LC-MS/MS setups. Finally, the analysis of 949 plasma runs with DIAlignR increased the number of statistically significant proteins by 43% and 62% for insulin resistant (IR) and respiratory viral infection (RVI), respectively compared to prior analysis without it. Hence, DIAlignR fills a gap in analyzing SWATH-MS runs acquired in-parallel using different LC-MS/MS instrumentation.",Signal Alignment Enables Analysis of DIA Proteomics Data from Multisite Experiments,2022-01-01 00:00:00
1269,Hannes Rӧst,"Mass spectrometry is the method of choice in large-scale proteomics studies. One common method is data-independent acquisition (DIA), which allows for high-throughput analysis of biological samples, but also produces complex data. Methods of peptide separation, in addition to retention time, improve data analysis and there has been increasing interest in separating peptides based on collisional cross section (CCS), which is a measure of the size of a peptide. However, existing libraries that are used during data analysis lack CCS measurements, and this data is expensive and time-consuming to acquire. This has led to the desire to predict library values for mass spectrometry analysis. Here we compare three deep learning architectures, LSTM, CNN, and transformer, for the tasks of retention time and collisional cross section prediction. We show that the LSTM and CNN models perform similarly and that the transformer has a lower performance than expected.",Comparing Machine Learning Architectures for the Prediction of Peptide Collisional Cross Section,2022-01-01 00:00:00
1270,Hannes Rӧst,"In bottom-up mass spectrometry-based proteomics, deep proteome coverage is limited by high cofragmentation rates. Cofragmentation occurs when more than one analyte is isolated by the quadrupole and the subsequent fragmentation event produces fragment ions of heterogeneous origin. One strategy to reduce cofragmentation rates is through effective peptide separation techniques such as chromatographic separation and, the more recently popularized, ion mobility (IM) spectrometry, which separates peptides by their collisional cross section. Here, we use a computational model to investigate the capability of the trapped IM spectrometry (TIMS) device at effectively separating peptide ions and quantify the separation power of the TIMS device in the context of a parallel accumulation-serial fragmentation (PASEF) workflow.",Trapped Ion Mobility Spectrometry Reduces Spectral Complexity in Mass Spectrometry-Based Proteomics,2021-12-09 00:00:00
1271,Hannes Rӧst,"Women with a history of gestational diabetes mellitus (GDM) have a 7-fold higher risk of developing type 2 diabetes (T2D). It is estimated that 20-50% of women with GDM history will progress to T2D within 10 years after delivery. Intensive lactation could be negatively associated with this risk, but the mechanisms behind a protective effect remain unknown. In this study, we utilized a prospective GDM cohort of 1010 women without T2D at 6-9 weeks postpartum (study baseline) and tested for T2D onset up to 8 years post-baseline (n=980). Targeted metabolic profiling was performed on fasting plasma samples collected at both baseline and follow-up (1-2 years post-baseline) during research exams in a subset of 350 women (216 intensive breastfeeding, IBF vs. 134 intensive formula feeding or mixed feeding, IFF/Mixed). The relationship between lactation intensity and circulating metabolites at both baseline and follow-up were evaluated to discover underlying metabolic responses of lactation and to explore the link between these metabolites and T2D risk.",Intensive lactation among women with recent gestational diabetes significantly alters the early postpartum circulating lipid profile: the SWIFT study,2021/12
1272,Hannes Rӧst,"Mass spectrometry is a key tool in the study of small molecules, playing an important role in metabolomics, drug discovery, and environmental chemistry. Tandem mass spectra capture fragmentation patterns that provide key structural information about a molecule and help with its identification. Practitioners often rely on spectral library searches to match unknown spectra with known compounds. However, such search-based methods are limited by availability of reference experimental data. In this work we show that graph transformers can be used to accurately predict tandem mass spectra. Our model, MassFormer, outperforms competing deep learning approaches for spectrum prediction, and includes an interpretable attention mechanism to help explain predictions. We demonstrate that our model can be used to improve reference library coverage on a synthetic molecule identification task. ",MassFormer: Tandem Mass Spectrum Prediction with Graph Transformers,2021-11-08 00:00:00
1273,Hannes Rӧst,"Mass spectrometry (MS) data, used for proteomics and metabolomics analyses, have seen considerable growth in the last years. Aiming at reducing the associated storage costs, dedicated compression algorithms for MS data have been proposed, such as MassComp and MSNumpress. However, these algorithms focus on either lossless or lossy compression, respectively, and do not exploit the additional redundancy existing across scans contained in a single file. We introduce mspack, a compression algorithm for MS data that exploits this additional redundancy and that supports both lossless and lossy compression, as well as the mzML and the legacy mzXML formats. mspack applies several preprocessing lossless transforms and optional lossy transforms with a configurable error, followed by the general purpose compressors gzip or bsc to achieve a higher compression ratio",mspack: efficient lossless and lossy mass spectrometry data compression,2021-11-01 00:00:00
1274,Hannes Rӧst,"Targeted, untargeted, and data-independent acquisition (DIA) metabolomics workflows are often hampered by ambiguous identification based on either MS1 information alone or relatively few MS2 fragment ions. While DIA methods have been popularized in proteomics, it is less clear whether they are suitable for metabolomics workflows due to their large precursor isolation windows and complex coisolation patterns. Here, we quantitatively investigate the conditions necessary for unique metabolite detection in complex backgrounds using precursor and fragment ion mass-to-charge (m/z) separation, comparing three benchmarked mass spectrometry (MS) methods [MS1, MRM (multiple reaction monitoring), and DIA]. ",Analyzing Assay Specificity in Metabolomics Using Unique Ion Signature Simulations,2021-08-10 00:00:00
1275,Hannes Rӧst,"While molecules that promote the growth of animal cells have been identified, it remains unclear how such signals are orchestrated to determine a characteristic target size for different cell types. It is increasingly clear that cell size is determined by size checkpoints—mechanisms that restrict the cell cycle progression of cells that are smaller than their target size. Previously, we described a p38 MAPK-dependent cell size checkpoint mechanism whereby p38 is selectively activated and prevents cell cycle progression in cells that are smaller than a given target size. In this study, we show that the specific target size required for inactivation of p38 and transition through the cell cycle is determined by CDK4 activity. Our data suggest a model whereby p38 and CDK4 cooperate analogously to the function of a thermostat: while p38 senses irregularities in size, CDK4 corresponds to the thermostat dial that sets the target size.",Cell size homeostasis is maintained by CDK4-dependent activation of p38 MAPK,2021-06-21 00:00:00
1276,Hannes Rӧst,mass spectrometry is a key tool in the study of small molecules playing an important role in metabolomics drug discovery and environmental chemistry tandem mass spectra capture fragmentation patterns that provide key structural information about a molecule and help with its identification practitioners often rely on spectral library searches to match unknown spectra with known compounds however such searchbased methods are limited by availability of reference experimental data in this work we show that graph transformers can be used to accurately predict tandem mass spectra our model massformer outperforms competing deep learning approaches for spectrum prediction and includes an interpretable attention mechanism to help explain predictions we demonstrate that our model can be used to improve reference library coverage on a synthetic molecule identification task through quantitative analysis and visual inspection we verify that our model recovers prior knowledge about the effect of collision energy on the generated spectrum we evaluate our model on different types of mass spectra from two independent ms datasets and show that its performance generalizes code available at githubcomroestlabmassformer less,MassFormer: Tandem Mass Spectrum Prediction with Graph Transformers,"15 November, 2021"
1277,Fritz Roth,"While genetic testing is becoming standard of care for patients with potentially inherited cardiovascular disease, the prevalence of uncertain results severely limits its utility. One promising approach is to generate variant effect maps that report the function of all possible variants in a gene prospectively. The proactive clinical application of these maps is nascent, and requires careful integration with current American College of Medical Genetics guidelines for variant interpretation. Here, we describe three pediatric cases of cardiac arrest or sudden cardiac death with variants of uncertain significance in calmodulin genes. We demonstrate the prospective clinical utility of a calmodulin variant effect map to inform variant interpretation, and therefore diagnosis and family care, in each case. This study was approved by the Stanford University and Vanderbilt University Medical Center IRBs.",Proactive variant effect mapping to accelerate genetic diagnosis for pediatric cardiac arrest,2022-01-12 00:00:00
1278,Fritz Roth,"Diabetes is a complex disease spanning from the heterogeneous etiology of type 1 and type 2 diabetes to monogenic diabetes. A common monogenic form of diabetes is glucokinase (GCK) maturity-onset diabetes of the young (GCK-MODY), which is caused by heterozygous inactivating variants in the gene encoding GCK. GCK is known as the pancreatic glucose sensor, as it regulates insulin secretion to maintain appropriate blood glucose levels. Accordingly, variants that alter GCK activity can cause hypo- and hyperglycemia, associated with hyperinsulinemic hypoglycemia (HH) and GCK-MODY, respectively, affecting up to 10 million people worldwide. Patients with GCK-MODY, in contrast to other people with diabetes, often do not require treatment but are frequently misdiagnosed and treated unnecessarily. Genetic testing can prevent this but is hampered by the challenge of interpreting novel missense variants. To address this, we generated a comprehensive map of GCK variant activity in yeast.",A multiplexed assay of human glucokinase reveals thousands of potential disease variants with both decreased and increased activity,2022-01-01 00:00:00
1279,Fritz Roth,"Here, we present satmut_utils as a flexible solution for 1) simulation of saturation mutagenesis data; and 2) quantification of variants across four orders of magnitude from multiplexed assay data. Improvements of satmut_utils over existing solutions include support for multiple experimental strategies, unique molecular identifier-based consensus deduplication, and machine learning-based error correction. We developed a rigorous simulation workflow to validate the performance of satmut_utils and carried out the first benchmarking of existing software for variant calling. Finally, we used satmut_utils to determine the mRNA abundance of thousands of coding variants in cystathionine beta-synthase (CBS) by two library preparation methods. We identified an association between variants near chemical cofactor binding sites and decreased mRNA abundance. We also found a correlation between codon optimality and the magnitude of variant effects, emphasizing the potential of single-nucleotide variants to alter mRNA abundance.",satmut_utils: a simulation and variant calling package for multiplexed assays of variant effect,2022-01-01 00:00:00
1280,Fritz Roth,"The success of personalized genomic medicine depends on our ability to assess the pathogenicity of rare human variants, including the important class of missense variation. There are many challenges in training accurate computational systems, e.g., in finding the balance between quantity, quality, and bias in the variant sets used as training examples and avoiding predictive features that can accentuate the effects of bias. Here, we describe VARITY, which judiciously exploits a larger reservoir of training examples with uncertain accuracy and representativity. To limit circularity and bias, VARITY excludes features informed by variant annotation and protein identity. To provide a rationale for each prediction, we quantified the contribution of features and feature combinations to the pathogenicity inference of each variant. VARITY outperformed all previous computational methods evaluated, identifying at least 10 ",Improved pathogenicity prediction for rare human missense variants,2021-10-07 00:00:00
1281,Fritz Roth,"Most rare clinical missense variants cannot currently be classified as pathogenic or benign. Deficiency in human 5,10-methylenetetrahydrofolate reductase (MTHFR), the most common inherited disorder of folate metabolism, is caused primarily by rare missense variants. Further complicating variant interpretation, variant impacts often depend on environment. An important example of this phenomenon is the MTHFR variant p.Ala222Val (c.665C>T), which is carried by half of all humans and has a phenotypic impact that depends on dietary folate. Here we describe the results of 98,336 variant functional-impact assays, covering nearly all possible MTHFR amino acid substitutions in four folinate environments, each in the presence and absence of p.Ala222Val. The resulting atlas of MTHFR variant effects reveals many complex dependencies on both folinate and p.Ala222Val. MTHFR atlas scores can distinguish ",Shifting landscapes of human MTHFR missense-variant effects,2021-07-01 00:00:00
1282,Fritz Roth,"Multiplexed assays of variant effect (MAVEs) are capable of experimentally testing all possible single nucleotide or amino acid variants in selected genomic regions, generating ‘variant effect maps’, which provide biochemical insight and functional evidence to enable more rapid and accurate clinical interpretation of human variation. Because the international community applying MAVE approaches is growing rapidly, we developed the online MaveRegistry platform to catalyze collaboration, reduce redundant efforts, allow stakeholders to nominate targets and enable tracking and sharing of progress on ongoing MAVE projects.",MaveRegistry: a collaboration platform for multiplexed assays of variant effect,2021-03-27 00:00:00
1283,Fritz Roth,"Next generation sequencing has become a common tool in the diagnosis of genetic diseases. However, for the vast majority of genetic variants that are discovered, a clinical interpretation is not available. Variant effect mapping allows the functional effects of large numbers of single amino acid variants to be characterized in parallel. Here, we employ a variant effect mapping framework, combining functional assays with machine learning, to assess the effects of 89% of all possible amino acid substitutions in the human intellectual disability-associated gene, GDI1. We show that the resulting variant effect map can be used to discriminate pathogenic from benign variants at levels of precision higher than those achieved by current computational prediction tools. ",A systematic genotype-phenotype map for missense variants in the human intellectual disability-associated gene GDI1,2021-01-01 00:00:00
1284,Fritz Roth,"Computational predictors can help interpret pathogenicity of human genetic variants, especially for the majority of variants where no experimental data are available. However, because we lack a high-quality unbiased test set, identifying the best-performing predictors remains a challenge. To address this issue, we evaluated missense variant effect predictors using genotypes and traits from a prospective cohort. We considered 139 gene-trait combinations with rare-variant burden association based on at least one of four systematic studies using phenotypes and whole-exome sequences from ~200K UK Biobank participants. ",Assessing computational variant effect predictors with a large prospective cohort,2021-01-01 00:00:00
1285,Fritz Roth,"Key steps in viral propagation, immune suppression, and pathology are mediated by direct, binary, physical interactions between viral and host proteins. To understand the biology of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection, we generated an unbiased systematic map of binary interactions between viral and host proteins, complementing previous co-complex association maps by conveying more direct mechanistic understanding and potentially enabling targeted disruption of direct interactions. To this end, we deployed two parallel strategies, identifying 205 virus-host and 27 intraviral binary interactions amongst 171 host and 19 viral proteins, and confirming high quality of these interactions via a calibrated orthogonal assay. Host proteins interacting with SARS-CoV-2 proteins are enriched in various cellular processes, including immune signaling and inflammation, protein ubiquitination, and membrane trafficking. Specific subnetworks provide new hypotheses related to viral modulation of host protein homeostasis and T-cell regulation. ",A map of binary SARS-CoV-2 protein interactions implicates host immune regulation and ubiquitination,2021-01-01 00:00:00
1286,Fritz Roth,"The interactome is often conceived of primarily as a collection of hundreds of multimeric machines, collectively referred to as the “complexome”. However, a large proportion of the interactome exists outside of the complexome, or in the “outer-complexome”, and may account for most of the functional plasticity exhibited by cellular systems. To compare features of inner- versus outer-complexome organization, we systematically generated a yeast all-by-all binary interactome map, integrated it with previous binary maps, and compared the resulting interactome “atlas” with systematic co-complex association and functional similarity network datasets. Direct protein-protein interactions in the inner-complexome tend to be readily detected in multiple assays and exhibit high levels of coherence with functional similarity relationships. In contrast, pairs of proteins connected by relatively transient, harder-to-detect binary interactions in the outer-complexome, exhibit higher levels of functional heterogeneity. ",Binary Interactome Models of Inner-Versus Outer-Complexome Organization,2021-01-01 00:00:00
1287,Fritz Roth,"Kinases are critical components of intracellular signaling pathways and have been extensively investigated with regard to their roles in cancer. p21-activated kinase-1 (PAK1) is a serine/threonine kinase that has been previously implicated in numerous biological processes, such as cell migration, cell cycle progression, cell motility, invasion, and angiogenesis, in glioma and other cancers. However, the signaling network linked to PAK1 is not fully defined. We previously reported a large-scale yeast genetic interaction screen using toxicity as a readout to identify candidate PAK1 genetic interactions.",Interrogation of kinase genetic interactions provides a global view of PAK1-mediated signal transduction pathways,2020-12-11 00:00:00
1288,Fritz Roth,gammaray bursts grbs serve as powerful probes of the early universe with their luminous afterglows revealing the locations and physical properties of star forming galaxies at the highest redshifts and potentially locating first generation population iii stars since grb afterglows have intrinsically very simple spectra they allow robust redshifts from low signal to noise spectroscopy or photometry here we present a photometric redshift of z for the swiftdetected grb b based on deep observations with gemininorth the very large telescope and the grb optical and nearinfrared detector assuming a small magellanic cloud dust law which has been found in a majority of grb sightlines the likelihood range for the redshift is z although there is a lowprobability tail to somewhat lower redshifts adopting milky way or large magellanic cloud dust laws leads to very similar conclusions while a maiolino law does allow somewhat lower redshift solutions but in all cases the most likely redshift is found to be z the nondetection of the host galaxy to deep limits yab mag which would correspond roughly to l at z in our late time optical and infrared observations with the hubble space telescope strongly supports the extreme redshift origin of grb b since we would expect to have detected any lowz galaxy even if it were highly dusty finally the energetics of grb b are comparable to those of other grbs and suggest that its progenitor is not greatly different to those of lower redshift bursts less,A Photometric Redshift of z ~ 9.4 for GRB 090429B,"31 May, 2011"
1289,Daniel Roy,we study the mutual information between certain summaries of the output of a learning algorithm and its n training data conditional on a supersample of n iid data from which the training data is chosen at random without replacement these leaveoneout variants of the conditional mutual information cmi of an algorithm steinke and zakynthinou are also seen to control the mean generalization error of learning algorithms with bounded loss functions for learning algorithms achieving zero empirical risk under loss ie interpolating algorithms we provide an explicit connection between leaveoneout cmi and the classical leaveoneout error estimate of the risk using this connection we obtain upper and lower bounds on risk in terms of the evaluated leaveoneout cmi when the limiting risk is constant or decays polynomially the bounds converge to within a constant factor of two as an application we analyze the population risk of the oneinclusion graph algorithm a generalpurpose transductive learning algorithm for vc classes in the realizable setting using leaveoneout cmi we match the optimal bound for learning vc classes in the realizable setting answering an open challenge raised by steinke and zakynthinou finally in order to understand the role of leaveoneout cmi in studying generalization we place leaveoneout cmi in a hierarchy of measures with a novel unconditional mutual information at the root for loss and interpolating learning algorithms this mutual information is observed to be precisely the risk less,Understanding Generalization via Leave-One-Out Conditional Mutual Information,"29 June, 2022"
1290,Daniel Roy,we report measurements of gigahertzfrequency antiferromagnetic resonances that are anisotropic as a function of the direction of applied magnetic field relative to the crystal axes in the van der waals easyaxis antiferromagnet crsbr we map the resonance frequencies as function of the magnitude and angle of an applied magnetic field which allows us to identify different modes in the low and highfield regimes the spectra show good agreement with a landaulifshitz model for two antiferromagneticallycoupled sublattices accounting for interlayer exchange and triaxial magnetic anisotropy fits allow us to quantify the parameters governing the magnetic dynamics at k the interlayer exchange field is he t and the hard and intermediateaxis anisotropy parameters are hc t and ha t the existence of withinplane anisotropy makes it possible to control the degree of hybridization between the antiferromagnetic resonances using an inplane magnetic field less,Anisotropic Gigahertz Antiferromagnetic Resonances of the Easy-Axis van der Waals Antiferromagnet CrSBr,"2 June, 2022"
1291,Daniel Roy,we present the validation of a transiting lowdensity exoplanet orbiting the m dwarf toi discovered by the nasa tess mission we utilize photometric data from both tess and groundbased followup observations to validate the ephemerides of the day transiting signal and vet false positive scenarios highcontrast imaging data are used to resolve the stellar host and exclude stellar companions at separations gtrsim we obtain followup spectroscopy and corresponding precise radial velocities rvs with multiple prv spectrographs to confirm the planetary nature of the transiting exoplanet we calculate a upper limit of mp moplus and p g cm and we identify a nontransiting day candidate we also find evidence for a substellar mrm j companion with a projected separation lesssim au from a combined analysis of gaia ao imaging and rvs with the discovery of this outer companion we carry out a detailed exploration of the possibilities that toi b might instead be a circumsecondary planet or a pair of eclipsing binary stars orbiting the host in a hierarchical triple system we find under scrutiny that we can exclude both of these scenarios from the multiwavelength transit photometry thus validating toi b as a lowdensity exoplanet transiting the central star in this system the low density of toi b makes it one of the most amenable exoplanets for atmospheric characterization such as with jwst and ariel validated or confirmed by the tess mission to date less,A close-in puffy Neptune with hidden friends: The enigma of TOI 620,"6 April, 2022"
1292,Daniel Roy,this snowmass white paper describes the cosmic microwave background stage project cmbs which is designed to cross critical thresholds in our understanding of the origin and evolution of the universe from the highest energies at the dawn of time through the growth of structure to the present day we provide an overview of the science case the technical design and project plan less,Snowmass 2021 CMB-S4 White Paper,"15 March, 2022"
1293,Daniel Roy,linear temporal logic ltl is a specification language for finite sequences called traces widely used in program verification motion planning in robotics process mining and many other areas we consider the problem of learning ltl formulas for classifying traces despite a growing interest of the research community existing solutions suffer from two limitations they do not scale beyond small formulas and they may exhaust computational resources without returning any result we introduce a new algorithm addressing both issues our algorithm is able to construct formulas an order of magnitude larger than previous methods and it is anytime meaning that it in most cases successfully outputs a formula albeit possibly not of minimal size we evaluate the performances of our algorithm using an open source implementation against publicly available benchmarks less,Scalable Anytime Algorithms for Learning Fragments of Linear Temporal Logic,"1 February, 2022"
1294,Daniel Roy,we validate the presence of a twoplanet system orbiting the gyr k dwarf toi hd the system consists of an inner moderately eccentric transiting minineptune toi b p pm days e initially discovered in the sector tess mission observations and a transiting minineptune toi c p pm days discovered in the sector observations in a rare orbital resonance we utilize photometric data from tess textitspitzer and groundbased followup observations to confirm the ephemerides and period of the transiting planets and vet false positive scenarios we obtain followup spectroscopy and corresponding precise radial velocities rvs with the ishell spectrograph at the nasa infrared telescope facility and the hires spectrograph at keck observatory to validate the planetary nature of these signals which we combine with published pfs rvs from magellan observatory we place upper limits on the masses of both planets of and mnep for b and c respectively we apply a gaussian processes gp model to the tess light curves to place priors on a chromatic radial velocity gp model to constrain the stellar activity of the toi host star toi is a nearby moderately young multiplanet system with two planets suitable for atmospheric characterization with james webb space telescope jwst and other upcoming missions in particular it will undergo six transit pairs separated by hours before june less,"TOI 560 : Two Transiting Planets Orbiting a K Dwarf Validated with iSHELL, PFS and HIRES RVs","29 December, 2021"
1295,Daniel Roy,we consider the task of estimating a conditional density using iid samples from a joint distribution which is a fundamental problem with applications in both classification and uncertainty quantification for regression for joint density estimation minimax rates have been characterized for general density classes in terms of uniform metric entropy a wellstudied notion of statistical capacity when applying these results to conditional density estimation the use of uniform entropy which is infinite when the covariate space is unbounded and suffers from the curse of dimensionality can lead to suboptimal rates consequently minimax rates for conditional density estimation cannot be characterized using these classical results we resolve this problem for wellspecified models obtaining matching within logarithmic factors upper and lower bounds on the minimax kullbackleibler risk in terms of the empirical hellinger entropy for the conditional density class the use of empirical entropy allows us to appeal to concentration arguments based on local rademacher complexity which in contrast to uniform entropy leads to matching rates for large potentially nonparametric classes and captures the correct dependence on the complexity of the covariate space our results require only that the conditional densities are bounded above and do not require that they are bounded below or otherwise satisfy any tail conditions less,Minimax Rates for Conditional Density Estimation via Empirical Entropy,"23 September, 2021"
1296,Daniel Roy,we study the generalization properties of the popular stochastic optimization method known as stochastic gradient descent sgd for optimizing general nonconvex loss functions our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by sgd the key factors our bounds depend on are the variance of the gradients with respect to the data distribution and the local smoothness of the objective function along the sgd path and the sensitivity of the loss function to perturbations to the final output our key technical tool is combining the informationtheoretic generalization bounds previously used for analyzing randomized variants of sgd with a perturbation analysis of the iterates less,Information-Theoretic Generalization Bounds for Stochastic Gradient Descent,"15 August, 2021"
1297,Scott Sanner,contrastive learning has led to substantial improvements in the quality of learned embedding representations for tasks such as image classification however a key drawback of existing contrastive augmentation methods is that they may lead to the modification of the image content which can yield undesired alterations of its semantics this can affect the performance of the model on downstream tasks hence in this paper we ask whether we can augment image data in contrastive learning such that the taskrelevant semantic content of an image is preserved for this purpose we propose to leverage saliencybased explanation methods to create contentpreserving masked augmentations for contrastive learning our novel explanationdriven supervised contrastive learning excon methodology critically serves the dual goals of encouraging nearby image embeddings to have similar content and explanation to quantify the impact of excon we conduct experiments on the cifar and the tiny imagenet datasets we demonstrate that excon outperforms vanilla supervised contrastive learning in terms of classification explanation quality adversarial robustness as well as probabilistic calibration in the context of distributional shift less,ExCon: Explanation-driven Supervised Contrastive Learning for Image Classification,"17 April, 2022"
1298,Scott Sanner,weakly supervised semantic segmentation wsss with only imagelevel supervision is a challenging task most existing methods exploit class activation maps cam to generate pixellevel pseudo labels for supervised training however due to the local receptive field of convolution neural networks cnn cam applied to cnns often suffers from partial activation highlighting the most discriminative part instead of the entire object area in order to capture both local features and global representations the conformer has been proposed to combine a visual transformer branch with a cnn branch in this paper we propose transcam a conformerbased solution to wsss that explicitly leverages the attention weights from the transformer branch of the conformer to refine the cam generated from the cnn branch transcam is motivated by our observation that attention weights from shallow transformer blocks are able to capture lowlevel spatial feature similarities while attention weights from deep transformer blocks capture highlevel semantic context despite its simplicity transcam achieves a new stateoftheart performance of and on the respective pascal voc validation and test sets showing the effectiveness of transformer attentionbased refinement of cam for wsss less,TransCAM: Transformer Attention-based CAM Refinement for Weakly Supervised Semantic Segmentation,"14 March, 2022"
1299,Scott Sanner,spatiotemporal prediction of event data is a challenging task with a long history of research while recent work in spatiotemporal prediction has leveraged deep sequential models that substantially improve over classical approaches these models are prone to overfitting when the observation is extremely sparse as in the task of crime event prediction to overcome these sparsity issues we present multiaxis attentive prediction for sparse event data mapsed we propose a purely attentional approach to extract both shortterm dynamics and longterm semantics of event propagation through two observation angles unlike existing temporal prediction models that propagate latent information primarily along the temporal dimension the mapsed simultaneously operates over all axes time d space event type of the embedded data tensor we additionally introduce a novel frobenius normbased contrastive learning objective to improve latent representational generalizationempirically we validate mapsed on two publicly accessible urban crime datasets for spatiotemporal sparse event prediction where mapsed outperforms both classical and stateoftheart deep learning models the proposed contrastive learning objective significantly enhances the mapseds ability to capture the semantics and dynamics of the events resulting in better generalization ability to combat sparse observations less,Multi-axis Attentive Prediction for Sparse EventData: An Application to Crime Prediction,"4 October, 2021"
1300,Scott Sanner,recent years have seen the introduction of a range of methods for posthoc explainability of image classifier predictions however these posthoc explanations may not always be faithful to classifier predictions which poses a significant challenge when attempting to debug models based on such explanations to this end we seek a methodology that can improve the faithfulness of an explanation method with respect to model predictions which does not require ground truth explanations we achieve this through a novel explanationdriven data augmentation edda technique that augments the training data with occlusions inferred from model explanations this is based on the simple motivating principle that emphif the explainer is faithful to the model emphthen occluding salient regions for the model prediction should decrease the model confidence in the prediction while occluding nonsalient regions should not change the prediction to verify that the proposed augmentation method has the potential to improve faithfulness we evaluate edda using a variety of datasets and classification models we demonstrate empirically that our approach leads to a significant increase of faithfulness which can facilitate better debugging and successful deployment of image classification models in realworld applications less,EDDA: Explanation-driven Data Augmentation to Improve Explanation Faithfulness,"24 September, 2021"
1301,Scott Sanner,covid represented a major shock to global health systems not the least to resourcechallenged regions in the global south we report on a case of digital information system resilience in the response to data needs from the covid pandemic in two countries in the global south in contrast to dominant perspectives where digital resilience enables bounce back or maintenance of a status quo we identify five bounce forward resilience preconditions i distributed training ii local expertise iii local autonomy and ownership iv local infrastructure and v platform design infrastructure these preconditions enable an elevated degree of resilience that in the face of an external shock such as covid can deliver a bounce forward or strengthening of the information system beyond its preshock state less,Digital Resilience to Covid-19: A Model for National Digital Health Systems to Bounce Forward From the Shock of a Global Pandemic,"22 August, 2021"
1302,Scott Sanner,planning provides a framework for optimizing sequential decisions in complex environments recent advances in efficient planning in deterministic or stochastic highdimensional domains with continuous action spaces leverage backpropagation through a model of the environment to directly optimize actions however existing methods typically not take risk into account when optimizing in stochastic domains which can be incorporated efficiently in mdps by optimizing the entropic utility of returns we bridge this gap by introducing riskaware planning using pytorch raptor a novel framework for risksensitive planning through endtoend optimization of the entropic utility objective a key technical difficulty of our approach lies in that direct optimization of the entropic utility by backpropagation is impossible due to the presence of environment stochasticity the novelty of raptor lies in the reparameterization of the state distribution which makes it possible to apply stochastic backpropagatation through sufficient statistics of the entropic utility computed from forwardsampled trajectories the direct optimization of this empirical objective in an endtoend manner is called the riskaverse straightline plan which commits to a sequence of actions in advance and can be suboptimal in highly stochastic domains we address this shortcoming by optimizing for riskaware deep reactive policies radrp in our framework we evaluate and compare these two forms of raptor on three highly stochastic domains including nonlinear navigation hvac control and linear reservoir control demonstrating the ability to manage risk in complex mdps less,RAPTOR: End-to-end Risk-Aware MDP Planning and Policy Learning by Backpropagation,"14 June, 2021"
1303,Scott Sanner,as imagebased deep learning becomes pervasive on every device from cell phones to smart watches there is a growing need to develop methods that continually learn from data while minimizing memory footprint and power consumption while memory replay techniques have shown exceptional promise for this task of continual learning the best method for selecting which buffered images to replay is still an open question in this paper we specifically focus on the online classincremental setting where a model needs to learn new classes continually from an online data stream to this end we contribute a novel adversarial shapley value scoring method that scores memory data samples according to their ability to preserve latent decision boundaries for previously observed classes to maintain learning stability and avoid forgetting while interfering with latent decision boundaries of current classes being learned to encourage plasticity and optimal learning of new class boundaries overall we observe that our proposed aser method provides competitive or improved performance compared to stateoftheart replaybased continual learning methods on a variety of datasets less,Online Class-Incremental Continual Learning with Adversarial Shapley Value,"22 March, 2021"
1304,Scott Sanner,oneclass collaborative filtering occf is a common class of recommendation problem where only the positive class is explicitly observed eg purchases clicks autoencoder based recommenders such as autorec and variants demonstrate strong performance on many occf benchmarks but also empirically suffer from a strong popularity bias while a careful choice of negative samples in the occf setting can mitigate popularity bias negative sampling ns is often better for training embeddings than for the end task itself to address this we propose a twoheaded autorec to first train an embedding layer via one head using negative sampling then to train for the final task via the second head while this nsautorec improves results for autorec and outperforms many stateoftheart baselines on occf problems we notice that negative sampling can still take a large amount of time to train since negative sampling is known to be a special case of noise contrastive estimation nce we adapt a recently proposed closedform nce solution for collaborative filtering to autorec yielding nceautorec overall we show that our novel twoheaded autorec models nceautorec and nsautorec successfully mitigate the popularity bias issue and maintain competitive performance in comparison to stateoftheart recommenders on multiple realworld datasets less,Noise Contrastive Estimation for Autoencoding-based One-Class Collaborative Filtering,"5 August, 2020"
1305,Scott Sanner,continual learning is a branch of deep learning that seeks to strike a balance between learning stability and plasticity the cvpr clvision continual learning for computer vision challenge is dedicated to evaluating and advancing the current stateoftheart continual learning methods using the core dataset with three different continual learning scenarios this paper presents our approach called batchlevel experience replay with review to this challenge our team achieved the st place in all three scenarios out of participated teams the codebase of our implementation is publicly available at httpsgithubcomraptormaicvprclvisionchallenge less,Batch-level Experience Replay with Review for Continual Learning,"11 July, 2020"
1306,Scott Sanner,learning from demonstrations lfd improves the exploration efficiency of a learning agent by incorporating demonstrations from experts however demonstration data can often come from multiple experts with conflicting goals making it difficult to incorporate safely and effectively in online settings we address this problem in the static and dynamic optimization settings by modelling the uncertainty in source and target task functions using normalinversegamma priors whose corresponding posteriors are respectively learned from demonstrations and target data using bayesian neural networks with shared features we use this learned belief to derive a quadratic programming problem whose solution yields a probability distribution over the expert models finally we propose bayesian experience reuse bers to sample demonstrations in accordance with this distribution and reuse them directly in new tasks we demonstrate the effectiveness of this approach for static optimization of smooth functions and transfer learning in a highdimensional supply chain problem with cost uncertainty less,Bayesian Experience Reuse for Learning from Multiple Demonstrators,"10 June, 2020"
1307,Scott Sanner,in this paper we leverage the efficiency of binarized neural networks bnns to learn complex state transition models of planning domains with discretized factored state and action spaces in order to directly exploit this transition structure for planning we present two novel compilations of the learned factored planning problem with bnns based on reductions to weighted partial maximum boolean satisfiability fdsatplan as well as binary linear programming fdblpplan theoretically we show that our satbased bidirectional neuron activation encoding is asymptotically the most compact encoding relative to the current literature and supports unit propagation up an important property that facilitates efficiency in sat solvers experimentally we validate the computational efficiency of our bidirectional neuron activation encoding in comparison to an existing neuron activation encoding and demonstrate the ability to learn complex transition models with bnns we test the runtime efficiency of both fdsatplan and fdblpplan on the learned factored planning problem showing that fdsatplan scales better with increasing bnn size and complexity finally we present a finitetime incremental constraint generation algorithm based on generalized landmark constraints to improve the planning accuracy of our encodings through simulated or realworld interaction less,Compact and Efficient Encodings for Planning in Factored State and Action Spaces with Learned Binarized Neural Network Transition Models,"6 March, 2020"
1308,Scott Sanner,optimal planning with respect to learned neural network nn models in continuous action and state spaces using mixedinteger linear programming milp is a challenging task for branchandbound solvers due to the poor linear relaxation of the underlying milp model for a given set of features potential heuristics provide an efficient framework for computing bounds on cost reward functions in this paper we model the problem of finding optimal potential bounds for learned nn models as a bilevel program and solve it using a novel finitetime constraint generation algorithm we then strengthen the linear relaxation of the underlying milp model by introducing constraints to bound the reward function based on the precomputed reward potentials experimentally we show that our algorithm efficiently computes reward potentials for learned nn models and that the overhead of computing reward potentials is justified by the overall strengthening of the underlying milp model for the task of planning over long horizons less,Reward Potentials for Planning with Learned Neural Network Transition Models,"26 July, 2019"
1309,Scott Sanner,variational autoencoders vaes are a popular generative model but one in which conditional inference can be challenging if the decomposition into query and evidence variables is fixed conditional vaes provide an attractive solution to support arbitrary queries one is generally reduced to markov chain monte carlo sampling methods that can suffer from long mixing times in this paper we propose an idea we term crosscoding to approximate the distribution over the latent variables after conditioning on an evidence assignment to some subset of the variables this allows generating query samples without retraining the full vae we experimentally evaluate three variations of crosscoding showing that i they can be quickly optimized for different decompositions of evidence and query and ii they quantitatively and qualitatively outperform hamiltonian monte carlo less,Conditional Inference in Pre-trained Variational Autoencoders via Cross-coding,"3 October, 2018"
1310,Scott Sanner,given recent deep learning results that demonstrate the ability to effectively optimize highdimensional nonconvex functions with gradient descent optimization on gpus we ask in this paper whether symbolic gradient optimization tools such as tensorflow can be effective for planning in hybrid mixed discrete and continuous nonlinear domains with high dimensional state and action spaces to this end we demonstrate that hybrid planning with tensorflow and rmsprop gradient descent is competitive with mixed integer linear program milp based optimization on piecewise linear planning domains where we can compute optimal solutions and substantially outperforms stateoftheart interior point methods for nonlinear planning domains furthermore we remark that tensorflow is highly scalable converging to a strong plan on a largescale concurrent domain with a total of continuous action parameters distributed over a horizon of time steps and parallel instances in only minutes we provide a number of insights that clarify such strong performance including observations that despite long horizons rmsprop avoids both the vanishing and exploding gradient problems together these results suggest a new frontier for highly scalable planning in nonlinear hybrid domains by leveraging gpus and the power of recent advances in gradient descent with highly optimized toolkits like tensorflow less,Scalable Planning with Tensorflow for Hybrid Nonlinear Domains,"4 November, 2017"
1311,Scott Sanner,lifted probabilistic inference poole and symbolic dynamic programming for lifted stochastic planning boutilier et al were introduced around the same time as algorithmic efforts to use abstraction in stochastic systems over the years these ideas evolved into two distinct lines of research each supported by a rich literature lifted probabilistic inference focused on efficient arithmetic operations on templatebased graphical models under a finite domain assumption while symbolic dynamic programming focused on supporting sequential decisionmaking in rich quantified logical action models and on open domain reasoning given their common motivation but different focal points both lines of research have yielded highly complementary innovations in this chapter we aim to help close the gap between these two research areas by providing an overview of lifted stochastic planning from the perspective of probabilistic inference showing strong connections to other chapters in this book this also allows us to define generalized lifted inference as a paradigm that unifies these areas and elucidates open problems for future research that can benefit both lifted inference and stochastic planning less,Stochastic Planning and Lifted Inference,"4 January, 2017"
1312,Scott Sanner,recent advances in symbolic dynamic programming sdp combined with the extended algebraic decision diagram xadd data structure have provided exact solutions for mixed discrete and continuous hybrid mdps with piecewise linear dynamics and continuous actions since xaddbased exact solutions may grow intractably large for many problems we propose a bounded error compression technique for xadds that involves the solution of a constrained bilinear saddle point problem fortuitously we show that given the special structure of this problem it can be expressed as a bilevel linear programming problem and solved to optimality in finite time via constraint generation despite having an infinite set of constraints this solution permits the use of efficient linear program solvers for xadd compression and enables a novel class of bounded approximate sdp algorithms for hybrid mdps that empirically offers orderofmagnitude speedups over the exact solution in exchange for a small approximation error less,Bounded Approximate Symbolic Dynamic Programming for Hybrid MDPs,"26 September, 2013"
1313,Scott Sanner,recent work on approximate linear programming alp techniques for firstorder markov decision processes fomdps represents the value function linearly wrt a set of firstorder basis functions and uses linear programming techniques to determine suitable weights this approach offers the advantage that it does not require simplification of the firstorder value function and allows one to solve fomdps independent of a specific domain instantiation in this paper we address several questions to enhance the applicability of this work can we extend the firstorder alp framework to approximate policy iteration to address performance deficiencies of previous approaches can we automatically generate basis functions and evaluate their impact on value function quality how can we decompose intractable problems with universally quantified rewards into tractable subproblems we propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on logistics problems from the icaps probabilistic planning competition less,Practical Linear Value-approximation Techniques for First-order MDPs,"27 June, 2012"
1314,Yu Sun,to date the most powerful semisupervised object detectors ssod are based on pseudoboxes which need a sequence of postprocessing with finetuned hyperparameters in this work we propose replacing the sparse pseudoboxes with the dense prediction as a united and straightforward form of pseudolabel compared to the pseudoboxes our dense pseudolabel dpl does not involve any postprocessing method thus retaining richer information we also introduce a region selection technique to highlight the key information while suppressing the noise carried by dense labels we name our proposed ssod algorithm that leverages the dpl as dense teacher on coco and voc dense teacher shows superior performance under various settings compared with the pseudoboxbased methods less,Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection,"6 July, 2022"
1315,Yu Sun,rna structure determination and prediction can promote rnatargeted drug development and engineerable synthetic elements design but due to the intrinsic structural flexibility of rnas all the three mainstream structure determination methods xray crystallography nmr and cryoem encounter challenges when resolving the rna structures which leads to the scarcity of the resolved rna structures computational prediction approaches emerge as complementary to the experimental techniques however none of the textitde novo approaches is based on deep learning since too few structures are available instead most of them apply the timeconsuming samplingbased strategies and their performance seems to hit the plateau in this work we develop the first endtoend deep learning approach eefoldd to accurately perform the textitde novo rna structure prediction several novel components are proposed to overcome the data scarcity such as a fullydifferentiable endtoend pipeline secondary structureassisted selfdistillation and parameterefficient backbone formulation such designs are validated on the independent nonoverlapping rna puzzle testing dataset and reach an average sub rootmeansquare deviation demonstrating its superior performance compared to stateoftheart approaches interestingly it also achieves promising results when predicting rna complex structures a feat that none of the previous systems could accomplish when eefoldd is coupled with the experimental techniques the rna structure prediction field can be greatly advanced less,E2Efold-3D: End-to-End Deep Learning Method for accurate de novo RNA 3D Structure Prediction,"4 July, 2022"
1316,Yu Sun,using ee annihilation data sets collected with the besiii detector we measure the cross sections of the processes ee to ee and ee to at fifteen centerofmass energy points in the vicinity of the j resonance by a simultaneous fit to the measured centerofmass energy dependent cross sections of the two processes the combined quantities ee ee rm tot and ee rm tot are determined to be pm and pm kev respectively where ee and rm tot are the electronic muonic and total decay widths of the j resonance respectively using the resultant ee rm tot and ee ee rm tot the ratio ee is calculated to be pm which is consistent with the expectation of lepton universality within about two standard deviations assuming lepton universality and using the branching fraction of the j leptonic decay measured by besiii in rm tot and ll are determined to be pm and pm kev respectively where ll is the average leptonic decay width of the j resonance less,Measurement of the total and leptonic decay widths of the $J/ψ$ resonance with an energy scan method at BESIII,"27 June, 2022"
1317,Yu Sun,the oscillating light axion field is known as wave dark matter we propose an lcresonance enhanced detection of the narrow band electric signals induced by the axion dark matter using a solenoid magnet facility we provide full d electromagnetic simulation results for the signal electric field the electric signal is enhanced by the high qfactor of a resonant lc circuit and then amplified and detected by the stateoftheart cryogenic electrical transport measurement technique the amplifier noise is the leading noise in the setup we demonstrate that the setup has promising sensitivity for axionic dark matter with mass ma below ev the projected sensitivities increase with the size of the magnetic field and the electric signal measurement can be potentially sensitive to the qcd axion with ga sim gev with a multimeter scale magnetized region less,Resonant Electric Probe to Axionic Dark Matter,"27 June, 2022"
1318,Yu Sun,using a sample of pm times decays collected with the besiii detector at bepcii we report an observation of transverse polarization with a significance of in the decay rightarrowbar rightarrow barrightarrowbar to p bartobarp the relative phase of the electric and magnetic form factors is determined to be pm pm rad this is the first measurement of the relative phase for a decay into a pair of bar hyperons the decay parameters and their conjugates bar bar the angulardistribution parameter and the strongphase difference ps for scattering are measured to be consistent with previous besiii results less,Observation of $Ξ^{-}$ Hyperon Transverse Polarization in $ψ(3686)\rightarrowΞ^{-}\barΞ^{+}$,"22 June, 2022"
1319,Yu Sun,adapting to a continuously evolving environment is a safetycritical challenge inevitably faced by all autonomous driving systems existing image and video driving datasets however fall short of capturing the mutable nature of the real world in this paper we introduce the largest multitask synthetic dataset for autonomous driving shift it presents discrete and continuous shifts in cloudiness rain and fog intensity time of day and vehicle and pedestrian density featuring a comprehensive sensor suite and annotations for several mainstream perception tasks shift allows investigating the degradation of a perception system performance at increasing levels of domain shift fostering the development of continuous adaptation strategies to mitigate this problem and assess model robustness and generality our dataset and benchmark toolkit are publicly available at wwwvisxyzshift less,SHIFT: A Synthetic Driving Dataset for Continuous Multi-Task Domain Adaptation,"16 June, 2022"
1320,Yu Sun,we study damping signatures at the jiangmen underground neutrino observatory juno a mediumbaseline reactor neutrino oscillation experiment these damping signatures are motivated by various new physics models including quantum decoherence decay neutrino absorption and wave packet decoherence the phenomenological effects of these models can be characterized by exponential damping factors at the probability level we assess how well juno can constrain these damping parameters and how to disentangle these different damping signatures at juno compared to current experimental limits juno can significantly improve the limits on m in the decay model the width of the neutrino wave packet x and the intrinsic relative dispersion of neutrino momentum rm rel less,"Damping signatures at JUNO, a medium-baseline reactor neutrino oscillation experiment","14 June, 2022"
1321,Yu Sun,multilabel aspect category detection allows a given review sentence to contain multiple aspect categories which is shown to be more practical in sentiment analysis and attracting increasing attention as annotating large amounts of data is timeconsuming and laborintensive data scarcity occurs frequently in realworld scenarios which motivates multilabel fewshot aspect category detection however research on this problem is still in infancy and few methods are available in this paper we propose a novel labelenhanced prototypical network lpn for multilabel fewshot aspect category detection the highlights of lpn can be summarized as follows first it leverages label description as auxiliary knowledge to learn more discriminative prototypes which can retain aspectrelevant information while eliminating the harmful effect caused by irrelevant aspects second it integrates with contrastive learning which encourages that the sentences with the same aspect label are pulled together in embedding space while simultaneously pushing apart the sentences with different aspect labels in addition it introduces an adaptive multilabel inference module to predict the aspect count in the sentence which is simple yet effective extensive experimental results on three datasets demonstrate that our proposed model lpn can consistently achieve stateoftheart performance less,Label-enhanced Prototypical Network with Contrastive Learning for Multi-label Few-shot Aspect Category Detection,"13 June, 2022"
1322,Andreas Veneris,"Modern vehicles rely on data from a vast array of sensors such as radar and GPS equipment that can be shared with surrounding vehicles and other interested parties. Vehicle-to-everything (V2X) is the collection of systems that enable such communication. Although this data sharing has the potential to improve both the safety and efficiency of vehicles, ensuring that what is shared has not been altered, deleted, forged, leaked, or otherwise tampered with remains a challenging problem. Today, blockchain technology allows a system’s participants to come to an agreement (consensus) on the state of the system and its data in a decentralized, trustless manner. ",Blockchain for V2X: Applications and Architectures,2022-05-05 00:00:00
1323,Andreas Veneris,"With the emergence of decentralized finance, smart contracts and their users become more and more susceptible to expensive exploitations. This paper investigates the price gouging transaction order dependency vulnerabilities in smart contracts. A static analysis based approach is proposed to automatically locate and rectify such vulnerabilities, and a prototype tool using Slither, a static analyzer for Solidity, is also developed. All in all, empirical results on a benchmark suite containing 51 Solidity smart contracts show that the proposed methodology can be used successfully to both detect such vulnerabilities and rectify them, or to certify that a Solidity smart contract under question does not contain such vulnerabilities.",Automated Auditing of Price Gouging TOD Vulnerabilities in Smart Contracts,2022-05-02 00:00:00
1324,Andreas Veneris,"We present the Layered Merkle Patricia Trie (LMPT), a performant storage data structure for processing transactions in high-throughput systems when com-pared to traditional Merkle Patricia Tries used in Ethereum clients. LMPTs keep smaller intermediary tries in memory to alleviate read and write amplification from high-latency disk storage. As an additional feat, they also allow for the I/O and transaction verifier threads to be scheduled in parallel and independently. LMPTs can ultimately reduce significant I/O traffic that happens on the critical path of transaction processing. Empirical results presented here confirm that LMPTs can process up to × 6 more transactions per second on real-life workloads when compared to existing Ethereum clients.",LMPTs: Eliminating Storage Bottlenecks for Processing Blockchain Transactions,2022-05-02 00:00:00
1325,Andreas Veneris,"A volatility surface is an important tool for pricing and hedging derivatives. The surface shows the volatility that is implied by the market price of an option on an asset as a function of the option’s strike price and maturity. Often, market data are incomplete, and it is necessary to estimate missing points on partially observed surfaces. In this article, the authors show how variational autoencoders can be used to model volatility surfaces. The first step is to train the model, deriving latent variables that can be used to construct synthetic volatility surfaces that are indistinguishable from those observed historically. The second step is to determine the synthetic surface generated by the latent variables that fits available data as closely as possible. The trained variational autoencoder can also be used to generate synthetic-yet-realistic surfaces, which can be used in stress testing, in market simulators for developing quantitative ",Variational autoencoders: A hands-off approach to volatility,2022-04-30 00:00:00
1326,Andreas Veneris,"Blockchain systems rely on oracles to bridge external information to the decentralized applications residing in the systems. Astraea protocols are decentralized oracle designs utilizing majority‐voting mechanism to determine the oracle outcomes and/or rewards to voters. However, the voters are indifferent between voting through a single or multiple identities, as the potential rewards by the decentralized oracles grow linearly with the voters stakes. Additionally, the majority‐voting mechanism may facilitate herd behaviors among the voters, as the voters are rewarded only if they are in agreement with the majority outcomes. In this paper, a novel oracle protocol is introduced by proposing a peer prediction‐based scoring scheme along with non‐linear staking rules, aiming at extracting subjective data truthfully. Specifically, an incentive compatible scoring scheme is designed so that voters uniquely maximize their ",Truthful decentralized blockchain oracles,2022/3
1327,Andreas Veneris,"Central banks and governments all over the world are increasingly exploring digital versions of fiat money, known as retail Central Bank Digital Currencies (CBDCs). Most initiatives rely on Distributed Ledger Technologies and are presented as alternatives to physical cash. Consequently, anonymity-related regulatory questions have naturally started to arise in terms of Anti-Money Laundering and Counter-Terrorist Financing compliance. Against this backdrop, this paper provides a techno-legal taxonomy of approaches to balance privacy and transparency in CBDCs without thwarting accountability, but it also underlines cross-sectoral impacts. The contribution heeds regulation-by-design as its core methodological foundation, with Privacy-Enhancing Technologies as the relevant use case. Thus, it highlights that not only technology aids legal purposes, but also that some regulatory requirements ought to be ",Privacy and transparency in cbdcs: A regulation-by-design aml/cft scheme,2021-12-21 00:00:00
1328,Andreas Veneris,"Smart contracts facilitate the execution of programmable code on a blockchain. The cost for executing smart contract code is metered using gas - the exact amount of which is based on the computational complexity of the underlying smart contract. Hence, it is imperative to optimize smart contract code to reduce gas consumption and, in some instances, to even avoid malicious attacks. In this paper, we propose an approach to optimize the gas consumption of smart contracts, specifically loop control structures. We present a prototype implementation of our approach using off-the-shelf tools for Solidity smart contracts. We experimentally evaluate our technique using 72 Solidity smart contracts. Our evaluation demonstrates the average gas cost savings per transaction to be around 23,943 gas units, or an equivalent 21% decrease in gas costs. Although the approach causes a slight increase in deployment costs due to",Smart Contracts Refinement for Gas Optimization,2021-09-27 00:00:00
1329,Andreas Veneris,"Vehicles today contain a multitude of sensors creating vast amounts of data. For many applications, these data need to be shared with other entities so that they can also utilize it. Vehicle-to-everything (V2X) is the amalgamation of all potential vehicle communication systems. V2X technologies are enabling many smart-vehicle applications, such as autonomous vehicles. However, in utilizing these data from external entities, vehicles rely on the availability and trustworthiness of centralized entities who may be able to delete, forge, leak, or otherwise tamper with the underlying data. Blockchain technology provides a decentralized mechanism to allow vehicles to validate data they receive in a trustless manner. This paper explores potential applications of blockchain technology in the V2X space, categorizing and analyzing use cases based on their underlying blockchain requirements. It then uses this analysis to ",Blockchain for V2X: A taxonomy of design use cases and system requirements,2021-09-27 00:00:00
1330,Andreas Veneris,"Billions of Internet of Things (IoT) devices deployed today collect massive amounts of potentially valuable data. To efficiently utilize this data, markets must be developed where data can be traded in real time. Blockchain technology offers a potential platform for these types of markets. However, previous proposals using blockchain technology either require trusted third parties such as data brokers, or necessitate a large number of on-chain transactions to operate, incurring excessive overhead costs. This paper proposes a trustless data trading system that minimizes both the risk of fraud and the number of transactions performed on chain. In this system, data producers and consumers come to binding agreements while trading data off chain and they only settle on chain when a deposit or withdrawal of funds is required. A credit mechanism is also developed to further reduce the incurred fees. Additionally, the",Cost-effective blockchain-based iot data marketplaces with a credit invariant,2021-05-03 00:00:00
1331,Andreas Veneris,"This paper presents SigVM, a novel blockchain virtual machine that supports an event-driven execution model, enabling developers to build fully autonomous smart contracts. SigVM introduces another way for a contract to interact with another. Contracts in SigVM can emit signal events, on which other contracts can listen. Once an event is triggered, corresponding handler functions are automatically executed as signal transactions. We built an end-to-end blockchain platform SigChain and a contract language compiler SigSolid to realize the potential of SigVM. Experimental results show that SigVM enables contracts in our benchmark applications to be reimplemented in a fully autonomous way, eliminating the dependency on unreliable mechanisms like off-chain relay servers. SigVM can significantly simplify the execution flow of our benchmark applications, and can avoid security risks such as front-run attacks.",SigVM: Toward Fully Autonomous Smart Contracts,2021-02-22 00:00:00
1332,Andreas Veneris,"Global economic digitization continues to advance at exponential speed. This development is in sharp contrast to the financial sector and payment systems that still operate on legacy infrastructure that lacks the flexibility to serve those technology needs. Further, the emergence of Decentralized Finance demonstrates the capacity to disrupt the financial sector, impact national sovereignty, and affect established monetary transmission channels. Hence, it is no surprise that nation-states and tech-firms alike are now building new digital infrastructures that circumvent the legacy practices. Central banks, in particular, are racing to explore the issuance of Central Bank-issued Digital Currencies (CBDCs) in an attempt to rediscover the very essence and use of fiat cash.",Central bank digital loonie: Canadian cash for a new global economy,2021-02-11 00:00:00
1333,Andreas Veneris,"This paper presents SigVM, a novel blockchain virtual machine that supports an event-driven execution model, enabling developers to build autonomous smart contracts. Contracts in SigVM can emit signal events, on which other contracts can listen. Once an event is triggered, corresponding handler functions are automatically executed as signal transactions. We build an end-to-end blockchain platform SigChain and a contract language compiler SigSolid to realize the potential of SigVM. Experimental results show that our benchmark applications can be reimplemented with SigVM in an autonomous way, eliminating the dependency on unreliable mechanisms like off-chain relay servers. The development effort of reimplementing these contracts with SigVM is small, ie, we modified on average 2.6% of the contract code.",SigVM: Enabling Event-Driven Execution for Autonomous Smart Contracts,2021/2
1334,Andreas Veneris,"Proof-of-work blockchains need to be carefully designed so as to create the proper incentives for miners to faithfully maintain the network in a sustainable way. This paper describes how the economic engineering of the Conflux Network, a high throughput proof-of-work blockchain, leads to sound economic incentives that support desirable and sustainable mining behavior. In detail, this paper parameterizes the level of income, and thus network security, that Conflux can generate, and it describes how this depends on user behavior and “policy variables” such as block and interest inflation. It also discusses how the underlying economic engineering design makes the Conflux Network resilient against double spending and selfish mining attacks",Engineering economics in the conflux network,2020-09-28 00:00:00
1335,Andreas Veneris,"Many blockchain applications use decentralized oracles to trustlessly retrieve external information as those platforms are agnostic to real-world information. Some existing decentralized oracle protocols make use of majority-voting schemes to determine the outcomes and/or rewards to participants. In these cases, the awards (or penalties) grow linearly to the participant stakes, therefore voters are indifferent between voting through a single or multiple identities. Furthermore, the voters receive a reward only when they agree with the majority outcome, a tactic that may lead to herd behavior. This paper proposes an oracle protocol based on peer prediction mechanisms with non-linear staking rules. In the proposed approach, instead of being rewarded when agreeing with a majority outcome, a voter receives awards when their report achieves a relatively high score based on a peer prediction scoring scheme.",A truth-inducing sybil resistant decentralized blockchain oracle,2020-09-28 00:00:00
1336,Andreas Veneris,this paper presents sigvm a novel blockchain virtual machine that supports an eventdriven execution model enabling developers to build autonomous smart contracts contracts in sigvm can emit signal events on which other contracts can listen once an event is triggered corresponding handler functions are automatically executed as signal transactions we build an endtoend blockchain platform sigchain and a contract language compiler sigsolid to realize the potential of sigvm experimental results show that our benchmark applications can be reimplemented with sigvm in an autonomous way eliminating the dependency on unreliable mechanisms like offchain relay servers the development effort of reimplementing these contracts with sigvm is small ie we modified on average of the contract code less,SigVM: Enabling Event-Driven Execution for Autonomous Smart Contracts,"17 November, 2021"
1337,Eric Yu,we present the detection potential for the diffuse supernova neutrino background dsnb at the jiangmen underground neutrino observatory juno using the inversebetadecay ibd detection channel on free protons we employ the latest information on the dsnb flux predictions and investigate in detail the background and its reduction for the dsnb search at juno the atmospheric neutrino induced neutral current nc background turns out to be the most critical background whose uncertainty is carefully evaluated from both the spread of model predictions and an envisaged textitin situ measurement we also make a careful study on the background suppression with the pulse shape discrimination psd and triple coincidence tc cuts with latest dsnb signal predictions more realistic background evaluation and psd efficiency optimization and additional tc cut juno can reach the significance of for years of data taking and achieve better than after years for a reference dsnb model in the pessimistic scenario of nonobservation juno would strongly improve the limits and exclude a significant region of the model parameter space less,Prospects for Detecting the Diffuse Supernova Neutrino Background with JUNO,"18 May, 2022"
1338,Eric Yu,although it is generally accepted that massive galaxies form in a twophased fashion beginning with a rapid mass buildup through intense starburst activities followed by primarily dry mergers that mainly deposit stellar mass at outskirts the late time stellar mass growth of brightest cluster galaxies bcgs the most massive galaxies in the universe is still not well understood several independent measurements have indicated a slower mass growth rate than predictions from theoretical models we attempt to resolve the discrepancy by measuring the frequency of bcgs with multiplecores which serve as a proxy of the merger rates in the central region and facilitate a more direct comparison with theoretical predictions using bcgs at z with integral field spectroscopic ifs data from the mapping nearby galaxies at apo manga project we obtain a multiplecore fraction of pm at zapprox within a kpc radius from the center which is comparable to the value of pm derived from mock observations of simulated bcgs from the cosmological hydrodynamical simulation illustristng we find that most of cores that appear close to the bcgs from imaging data turn out to be physically associated systems anchoring on the similarity in the multiplecore frequency between the manga and illustristng we discuss the mass growth rate of bcgs over the past gyr less,SDSS-IV MaNGA: Cannibalism Caught in the Act -- on the Frequency of Occurrence of Multiple Cores in Brightest Cluster Galaxies,"16 May, 2022"
1339,Eric Yu,this snowmass white paper describes the cosmic microwave background stage project cmbs which is designed to cross critical thresholds in our understanding of the origin and evolution of the universe from the highest energies at the dawn of time through the growth of structure to the present day we provide an overview of the science case the technical design and project plan less,Snowmass 2021 CMB-S4 White Paper,"15 March, 2022"
1340,Eric Yu,distributionally robust supervised learning drsl is emerging as a key paradigm for building reliable machine learning systems for realworld applications reflecting the need for classifiers and predictive models that are robust to the distribution shifts that arise from phenomena such as selection bias or nonstationarity existing algorithms for solving wasserstein drsl one of the most popular drsl frameworks based around robustness to perturbations in the wasserstein distance have serious limitations that limit their use in largescale problems in particular they involve solving complex subproblems and they fail to make use of stochastic gradients we revisit wasserstein drsl through the lens of minmax optimization and derive scalable and efficiently implementable stochastic extragradient algorithms which provably achieve faster convergence rates than existing approaches we demonstrate their effectiveness on synthetic and real data when compared to existing drsl approaches key to our results is the use of variance reduction and random reshuffling to accelerate stochastic minmax optimization the analysis of which may be of independent interest less,Fast Distributionally Robust Learning with Variance Reduced Min-Max Optimization,"25 January, 2022"
1341,Eric Yu,the serappis search for rare ppneutrinos in scintillator project aims at a precision measurement of the flux of solar pp neutrinos on the fewpercent level such a measurement will be a relevant contribution to the study of solar neutrino oscillation parameters and a sensitive test of the solar luminosity constraint the concept of serappis relies on a small organic liquid scintillator detector sim m with excellent energy resolution sim at mev low internal background and sufficient shielding from surrounding radioactivity this can be achieved by a minor upgrade of the osiris facility at the site of the juno neutrino experiment in southern china to go substantially beyond current accuracy levels for the pp flux an organic scintillator with ultralow c levels below is required the existing osiris detector and juno infrastructure will be instrumental in identifying suitable scintillator materials offering a unique chance for a lowbudget highprecision measurement of a fundamental property of our sun that will be otherwise hard to access less,Potential for a precision measurement of solar $pp$ neutrinos in the Serappis Experiment,"18 January, 2022"
1342,Eric Yu,sno is a largescale liquid scintillator experiment with the primary goal of searching for neutrinoless double beta decay and is located approximately km underground in snolab sudbury canada the detector acquired data for two years as a pure water cherenkov detector starting in may during this period the optical properties of the detector were measured in situ using a deployed light diffusing sphere with the goal of improving the detector model and the energy response systematic uncertainties the measured parameters included the water attenuation coefficients effective attenuation coefficients for the acrylic vessel and the angular response of the photomultiplier tubes and their surrounding light concentrators all across different wavelengths the calibrated detector model was validated using a deployed tagged gamma source which showed a variation in energy scale across the primary target volume less,Optical calibration of the SNO+ detector in the water phase with deployed sources,"4 October, 2021"
1343,Eric Yu,aerial robot solutions are becoming ubiquitous for an increasing number of tasks among the various types of aerial robots blimps are very well suited to perform longduration tasks while being energy efficient relatively silent and safe to address the blimp navigation and control task in our recent work we have developed a softwareintheloop simulation and a pidbased controller for large blimps in the presence of wind disturbance however blimps have a deformable structure and their dynamics are inherently nonlinear and timedelayed often resulting in large trajectory tracking errors moreover the buoyancy of a blimp is constantly changing due to changes in the ambient temperature and pressure in the present paper we explore a deep reinforcement learning drl approach to address these issues we train only in simulation while keeping conditions as close as possible to the realworld scenario we derive a compact state representation to reduce the training time and a discrete action space to enforce control smoothness our initial results in simulation show a significant potential of drl in solving the blimp control task and robustness against moderate wind and parameter uncertainty extensive experiments are presented to study the robustness of our approach we also openly provide the source code of our approach less,Autonomous Blimp Control using Deep Reinforcement Learning,"27 September, 2021"
1344,Eric Yu,the determinants of pm matrices are calculated by via the oriented hypergraphic laplacian and summing over an incidence generalization of vertex cyclecovers these cyclecovers are signed and partitioned into families based on their hyperedge containment every nonedgemonic family is shown to contribute a net value of to the laplacian while each edgemonic family is shown to sum to the absolute value of the determinant of the original incidence matrix simple symmetries are identified as well as their relationship to hadamards maximum determinant problem finally the entries of the incidence matrix are reclaimed using only the signs of an adjacencyminimal set of cyclecovers from an edgemonic family less,The Determinant of $\{\pm 1\}$-Matrices and Oriented Hypergraphs,"29 June, 2021"
1345,Eric Yu,in this paper we propose a novel variable selection approach in the framework of highdimensional linear models where the columns of the design matrix are highly correlated it consists in rewriting the initial highdimensional linear model to remove the correlation between the columns of the design matrix and in applying a generalized elastic net criterion since it can be seen as an extension of the generalized lasso the properties of our approach called gen generalized elastic net are investigated both from a theoretical and a numerical point of view more precisely we provide a new condition called gic generalized irrepresentable condition which generalizes the eic elastic net irrepresentable condition of jia and yu under which we prove that our estimator can recover the positions of the null and non null entries of the coefficients when the sample size tends to infinity we also assess the performance of our methodology using synthetic data and compare it with alternative approaches our numerical experiments show that our approach improves the variable selection performance in many cases less,Sign Consistency of the Generalized Elastic Net Estimator,"9 June, 2021"
1346,Eric Yu,the june impact of asteroid la over botswana is only the second asteroid detected in space prior to impacting over land here we report on the successful recovery of meteorites additional astrometric data refine the approach orbit and define the spin period and shape of the asteroid video observations of the fireball constrain the asteroids position in its orbit and were used to triangulate the location of the fireballs main flare over the central kalahari game reserve meteorites were recovered a consortium study of eight of these classifies motopi pan as a hed polymict breccia derived from howardite cumulate and basaltic eucrite and diogenite lithologies before impact la was a solid rock of about cm diameter with high bulk density about gcm a relatively low albedo pv about no significant opposition effect on the asteroid brightness and an impact kinetic energy of about kt the orbit of la is consistent with an origin at vesta or its vestoids and delivery into an earthimpacting orbit via the nu resonance the impact that ejected la in an orbit towards earth occurred ma ago zircons record a concordant upb age of ma and a consistent pbpb age of ma a much younger pbpb phosphate resetting age of ma was found from this impact chronology we discuss what is the possible source crater of motopi pan and the age of vestas veneneia impact basin less,The impact and recovery of asteroid 2018 LA,"12 May, 2021"
1347,William Yu,in this paper we present a reanalysis of supercdms data using a profile likelihood approach to search for subgev dark matter particles dm through two inelastic scattering channels bremsstrahlung radiation and the migdal effect by considering possible inelastic scattering channels experimental sensitivity can be extended to dm masses that would otherwise be undetectable through the dmnucleon elastic scattering channel given the energy threshold of current experiments we exclude dm masses down to textrmmevc at times textrmcm via the bremsstrahlung channel the migdal channel search excludes dm masses down to textrmmevc at times textrmcm less,A Search for Low-mass Dark Matter via Bremsstrahlung Radiation and the Migdal Effect in SuperCDMS,"19 May, 2022"
1348,William Yu,temporal expression extraction tee is essential for understanding time in natural language it has applications in natural language processing nlp tasks such as question answering information retrieval and causal inference to date work in this area has mostly focused on english as there is a scarcity of labeled data for other languages we propose xltime a novel framework for multilingual tee xltime works on top of pretrained language models and leverages multitask learning to prompt crosslanguage knowledge transfer both from english and within the nonenglish languages xltime alleviates problems caused by a shortage of data in the target language we apply xltime with different language models and show that it outperforms the previous automatic sota methods on french spanish portuguese and basque by large margins xltime also closes the gap considerably on the handcrafted heideltime method less,XLTime: A Cross-Lingual Knowledge Transfer Framework for Temporal Expression Extraction,"3 May, 2022"
1349,William Yu,advanced wearable devices are increasingly incorporating highresolution multicamera systems as stateoftheart neural networks for processing the resulting image data are computationally demanding there has been growing interest in leveraging fifth generation g wireless connectivity and mobile edge computing for offloading this processing to the cloud to assess this possibility this paper presents a detailed simulation and evaluation of g wireless offloading for object detection within a powerful new smart wearable called vision for the blindandvisually impaired bvi the current vision system is an instrumented bookbag with highresolution cameras vision processing and haptic and audio feedback the paper considers uploading the camera data to a mobile edge cloud to perform realtime object detection and transmitting the detection results back to the wearable to determine the video requirements the paper evaluates the impact of video bit rate and resolution on object detection accuracy and range a new street scene dataset with labeled objects relevant to bvi navigation is leveraged for analysis the vision evaluation is combined with a detailed fullstack wireless network simulation to determine the distribution of throughputs and delays with real navigation paths and raytracing from new highresolution d models in an urban environment for comparison the wireless simulation considers both a standard glong term evolution lte carrier and highrate g millimeterwave mmwave carrier the work thus provides a thorough and realistic assessment of edge computing with mmwave connectivity in an application with both high bandwidth and low latency requirements less,"Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to ""See"" More, Farther and Faster","15 April, 2022"
1350,William Yu,we present the findings of the alzheimers disease prediction of longitudinal evolution tadpole challenge which compared the performance of algorithms from international teams at predicting the future trajectory of individuals at risk of alzheimers disease challenge participants were required to make a prediction for each month of a year future time period of three key outcomes clinical diagnosis alzheimers disease assessment scale cognitive subdomain adascog and total volume of the ventricles the methods used by challenge participants included multivariate linear regression machine learning methods such as support vector machines and deep neural networks as well as disease progression models no single submission was best at predicting all three outcomes for clinical diagnosis and ventricle volume prediction the best algorithms strongly outperform simple baselines in predictive ability however for adascog no single submitted prediction method was significantly better than random guesswork two ensemble methods based on taking the mean and median over all predictions obtained top scores on almost all tasks better than average performance at diagnosis prediction was generally associated with the additional inclusion of features from cerebrospinal fluid csf samples and diffusion tensor imaging dti on the other hand better performance at ventricle volume prediction was associated with inclusion of summary statistics such as the slope or maximaminima of biomarkers tadpoles unique results suggest that current prediction algorithms provide sufficient accuracy to exploit biomarkers related to clinical diagnosis and ventricle volume for cohort refinement in clinical trials for alzheimers disease however results call into question the usage of cognitive test scores for patient selection and as a primary endpoint in clinical trials less,The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE) Challenge: Results after 1 Year Follow-up,"27 December, 2021"
1351,William Yu,we summarize the results of a host of efforts using giant automatic speech recognition asr models pretrained using large diverse unlabeled datasets containing approximately a million hours of audio we find that the combination of pretraining selftraining and scaling up model size greatly increases data efficiency even for extremely large tasks with tens of thousands of hours of labeled data in particular on an asr task with k hours of labeled data by finetuning an billion parameter pretrained conformer model we can match stateoftheart sota performance with only of the training data and significantly improve sota with the full training set we also report on the universal benefits gained from using big pretrained and selftrained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes including obtaining sota performance on many public benchmarks in addition we utilize the learned representation of pretrained networks to achieve sota results on nonasr tasks less,BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition,"1 October, 2021"
1352,William Yu,characterising the inputoutput photonnumber distribution of an unknown optical quantum channel is an important task for many applications in quantum information processing ideally this would require deterministic photonnumber sources and photonnumberresolving detectors but these technologies are still workinprogress in this work we propose a general method to rigorously bound the inputoutput photon number distribution of an unknown optical channel using standard optical devices such as coherent light sources and nonphotonnumberresolving detectorshomodyne detectors to demonstrate the broad utility of our method we consider the security analysis of practical quantum key distribution systems based on calibrated singlephoton detectors and an experimental proposal to implement timecorrelated single photon counting technology using homodyne detectors instead of singlephoton detectors less,Estimating the photon-number distribution of photonic channels with realistic devices and applications in photonic quantum information processing,"10 September, 2021"
1353,William Yu,we report results from continued timing observations of psr j a highmass ms radio pulsar in orbit with a likely ultracool white dwarf companion our data set consists of combined pulse arrivaltime measurements made with the m green bank telescope and the canadian hydrogen intensity mapping experiment telescope we explore the significance of timingbased phenomena arising from generalrelativistic dynamics and variations in pulse dispersion when using various statistical methods we find that combining sim years of additional highcadence timing data with previous measurements confirms and improves upon previous estimates of relativistic effects within the psr j system with the pulsar mass mrm p modot credibility determined by the relativistic shapiro time delay for the first time we measure secular variation in the orbital period and argue that this effect arises from apparent acceleration due to significant transverse motion after incorporating contributions from galactic differential rotation and offplane acceleration in the galactic potential we obtain a modeldependent distance of d kpc credibility this improved distance confirms the ultracool nature of the white dwarf companion determined from recent optical observations we discuss the prospects for future observations with nextgeneration facilities which will likely improve the precision on mrm p for j by an order of magnitude within the next few years less,Refined Mass and Geometric Measurements of the High-Mass PSR J0740+6620,"6 July, 2021"
1354,William Yu,the june impact of asteroid la over botswana is only the second asteroid detected in space prior to impacting over land here we report on the successful recovery of meteorites additional astrometric data refine the approach orbit and define the spin period and shape of the asteroid video observations of the fireball constrain the asteroids position in its orbit and were used to triangulate the location of the fireballs main flare over the central kalahari game reserve meteorites were recovered a consortium study of eight of these classifies motopi pan as a hed polymict breccia derived from howardite cumulate and basaltic eucrite and diogenite lithologies before impact la was a solid rock of about cm diameter with high bulk density about gcm a relatively low albedo pv about no significant opposition effect on the asteroid brightness and an impact kinetic energy of about kt the orbit of la is consistent with an origin at vesta or its vestoids and delivery into an earthimpacting orbit via the nu resonance the impact that ejected la in an orbit towards earth occurred ma ago zircons record a concordant upb age of ma and a consistent pbpb age of ma a much younger pbpb phosphate resetting age of ma was found from this impact chronology we discuss what is the possible source crater of motopi pan and the age of vestas veneneia impact basin less,The impact and recovery of asteroid 2018 LA,"12 May, 2021"
1355,Ding Yuan,the pandora software development kit and algorithm libraries provide patternrecognition logic essential to the reconstruction of particle interactions in liquid argon time projection chamber detectors pandora is the primary event reconstruction software used at protodunesp a prototype for the deep underground neutrino experiment far detector protodunesp located at cern is exposed to a chargedparticle test beam this paper gives an overview of the pandora reconstruction algorithms and how they have been tailored for use at protodunesp in complex events with numerous cosmicray and beam background particles the simulated reconstruction and identification efficiency for triggered testbeam particles is above for the majority of particle type and beam momentum combinations specifically simulated gevc charged pions and protons are correctly reconstructed and identified with efficiencies of pm and pm respectively the efficiencies measured for testbeam data are shown to be within of those predicted by the simulation less,Reconstruction of interactions in the ProtoDUNE-SP detector with Pandora,"29 June, 2022"
1356,Ding Yuan,the dark energy spectroscopic instrument desi has embarked on an ambitious fiveyear survey to explore the nature of dark energy with spectroscopy of million galaxies and quasars desi will determine precise redshifts and employ the baryon acoustic oscillation method to measure distances from the nearby universe to z as well as measure the growth of structure and probe potential modifications to general relativity in this paper we describe the significant instrumentation we developed for the desi survey the new instrumentation includes a widefield deg diameter primefocus corrector that focuses the light onto robotic fiber positioners on the m diameter aspheric focal surface the positioners and their fibers are divided among ten wedgeshaped petals each petal is connected to one of ten spectrographs via a contiguous highefficiency nearly m fiber cable bundle the ten spectrographs each use a pair of dichroics to split the light into three channels that together record the light from nm with a resolution of to we describe the science requirements technical requirements on the instrumentation and management of the project desi was installed at the m mayall telescope at kitt peak and we also describe the facility upgrades to prepare for desi and the installation and functional verification process desi has achieved all of its performance goals and the desi survey began in may some performance highlights include rms positioner accuracy better than snr per sqrt for a z quasar with flux e ergscma at nm in s and median snr of the oii doublet at e ergscm in a s exposure for emission line galaxies at z we conclude with highlights from the onsky validation and commissioning of the instrument key successes and lessons learned abridged less,Overview of the Instrumentation for the Dark Energy Spectroscopic Instrument,"22 May, 2022"
1357,Ding Yuan,main goal of the juno experiment is to determine the neutrino mass ordering using a kt liquidscintillator detector its key feature is an excellent energy resolution of at least at mev for which its instruments need to meet a certain quality and thus have to be fully characterized more than inch pmts have been received and assessed by juno after a detailed testing program which began in and elapsed for about four years based on this mass characterization and a set of specific requirements a good quality of all accepted pmts could be ascertained this paper presents the performed testing procedure with the designed testing systems as well as the statistical characteristics of all inch pmts intended to be used in the juno experiment covering more than fifteen performance parameters including the photocathode uniformity this constitutes the largest sample of inch pmts ever produced and studied in detail to date ie of the newly developed inch mcppmts from northern night vision technology co nnvt and of dynode pmts from hamamatsu photonics k khpk less,Mass Testing and Characterization of 20-inch PMTs for JUNO,"17 May, 2022"
1358,Ding Yuan,juno is a multipurpose neutrino observatory under construction in the south of china this publication presents new sensitivity estimates for the measurement of the m m sin and sin oscillation parameters using reactor antineutrinos which is one of the primary physics goals of the experiment the sensitivities are obtained using the best knowledge available to date on the location and overburden of the experimental site the nuclear reactors in the surrounding area and beyond the detector response uncertainties and the reactor antineutrino spectral shape constraints expected from the tao satellite detector it is found that the m m and sin oscillation parameters will be determined to better than precision in six years of data collection which represents approximately an order of magnitude improvement over existing constraints less,Sub-percent Precision Measurement of Neutrino Oscillation Parameters with JUNO,"27 April, 2022"
1359,Ding Yuan,using ee annihilation data corresponding to an integrated luminosity of fb collected at centerofmass energies between gev and gev with the besiii detector we perform the first amplitude analysis of the decay dsto ksk and determine the relative branching fractions and phases for intermediate processes we observe the a the isovector partner of the f and f mesons in its decay to ksk for the first time in addition we measure the ratio fracmathcalbds to barkkmathcalbds to barkk to be textstatpm rm syst finally we provide a precision measurement of the absolute branching fraction mathcalbdsto ksk pm textstatpm textsyst less,Observation of $a_0(1710)^+ \to K_S^0K^+$ in study of the $D_s^+\to K_S^0K^+π^0$ decay,"20 April, 2022"
1360,Zhaolei Zhang,"RNA molecules can fold into complex and stable 3D structures, allowing them to carry out important genetic, structural, and regulatory roles inside the cell. These complex structures often contain 3D pockets made up of secondary structural motifs that can be potentially targeted by small molecule ligands. Indeed, many RNA structures in PDB contain bound small molecules, and high-throughput experimental studies have generated a large number of interacting RNA and ligand pairs. There is considerable interest in developing small molecule lead compounds targeting viral RNAs or those RNAs implicated in neurological diseases or cancer. We hypothesize that RNAs that have similar secondary structural motifs may bind to similar small molecule ligands. Toward this goal, we established a database collecting RNA secondary structural motifs and bound small molecule ligands. We further developed a ",RNALigands: a database and web server for RNA–ligand interactions,2022-02-01 00:00:00
1361,Zhaolei Zhang,"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) nucleocapsid (N) protein is essential for viral replication, making it a promising target for antiviral drug and vaccine development. SARS-CoV-2 infected patients exhibit an uncoordinated immune response; however, the underlying mechanistic details of this imbalance remain obscure. Here, starting from a functional proteomics workflow, we cataloged the protein–protein interactions of SARS-CoV-2 proteins, including an evolutionarily conserved specific interaction of N with the stress granule resident proteins G3BP1 and G3BP2. N localizes to stress granules and sequesters G3BPs away from their typical interaction partners, thus attenuating stress granule formation. We found that N binds directly to host mRNAs in cells, with a preference for 3′ UTRs, and modulates target mRNA stability. We show that the N protein rewires the G3BP1 mRNA-binding ",SARS-CoV-2 nucleocapsid protein binds host mRNAs and attenuates stress granules to impair host stress response,2022-01-21 00:00:00
1362,Zhaolei Zhang,"Different subtypes of the same cancer often show distinct genomic signatures and require targeted treatments. The differences at the cellular and molecular levels of tumor microenvironment in different cancer subtypes have significant effects on tumor pathogenesis and prognostic outcomes. Although there have been significant researches on the prognostic association of tumor infiltrating lymphocytes in selected histological subtypes, few investigations have systemically reported the prognostic impacts of immune cells in molecular subtypes, as quantified by machine learning approaches on multi-omics datasets. This paper describes a new computational framework, ProTICS, to quantify the differences in the proportion of immune cells in tumor microenvironment and estimate their prognostic effects in different subtypes. First, we stratified patients into molecular subtypes based on gene expression and ",ProTICS reveals prognostic impact of tumor infiltrating immune cells in different molecular subtypes,2021/11
1363,Zhaolei Zhang,"De-identification is a fundamental task in electronic health records to remove protected health information entities. Deep learning models have proven to be promising tools to automate de-identification processes. However, when the target domain (where the model is applied) is different from the source domain (where the model is trained), the model often suffers a significant performance drop, commonly referred to as domain adaptation issue. In de-identification, domain adaptation issues can make the model vulnerable for deployment. In this work, we aim to close the domain gap by leveraging unlabeled data from the target domain.",Improving domain adaptation in de-identification of electronic health records through self-training,2021/10
1364,Zhaolei Zhang,"The impact of adverse risk genetic profiles on outcomes in acute myeloid leukemia (AML) patients following allogeneic hematopoietic stem cell transplantation (HCT) has not been fully elucidated. Accordingly, we have profiled somatic mutations at diagnosis using next-generation sequencing (NGS) in 178 AML patients who received allogeneic HCT. NGS revealed 598 somatic mutations in 165/178 patients (92.7%). Frequently mutated genes include DNMT3A, TET2, NPM1, RUNX1, IDH2, and FLT3. Commonly detected cytogenetic profiles include normal karyotype, trisomy 8, monosomal karyotype (MK), deletion 5, complex karyotype (CK), and monosomy 7. In univariate analyses, TP53 mutation, MK, CK, and monosomy 7 were associated with decreased overall survival (OS), relapse-free survival (RFS), and a higher relapse incidence (RI). We defined adverse molecular-genetic profile as harboring at least one of ",Prognostic impact of the adverse molecular-genetic profile on long-term outcomes following allogeneic hematopoietic stem cell transplantation in acute myeloid leukemia,2021/8
1365,Zhaolei Zhang,"Retinoblastoma-binding proteins 4 and 7 (RBBP4 and RBBP7) are two highly homologous human histone chaperones. They function in epigenetic regulation as subunits of multiple chromatin-related complexes and have been implicated in numerous cancers. Due to their overlapping functions, our understanding of RBBP4 and 7, particularly outside of Opisthokonts, has remained limited. Here, we report that in the ciliate protozoan Tetrahymena thermophila a single orthologue of human RBBP4 and 7 proteins, RebL1, physically interacts with histone H4 and functions in multiple epigenetic regulatory pathways. Functional proteomics identified conserved functional links for Tetrahymena RebL1 protein as well as human RBBP4 and 7. We found that putative subunits of multiple chromatin-related complexes including CAF1, Hat1, Rpd3, and MuvB, co-purified with RebL1 during Tetrahymena growth",Functional characterization of RebL1 highlights the evolutionary conservation of oncogenic activities of the RBBP4/7 orthologue in Tetrahymena thermophila,2021-06-21 00:00:00
1366,Zhaolei Zhang,"In patients with acute myeloid leukemia (AML) consolidation treatment options are between allogeneic hematopoietic stem cell transplantation (HCT) and chemotherapy, based on disease risk at the time of initial presentation and age. Measurable residual disease (MRD) following induction chemotherapy could be incorporated as a useful parameter for treatment decisions. The present study evaluated treatment outcomes according to the next-generation sequencing (NGS)-based MRD status and the type of consolidation therapy in patients with normal karyotype (NK)-AML. By sequencing 278 paired samples collected at diagnosis and first remission (CR1), we identified 361 mutations in 124 patients at diagnosis and tracked these at CR1. After excluding mutations associated with age-related clonal hematopoiesis, 82 mutations in 50 of the 124 patients (40.3%) were detected at CR1. Survival benefit was observed ",Allogeneic transplant can abrogate the risk of relapse in the patients of first remission acute myeloid leukemia with detectable measurable residual disease by next-generation ,2021/5
1367,Zhaolei Zhang,"Focal sources are potential targets for atrial fibrillation (AF) catheter ablation, but they can be time-consuming and challenging to identify when unipolar electrograms (EGM) are numerous and complex. Our aim was to apply deep learning (DL) to raw unipolar EGMs in order to automate putative focal sources detection. We included 78 patients from the Focal Source and Trigger (FaST) randomized controlled trial that evaluated the efficacy of adjunctive FaST ablation compared to pulmonary vein isolation alone in reducing AF recurrence. FaST sites were identified based on manual classification of sustained periodic unipolar QS EGMs over 5-s. All periodic unipolar EGMs were divided into training (n= 10,004) and testing cohorts (n= 3,180). DL was developed using residual convolutional neural network to discriminate between FaST and non-FaST. A gradient-based method was applied to interpret the DL model. ",Deep learning classification of unipolar electrograms in human atrial fibrillation: application in focal source mapping,2021
1368,Zhaolei Zhang,"Recent advances in genomics and proteomics generated a large amount of trans regulatory data such as those mediated by RNA binding proteins (RBPs) and microRNAs. Since both types of trans regulators largely target 3’ UTR of mRNA transcripts, it is likely that there would be interactions, i.e. competitive or cooperative effect, among these trans factors. We compiled the available RBP and microRNA binding sites, mapped them to the mRNA transcripts, and correlated the binding data with mRNA expression data generated by TCGA (The Cancer Genome Atlas). We separated pairs of RBPs and microRNAs into three scenarios: those that have overlapping target sites on the same mRNA transcript (overlapping), those that have target sites on the same mRNA transcript but non-overlapping (neighboring), and those that do not target the same mRNA transcript (independent). Through a regression analysis on expression profiles, we indeed observed interaction effects between RBPs and microRNAs in the majority of the cancer expression data sets. We further discussed implication of such widespread interactions in the context of cancer and diseases.",A Survey of Regulatory Interactions Among RNA Binding Proteins and MicroRNAs in Cancer,2020-09-08 00:00:00
1369,Zhaolei Zhang,"The off‐target effects induced by guide RNAs in the CRISPR/Cas9 gene‐editing system have raised substantial concerns in recent years. Many in silico predictive models have been developed for predicting the off‐target activities; however, few are capable of predicting the off‐target activities with insertions or deletions between guide RNA and target DNA sequence pair. In order to fill this gap, a recurrent convolutional network named CRISPR‐Net is developed for scoring the gRNA‐target pairs with mismatches and indels; and a machine‐learning based model named CRISPR‐Net‐Aggregate is also developed for aggregating the scores as the consensus off‐target score for each potential guide RNA. It is demonstrated that CRISPR‐Net achieves competitive performance on CIRCLE‐Seq and GUIDE‐seq datasets with indels and mismatches, outperforming the state‐of‐the‐art off‐target prediction methods on two",CRISPR‐Net: A Recurrent Convolutional Network Quantifies CRISPR Off‐Target Activities with Mismatches and Indels,2020/7
1370,Zhaolei Zhang,"Clinically diagnosed pulmonary tuberculosis (PTB) patients lack microbiological evidence of Mycobacterium tuberculosis, and misdiagnosis or delayed diagnosis often occurs as a consequence. We investigated the potential of long noncoding RNAs (lncRNAs) and corresponding predictive models to diagnose these patients. We enrolled 1,764 subjects, including clinically diagnosed PTB patients, microbiologically confirmed PTB cases, non-TB disease controls, and healthy controls, in three cohorts (screening, selection, and validation). Candidate lncRNAs differentially expressed in blood samples of the PTB and healthy control groups were identified by microarray and reverse transcription-quantitative PCR (qRT-PCR) in the screening cohort. Logistic regression models were developed using lncRNAs and/or electronic health records (EHRs) from clinically diagnosed PTB patients and non-TB disease controls in ",Long noncoding RNA and predictive model to improve diagnosis of clinically diagnosed pulmonary tuberculosis,2020-06-24 00:00:00
1371,Zhaolei Zhang,"The COVID-19 pandemic has caused over one million deaths thus far. There is an urgent need for the development of specific viral therapeutics and a vaccine. SARS-CoV-2 nucleocapsid (N) protein is highly expressed upon infection and is essential for viral replication, making it a promising target for both antiviral drug and vaccine development. Here, starting from a functional proteomics workflow, we initially catalogued the protein-protein interactions of 21 SARS-CoV-2 proteins in HEK293 cells, finding that the stress granule resident proteins G3BP1 and G3BP2 copurify with N with high specificity. We demonstrate that N protein expression in human cells sequesters G3BP1 and G3BP2 through its physical interaction with these proteins, attenuating stress granule (SG) formation. The ectopic expression of G3BP1 in N-expressing cells was sufficient to reverse this phenotype.",SARS-CoV-2 Nucleocapsid protein attenuates stress granule formation and alters gene expression via direct interaction with host mRNAs,2020-01-01 00:00:00
1372,Zhaolei Zhang,with different genomes available unsupervised learning algorithms are essential in learning genomewide biological insights especially the functional characterization of different genomes is essential for us to understand lives in this book chapter we review the stateoftheart unsupervised learning algorithms for genome informatics from dna to microrna dna deoxyribonucleic acid is the basic component of genomes a significant fraction of dna regions transcription factor binding sites are bound by proteins transcription factors to regulate gene expression at different development stages in different tissues to fully understand genetics it is necessary of us to apply unsupervised learning algorithms to learn and infer those dna regions here we review several unsupervised learning methods for deciphering the genomewide patterns of those dna regions microrna mirna a class of small endogenous noncoding rna ribonucleic acid species regulate gene expression posttranscriptionally by forming imperfect basepair with the target sites primarily at the untranslated regions of the messenger rnas since the discovery of the first mirna emphlet in worms a vast amount of studies have been dedicated to functionally characterizing the functional impacts of mirna in a network context to understand complex diseases such as cancer here we review several representative unsupervised learning frameworks on inferring mirna regulatory network by exploiting the static sequencebased information pertinent to the prior knowledge of mirna targeting and the dynamic information of mirna activities implicated by the recently available large data compendia which interrogate genomewide expression profiles of mirnas andor mrnas across various cell conditions less,Unsupervised Learning in Genome Informatics,"3 August, 2015"
1373,Shurui Zhou,"Machine learning models have been widely developed, released, and adopted in numerous applications. Meanwhile, the documentation practice for machine learning models often falls short of established practices for traditional software components, which impedes model accountability, inadvertently abets inappropriate or misuse of models, and may trigger negative social impact. Recently, model cards, a template for documenting machine learning models, have attracted notable attention, but their impact on the practice of model documentation is unclear. In this work, we examine publicly available model cards and other similar documentation. Our analysis reveals a substantial gap between the suggestions made in the original model card work and the content in actual documentation. ",Aspirations and Practice of Model Documentation: Moving the Needle with Nudging and Traceability,2022-04-13 00:00:00
1374,Shurui Zhou,"The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges.","Collaboration Challenges in Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process",2022
1375,Shurui Zhou,"Data scientists commonly use computational notebooks because they provide a good environment for testing multiple models. However, once the scientist completes the code and finds the ideal model, he or she will have to dedicate time to clean up the code in order for others to easily understand it. In this paper, we perform a qualitative study on how scientists clean their code in hopes of being able to suggest a tool to automate this process. Our end goal is for tool builders to address possible gaps and provide additional aid to data scientists, who then can focus more on their actual work rather than the routine and tedious cleaning work. By sampling notebooks from GitHub and analyzing changes between subsequent commits, we identified common cleaning activities, such as changes to markdown (e.g., adding headers sections or descriptions) or comments (both deleting dead code and adding descriptions) as ","Splitting, renaming, removing: A study of common cleaning activities in Jupyter notebooks",2021-11-15 00:00:00
1376,Shurui Zhou,"With the emergence of social coding platforms, collaboration has become a key and dynamic aspect to the success of software projects. In such platforms, developers have to collaborate and deal with issues of collaboration in open‐source software development. Although collaboration is challenging, collaborative development produces better software systems than any developer could produce alone. Several approaches have investigated collaboration challenges, for instance, by proposing or evaluating models and tools to support collaborative work. Despite the undeniable importance of the existing efforts in this direction, there are few works on collaboration from perspectives of developers. In this work, we aim to investigate the perceptions of open‐source software developers on collaborations, such as motivations, techniques, and tools to support global, productive, and collaborative development. ",Perceptions of open‐source software developers on collaborations: An interview and survey study,2021-10-24 00:00:00
1377,Shurui Zhou,"The introduction of machine learning (ML) components in software projects has created the need for software engineers to collaborate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through interviews with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common collaboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. ","More Engineering, No Silos: Rethinking Processes and Interfaces in Collaboration between Interdisciplinary Teams for Machine Learning Projects",2021-10-19 00:00:00
1378,Shurui Zhou,"It is widely recognized that patches generated by program repair tools have to be correct to be useful. However, it is fundamentally difficult to ensure the correctness of the patches. Many tools generate only the patches that are highly likely to be correct by taking conservative strategies which inevitably limit the recall of APR approaches. While the recall of APR can potentially be improved by relaxing the requirement on precision, more incorrect patches may also be generated. In this paper, we conjecture that reviewing incorrect patches also helps developers to understand the bug, and with proper tool support, reviewing incorrect patches would at least not reduce the repair performance. To evaluate this, we propose an interactive patch filtering approach to facilitate developers in the patch review process via effectively filtering out groups of incorrect patches. ",Interactive Patch Filtering as Debugging Aid,2021-09-01 00:00:00
1379,Shurui Zhou,"Data scientists reportedly spend a significant amount of their time in their daily routines on data wrangling, i.e. cleaning data and extracting features. However, data wrangling code is often repetitive and error-prone to write. Moreover, it is easy to introduce subtle bugs when reusing and adopting existing code, which results in reduced model quality. To support data scientists with data wrangling, we present a technique to generate documentation for data wrangling code. We use (1) program synthesis techniques to automatically summarize data transformations and (2) test case selection techniques to purposefully select representative examples from the data based on execution information collected with tailored dynamic program analysis. We demonstrate that a JupyterLab extension with our technique can provide on-demand documentation for many cells in popular notebooks and find in a user study that users ",Subtle Bugs Everywhere: Generating Documentation for Data Wrangling Code,2021
1380,Shurui Zhou,"The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community. Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed",How has forking changed in the last 20 years? a study of hard forks on github,2020-10-05 00:00:00
1381,Shurui Zhou,"The fork-based development mechanism provides the flexibility and the unified processes for software teams to collaborate easily in a distributed setting without too much coordination overhead. Currently, multiple social coding platforms support fork-based development, such as GitHub, GitLab, and Bitbucket. Although these different platforms virtually share the same features, they have different emphasis. As GitHub is the most popular platform and the corresponding data is publicly available, most of the current studies are focusing on GitHub hosted projects. However, we observed anecdote evidences that people are confused about choosing among these platforms, and some projects are migrating from one platform to another, and the reasons behind these activities remain unknown. With the advances of Software Heritage Graph Dataset (SWHGD), we have the opportunity to investigate the forking activities",An Exploratory Study to Find Motives Behind Cross-platform Forks from Software Heritage Dataset,2020-06-29 00:00:00
1382,Shurui Zhou,"In globally distributed software development, many software developers have to collaborate and deal with issues of collaboration. Although collaboration is challenging, collaborative development produces better software than any developer could produce alone. Unlike previous work which focuses on the proposal and evaluation of models and tools to support collaborative work, this paper presents an interview study aiming to understand (i) the motivations,(ii) how collaboration happens, and (iii) the challenges and barriers of collaborative software development. After interviewing twelve experienced software developers from GitHub, we found different types of collaborative contributions, such as in the management of requests for changes. Our analysis also indicates that the main barriers for collaboration are related to non-technical, rather than technical issues.",Understanding collaborative software development: An interview study,2020-06-26 00:00:00
1383,Shurui Zhou,"Fork-based development is a lightweight mechanism that allows developers to collaborate with or without explicit coordination. Although it is easy to use and popular, when developers each create their own fork and develop independently, their contributions are usually not easily visible to others. When the number of forks grows, it becomes very difficult to maintain an overview of what happens in individual forks, which would lead to additional problems and inefficient practices: lost contributions, redundant development, fragmented communities, and so on. Facing the problems mentioned above, we developed two complementary strategies: (1) Identifying existing best practices and suggesting evidence-based interventions for projects that are inefficient; (2) designing new interventions that could improve the awareness of a community using fork-based development, and help developers to detect redundant ",Improving collaboration efficiency in fork-based development,2019-11-11 00:00:00
1384,Shurui Zhou,"Forking and pull requests have been widely used in open-source communities as a uniform development and contribution mechanism, giving developers the flexibility to modify their own fork without affecting others before attempting to contribute back. However, not all projects use forks efficiently; many experience lost and duplicate contributions and fragmented communities. In this paper, we explore how open-source projects on GitHub differ with regard to forking inefficiencies. First, we observed that different communities experience these inefficiencies to widely different degrees and interviewed practitioners to understand why. Then, using multiple regression modeling, we analyzed which context factors correlate with fewer inefficiencies. We found that better modularity and centralized management are associated with more contributions and a higher fraction of accepted pull requests, suggesting specific best ",What the fork: a study of inefficient and efficient forking practices in social coding,2019-08-12 00:00:00
1385,Shurui Zhou,machine learning models have been widely developed released and adopted in numerous applications meanwhile the documentation practice for machine learning models often falls short of established practices for traditional software components which impedes model accountability inadvertently abets inappropriate or misuse of models and may trigger negative social impact recently model cards a template for documenting machine learning models have attracted notable attention but their impact on the practice of model documentation is unclear in this work we examine publicly available model cards and other similar documentation our analysis reveals a substantial gap between the suggestions made in the original model card work and the content in actual documentation motivated by this observation and literature on fields such as software documentation interaction design and traceability we further propose a set of design guidelines that aim to support the documentation practice for machine learning models including the collocation of documentation environment with the coding environment nudging the consideration of model card sections during model development and documentation derived from and traced to the source we designed a prototype tool named docml following those guidelines to support model development in computational notebooks a lab study reveals the benefit of our tool to shift the behavior of data scientists towards documentation quality and accountability less,Aspirations and Practice of Model Documentation: Moving the Needle with Nudging and Traceability,"13 April, 2022"
1386,Shurui Zhou,the forkbased development mechanism provides the flexibility and the unified processes for software teams to collaborate easily in a distributed setting without too much coordination overheadcurrently multiple social coding platforms support forkbased development such as github gitlab and bitbucket although these different platforms virtually share the same features they have different emphasis as github is the most popular platform and the corresponding data is publicly available most of the current studies are focusing on github hosted projects however we observed anecdote evidences that people are confused about choosing among these platforms and some projects are migrating from one platform to another and the reasons behind these activities remain unknownwith the advances of software heritage graph dataset swhgdwe have the opportunity to investigate the forking activities across platforms in this paper we conduct an exploratory study on popular opensource projects to identify crossplatform forks and investigate the motivation behind preliminary result shows that crossplatform forks do exist for the subject systems in this study we found forks in total among which forks are on gitlab based on our qualitative analysis we found that most of the crossplatform forks that we identified are mirrors of the repositories on another platform but we still find cases that were created due to preference of using certain functionalities eg continuous integration ci supported by different platforms this study lays the foundation of future research directions such as understanding the differences between platforms and supporting crossplatform collaboration less,An Exploratory Study to Find Motives Behind Cross-platform Forks from Software Heritage Dataset,"17 March, 2020"
1387,Marcus Brubaker,most camera images are rendered and saved in the standard rgb srgb format by the cameras hardware due to the incamera photofinishing routines nonlinear srgb images are undesirable for computer vision tasks that assume a direct relationship between pixel values and scene radiance for such applications linear rawrgb sensor images are preferred saving images in their rawrgb format is still uncommon due to the large storage requirement and lack of support by many imaging applications several raw reconstruction methods have been proposed that utilize specialized metadata sampled from the rawrgb image at capture time and embedded in the srgb image this metadata is used to parameterize a mapping function to derender the srgb image back to its original rawrgb format when needed existing raw reconstruction methods rely on simple sampling strategies and global mapping to perform the derendering this paper shows how to improve the derendering results by jointly learning sampling and reconstruction our experiments show that our learned sampling can adapt to the image content to produce better raw reconstructions than existing methods we also describe an online finetuning strategy for the reconstruction network to improve results further less,Learning sRGB-to-Raw-RGB De-rendering with Content-Aware Metadata,"3 June, 2022"
1388,Marcus Brubaker,normalizing flows model a complex target distribution in terms of a bijective transform operating on a simple base distribution as such they enable tractable computation of a number of important statistical quantities particularly likelihoods and samples despite these appealing properties the computation of more complex inference tasks such as the cumulative distribution function cdf over a complex region eg a polytope remains challenging traditional cdf approximations using montecarlo techniques are unbiased but have unbounded variance and low sample efficiency instead we build upon the diffeomorphic properties of normalizing flows and leverage the divergence theorem to estimate the cdf over a closed region in target space in terms of the flux across its emphboundary as induced by the normalizing flow we describe both deterministic and stochastic instances of this estimator while the deterministic variant iteratively improves the estimate by strategically subdividing the boundary the stochastic variant provides unbiased estimates our experiments on popular flow architectures and uci benchmark datasets show a marked improvement in sample efficiency as compared to traditional estimators less,Efficient CDF Approximations for Normalizing Flows,"23 February, 2022"
1389,Marcus Brubaker,partial observations of continuous timeseries dynamics at arbitrary time stamps exist in many disciplines fitting this type of data using statistical models with continuous dynamics is not only promising at an intuitive level but also has practical benefits including the ability to generate continuous trajectories and to perform inference on previously unseen time stamps despite exciting progress in this area the existing models still face challenges in terms of their representational power and the quality of their variational approximations we tackle these challenges with continuous latent process flows clpf a principled architecture decoding continuous latent processes into continuous observable processes using a timedependent normalizing flow driven by a stochastic differential equation to optimize our model using maximum likelihood we propose a novel piecewise construction of a variational posterior process and derive the corresponding variational lower bound using trajectory reweighting our ablation studies demonstrate the effectiveness of our contributions in various inference tasks on irregular time grids comparisons to stateoftheart baselines show our models favourable performance on both synthetic and realworld timeseries data less,Continuous Latent Process Flows,"27 October, 2021"
1390,Marcus Brubaker,auto white balance awb is applied by camera hardware at capture time to remove the color cast caused by the scene illumination the vast majority of whitebalance algorithms assume a single light source illuminates the scene however real scenes often have mixed lighting conditions this paper presents an effective awb method to deal with such mixedilluminant scenes a unique departure from conventional awb our method does not require illuminant estimation as is the case in traditional camera awb modules instead our method proposes to render the captured scene with a small set of predefined whitebalance settings given this set of rendered images our method learns to estimate weighting maps that are used to blend the rendered images to generate the final corrected image through extensive experiments we show this proposed method produces promising results compared to other alternatives for single and mixedilluminant scene color correction our source code and trained models are available at httpsgithubcommahmoudnafifimixedillwb less,Auto White-Balance Correction for Mixed-Illuminant Scenes,"7 October, 2021"
1391,Marcus Brubaker,normalizing flows transform a simple base distribution into a complex target distribution and have proved to be powerful models for data generation and density estimation in this work we propose a novel type of normalizing flow driven by a differential deformation of the wiener process as a result we obtain a rich time series model whose observable process inherits many of the appealing properties of its base process such as efficient computation of likelihoods and marginals furthermore our continuous treatment provides a natural framework for irregular time series with an independent arrival process including straightforward interpolation we illustrate the desirable properties of the proposed model on popular stochastic processes and demonstrate its superior flexibility to variational rnn and latent ode baselines in a series of experiments on synthetic and realworld data less,Modeling Continuous Stochastic Processes with Dynamic Normalizing Flows,"13 July, 2021"
1392,Marcus Brubaker,the purpose of generative zeroshot learning zsl is to learning from seen classes transfer the learned knowledge and create samples of unseen classes from the description of these unseen categories to achieve better zsl accuracies models need to better understand the descriptions of unseen classes we introduce a novel form of regularization that encourages generative zsl models to pay more attention to the description of each category our empirical results demonstrate improvements over the performance of multiple stateoftheart models on the task of generalized zeroshot recognition and classification when trained on textual descriptionbased datasets like cub and nabirds and attributebased datasets like awa apy and sun less,Zero-shot Learning with Class Description Regularization,"30 June, 2021"
1393,Marcus Brubaker,we investigate the ability of popular flow based methods to capture tailproperties of a target density by studying the increasing triangular maps used in these flow methods acting on a tractable source density we show that the density quantile functions of the source and target density provide a precise characterization of the slope of transformation required to capture tails in a target density we further show that any lipschitzcontinuous transport map acting on a source density will result in a density with similar tail properties as the source highlighting the tradeoff between a complex source density and a sufficiently expressive transformation to capture desirable properties of a target density subsequently we illustrate that flow models like realnvp maf and glow as implemented originally lack the ability to capture a distribution with nongaussian tails we circumvent this problem by proposing tailadaptive flows consisting of a source distribution that can be learned simultaneously with the triangular map to capture tailproperties of a target density we perform several synthetic and realworld experiments to compliment our theoretical findings less,Tails of Lipschitz Triangular Flows,"18 September, 2020"
1394,Marcus Brubaker,in this work we propose a novel probabilistic sequence model that excels at capturing high variability in time series data both across sequences and within an individual sequence our method uses temporal latent variables to capture information about the underlying data pattern and dynamically decodes the latent information into modifications of weights of the base decoder and recurrent model the efficacy of the proposed method is demonstrated on a range of synthetic and realworld sequential data that exhibit large scale variations regime shifts and complex dynamics less,Variational Hyper RNN for Sequence Modeling,"24 February, 2020"
1395,Marcus Brubaker,the problem of efficiently training and evaluating image classifiers that can distinguish between a large number of object categories is considered a novel metric sharpness is proposed which is defined as the fraction of object categories that are above a threshold accuracy to estimate sharpness along with a confidence value a technique called fractionaccurate estimation is introduced which samples categories and samples instances from these categories in addition a technique called parity partition coding a special type of error correcting output code is introduced increasing sharpness while reducing the multiclass problem to a multilabel one with exponentially fewer outputs we demonstrate that this approach outperforms the baseline model for both multimnist and celeba while requiring fewer parameters and exceeding state of the art accuracy on individual labels less,Parity Partition Coding for Sharp Multi-Label Classification,"23 August, 2019"
1396,Marcus Brubaker,scientific imaging techniques such as optical and electron microscopy and computed tomography ct scanning are used to study the d structure of an object through d observations these observations are related to the original d object through orthogonal integral projections for common d reconstruction algorithms computational efficiency requires the modeling of the d structures to take place in fourier space by applying the fourier slice theorem at present it is unclear how to differentiate through the projection operator and hence current learning algorithms can not rely on gradient based methods to optimize d structure models in this paper we show how backpropagation through the projection operator in fourier space can be achieved we demonstrate the validity of the approach with experiments on d reconstruction of proteins we further extend our approach to learning probabilistic models of d objects this allows us to predict regions of low sampling rates or estimate noise a higher sample efficiency can be reached by utilizing the learned uncertainties of the d structure as an unsupervised estimate of the model fit finally we demonstrate how the reconstruction algorithm can be extended with an amortized inference scheme on unknown attributes such as object pose through empirical studies we show that joint inference of the d structure and the object pose becomes more difficult when the ground truth object contains more symmetries due to the presence of for instance approximate rotational symmetries the pose estimation can easily get stuck in local optima inhibiting a finegrained highquality estimate of the d structure less,Differentiable probabilistic models of scientific imaging with the Fourier slice theorem,"20 June, 2019"
1397,Marcus Brubaker,we propose a generative approach to physicsbased motion capture unlike prior attempts to incorporate physics into tracking that assume the subject and scene geometry are calibrated and known a priori our approach is automatic and online this distinction is important since calibration of the environment is often difficult especially for motions with props uneven surfaces or outdoor scenes the use of physics in this context provides a natural framework to reason about contact and the plausibility of recovered motions we propose a fast datadriven parametric body model based on linearblend skinning which decouples deformations due to pose anthropometrics and body shape pose and shape parameters are estimated using robust icp optimization with physicsbased dynamic priors that incorporate contact contact is estimated from torque trajectories and predictions of which contact points were active to our knowledge this is the first approach to take physics into account without explicit em a priori knowledge of the environment or body dimensions we demonstrate effective tracking from a noisy single depth camera improving on stateoftheart results quantitatively and producing better qualitative results reducing visual artifacts like footskate and jitter less,Walking on Thin Air: Environment-Free Physics-based Markerless Motion Capture,"3 December, 2018"
1398,Marcus Brubaker,in this paper we present a robust efficient and affordable approach to selflocalization which does not require neither gps nor knowledge about the appearance of the world towards this goal we utilize freely available cartographic maps and derive a probabilistic model that exploits semantic cues in the form of sun direction presence of an intersection road type speed limit as well as the egocar trajectory in order to produce very reliable localization results our experimental evaluation shows that our approach can localize much faster in terms of driving time with less computation and more robustly than competing approaches which ignore semantic information less,Find your Way by Observing the Sun and Other Semantic Cues,"23 June, 2016"
1399,Marcus Brubaker,as computational challenges in optimization and statistical inference grow ever harder algorithms that utilize derivatives are becoming increasingly more important the implementation of the derivatives that make these algorithms so powerful however is a substantial user burden and the practicality of these algorithms depends critically on tools like automatic differentiation that remove the implementation burden entirely the stan math library is a c reversemode automatic differentiation library designed to be usable extensive and extensible efficient scalable stable portable and redistributable in order to facilitate the construction and utilization of such algorithms usability is achieved through a simple direct interface and a cleanly abstracted functional interface the extensive builtin library includes functions for matrix operations linear algebra differential equation solving and most common probability functions extensibility derives from a straightforward objectoriented framework for expressions allowing users to easily create custom functions efficiency is achieved through a combination of custom memory management subexpression caching traitsbased metaprogramming and expression templates partial derivatives for compound functions are evaluated lazily for improved scalability stability is achieved by taking care with arithmetic precision in algebraic expressions and providing stable compound functions where possible for portability the library is standardscompliant c and has been tested for all major compilers for windows mac os x and linux less,The Stan Math Library: Reverse-Mode Automatic Differentiation in C++,"23 September, 2015"
1400,Garth Gibson,"High-Performance Computing (HPC) is known for its use of massive concurrency. But it can be challenging for a parallel filesystem's control plane to utilize cores when every client process must globally synchronize and serialize its metadata mutations with those of other clients. We present DeltaFS, a new paradigm for distributed filesystem metadata.",DeltaFS: a scalable no-ground-truth filesystem for massively-parallel computing,2021-11-14 00:00:00
1401,Garth Gibson,"Complex storage stacks providing data compression, indexing, and analytics help leverage the massive amounts of data generated today to derive insights. It is challenging to perform this computation, however, while fully utilizing the underlying storage media. This is because, while storage servers with large core counts are widely available, single-core performance and memory bandwidth per core grow slower than the core count per die. Computational storage offers a promising solution to this problem by utilizing dedicated compute resources along the storage processing path. We present DeltaFS Indexed Massive Directories (IMDs), a new approach to computational storage. DeltaFS IMDs harvest available (i.e., not dedicated) compute, memory, and network resources on the compute nodes of an application to perform computation on data. We demonstrate the efficiency of DeltaFS IMDs by using them to",Streaming data reorganization at scale with DeltaFS indexed massive directories,2020-09-24 00:00:00
1402,Garth Gibson,"Technology enhancements and the growing breadth of application workflows running on high-performance computing (HPC) platforms drive the development of new data services that provide high performance on these new platforms, provide capable and productive interfaces and abstractions for a variety of applications, and are readily adapted when new technologies are deployed. The Mochi framework enables composition of specialized distributed data services from a collection of connectable modules and subservices. Rather than forcing all applications to use a one-size-fits-all data staging and I/O software configuration, Mochi allows each application to use a data service specialized to its needs and access patterns. This paper introduces the Mochi framework and methodology. The Mochi core components and microservices are described. Examples of the application of the Mochi methodology to the ",Mochi: Composing data services for high-performance computing environments,2020/1
1403,Garth Gibson,"We are approaching a point in time when it will be infeasible to catalog and query data after it has been generated. This trend has fueled research on in-situ data processing (i.e. operating on data as it is streamed to storage). One important example of this approach is in-situ data indexing. Prior work has shown the feasibility of indexing at scale as a two-step process. First, one partitions data by key across the CPU cores of a parallel job. Then each core indexes its subset as data is persisted. Online partitioning requires transferring data over the network so that it can be indexed and stored by the core responsible for the data. This approach is becoming increasingly costly as new computing platforms emphasize parallelism instead of individual core performance that is crucial for communication libraries and systems software in general. In addition to indexing, scalable online data partitioning is also useful in other ",Compact Filters for Fast Online Data Partitioning,2019-09-23 00:00:00
1404,Garth Gibson,"Data parallel training is widely used for scaling distributed deep neural network (DNN) training. However, the performance benefits are often limited by the communication-heavy parameter synchronization step. In this paper, we take advantage of the domain specific knowledge of DNN training and overlap parameter synchronization with computation in order to improve the training performance. We make two key observations:(1) the optimal data representation granularity for the communication may differ from that used by the underlying DNN model implementation and (2) different parameters can afford different synchronization delays. Based on these observations, we propose a new synchronization mechanism called Priority-based Parameter Propagation (P3). P3 synchronizes parameters at a finer granularity and schedules data transmission in such a way that the training process incurs minimal communication delay. We show that P3 can improve the training throughput of ResNet-50, Sockeye and VGG-19 by as much as 25%, 38% and 66% respectively on clusters with realistic network bandwidth.",Priority-based parameter propagation for distributed DNN training,2019-04-15 00:00:00
1405,Garth Gibson,"Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the two",Mlsys: The new frontier of machine learning systems,2019-03-29 00:00:00
1406,Garth Gibson,"Machine learning (ML) training is commonly parallelized using data parallelism. A fundamental limitation of data parallelism is that conflicting (concurrent) parameter accesses during ML training usually diminishes or even negates the benefits provided by additional parallel compute resources. Although it is possible to avoid conflicting parameter accesses by carefully scheduling the computation, existing systems rely on programmer manual parallelization and it remains a question when such parallelization is possible.",Automating dependence-aware parallelization of machine learning training on distributed shared memory,2019-03-25 00:00:00
1407,Garth Gibson,"It is a daunting task for a data scientist to convert sequential code for a Machine Learning (ML) model, published by an ML researcher, to a distributed framework that runs on a cluster and operates on massive datasets. The process of fitting the sequential code to an appropriate programming model and data abstractions determined by the framework of choice requires significant engineering and cognitive effort. Furthermore, inherent constraints of frameworks sometimes lead to inefficient implementations, delivering suboptimal performance.",{STRADS-AP}: Simplifying Distributed Machine Learning Programming without Introducing a New Programming Model,2019
1408,Garth Gibson,"Machine learning (ML) techniques are enjoying rapidly increasing adoption. However, designing and implementing the systems that support ML models in real-world deployments remains a significant obstacle, in large part due to the radically different development and deployment profile of modern ML methods, and the range of practical concerns that come with broader adoption. We propose to foster a new systems machine learning research community at the intersection of the traditional systems and ML communities, focused on topics such as hardware systems for ML, software systems for ML, and ML optimized for metrics beyond predictive accuracy. To do this, we describe a new conference, MLSys, that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ML, and an explicit focus on topics at the intersection of the",Sysml: The new frontier of machine learning systems,2019-01-01 00:00:00
1409,Garth Gibson,"Analysis of large-scale simulation output is a core element of scientific inquiry, but analysis queries may experience significant I/O overhead when the data is not structured for efficient retrieval. While in-situ processing allows for improved time-to-insight for many applications, scaling in-situ frameworks to hundreds of thousands of cores can be difficult in practice. The DeltaFS in-situ indexing is a new approach for in-situ processing of massive amounts of data to achieve efficient point and small-range queries. This paper describes the challenges and lessons learned when scaling this in-situ processing function to hundreds of thousands of cores. We propose techniques for scalable all-to-all communication that is memory and bandwidth efficient, concurrent indexing, and specialized LSM-Tree formats. Combining these techniques allows DeltaFS to control the cost of in-situ processing while maintaining 3 orders of",Scaling embedded in-situ indexing with deltaFS,2018-11-11 00:00:00
1410,Garth Gibson,"Deep learning has revolutionized the machine learning field and has been attracting tremendous research interests since its inception. Resource intensive Deep Neural Networks (DNN) training computations are highly parallelizable and can be scaled on thousands of computing units in GPUs. In order to cater the ever-increasing demand for computing power, GPU manufacturers are building more powerful and specialized hardwares. However, it is not clearly analyzed how well the modern GPUs can handle the myriad of DNN based applications currently popular in the community. In this study, we focus on benchmarking, analyzing and comparing the performance of 8 popular GPUs using 8 archetypal DNN models.",Hardware Sensitivity Analysis for Deep Learning Models,2018/9
1411,Garth Gibson,"Apache Spark employs lazy evaluation [11, 6]; that is, in Spark, a dataset is represented as Resilient Distributed Dataset (RDD), and a single-threaded application (driver) program simply describes transformations (RDD to RDD), referred to as lineage [7, 12], without performing distributed computation until output is requested. The lineage traces computation and dependency back to external (and assumed durable) data sources, allowing Spark to opportunistically cache intermediate RDDs, because it can recompute everything from external data sources. To initiate computation on worker machines, the driver process constructs a directed acyclic graph (DAG) representing computation and dependency according to the requested RDD’s lineage. ",Addressing the Long-Lineage Bottleneck in Apache Spark,2018/1
1412,Garth Gibson,machine learning ml techniques are enjoying rapidly increasing adoption however designing and implementing the systems that support ml models in realworld deployments remains a significant obstacle in large part due to the radically different development and deployment profile of modern ml methods and the range of practical concerns that come with broader adoption we propose to foster a new systems machine learning research community at the intersection of the traditional systems and ml communities focused on topics such as hardware systems for ml software systems for ml and ml optimized for metrics beyond predictive accuracy to do this we describe a new conference mlsys that explicitly targets research at the intersection of systems and machine learning with a program committee split evenly between experts in systems and ml and an explicit focus on topics at the intersection of the two less,MLSys: The New Frontier of Machine Learning Systems,"1 December, 2019"
1413,Garth Gibson,when training large machine learning models with many variables or parameters a single machine is often inadequate since the model may be too large to fit in memory while training can take a long time even with stochastic updates a natural recourse is to turn to distributed cluster computing in order to harness additional memory and processors however naive unstructured parallelization of ml algorithms can make inefficient use of distributed memory while failing to obtain proportional convergence speedups or can even result in divergence we develop a framework of primitives for dynamic modelparallelism strads in order to explore partitioning and update scheduling of model variables in distributed ml algorithms thus improving their memory efficiency while presenting new opportunities to speed up convergence without compromising inference correctness we demonstrate the efficacy of modelparallel algorithms implemented in strads versus popular implementations for topic modeling matrix factorization and lasso less,Primitives for Dynamic Big Model Parallelism,"17 June, 2014"
1414,Garth Gibson,the heepin cross section was measured at fourmomentum transfers of q and gev at an invariant mass of the photon nucleon system of w gev the charged pion form factor fpi was extracted from the data by comparing the separated longitudinal pion electroproduction cross section to a regge model prediction in which fpi is a free parameter the results indicate that the pion form factor deviates from the chargeradius constrained monopole form at these values of q by one sigma but is still far from its perturbative quantum chromodynamics prediction less,Determination of the Charged Pion Form Factor at Q2=1.60 and 2.45 (GeV/c)2,"7 July, 2006"
1415,Benjamin Haibe-Kains,accurate survival prediction is crucial for development of precision cancer medicine creating the need for new sources of prognostic information recently there has been significant interest in exploiting routinely collected clinical and medical imaging data to discover new prognostic markers in multiple cancer types however most of the previous studies focus on individual data modalities alone and do not make use of recent advances in machine learning for survival prediction we present deepcr mtlr a novel machine learning approach for accurate cancer survival prediction from multimodal clinical and imaging data in the presence of competing risks based on neural networks and an extension of the multitask logistic regression framework we demonstrate improved prognostic performance of the multimodal approach over single modality predictors in a cohort of head and neck cancer patients particularly for cancer specific survival where our approach achieves year auroc of and cindex of less,Deep-CR MTLR: a Multi-Modal Approach for Cancer Survival Prediction with Competing Risks,"21 March, 2021"
1416,Benjamin Haibe-Kains,performance of neural network models relies on the availability of large datasets with minimal levels of uncertainty transfer learning tl models have been proposed to resolve the issue of small dataset size by letting the model train on a bigger taskrelated reference dataset and then finetune on a smaller taskspecific dataset in this work we apply a transfer learning approach to improve predictive power in noisy data systems with large variable confidence datasets we propose a deep neural network method called filtered transfer learning ftl that defines multiple tiers of data confidence as separate tasks in a transfer learning setting the deep neural network is finetuned in a hierarchical process by iteratively removing filtering data points with lower label confidence and retraining in this report we use ftl for predicting the interaction of drugs and proteins we demonstrate that using ftl to learn stepwise across the label confidence distribution results in higher performance compared to deep neural network models trained on a single confidence range we anticipate that this approach will enable the machine learning community to benefit from large datasets with uncertain labels in fields such as biology and medicine less,Learning across label confidence distributions using Filtered Transfer Learning,"3 June, 2020"
1417,Benjamin Haibe-Kains,in their study mckinney et al showed the high potential of artificial intelligence for breast cancer screening however the lack of detailed methods and computer code undermines its scientific value we identify obstacles hindering transparent and reproducible ai research as faced by mckinney et al and provide solutions with implications for the broader field less,The importance of transparency and reproducibility in artificial intelligence research,"7 March, 2020"
1418,Benjamin Haibe-Kains,many deep learning algorithms can be easily fooled with simple adversarial examples to address the limitations of existing defenses we devised a probabilistic framework that can generate an exponentially large ensemble of models from a single model with just a linear cost this framework takes advantage of neural network depth and stochastically decides whether or not to insert noise removal operators such as vaes between layers we show empirically the important role that model gradients have when it comes to determining transferability of adversarial examples and take advantage of this result to demonstrate that it is possible to train models with limited adversarial attack transferability additionally we propose a detection method based on metric learning in order to detect adversarial examples that have no hope of being cleaned of maliciously engineered noise less,Stochastic Combinatorial Ensembles for Defending Against Adversarial Examples,"8 September, 2018"
1419,Michael Hoffman,we present bvri and unfiltered clear light curves of strippedenvelope supernovae sesne observed between and from the lick observatory supernova search loss followup program our sesn sample consists of spectroscopically normal sneib two peculiar sne ib six sn ibn normal sne ic one peculiar sn ic ten sne icbl sne iib one ambiguous sn iibibc and two superluminous sne our followup photometry has on a persn basis a mean coverage of photometric points median of points and a mean cadence of d median of d from our full sample a subset of sne have premaximum coverage in at least one passband allowing for the peak brightness of each sn in this subset to be quantitatively determined we describe our data collection and processing techniques with emphasis toward our automated photometry pipeline from which we derive publicly available data products to enable and encourage further study by the community using these data products we derive hostgalaxy extinction values through the empirical colour evolution relationship and for the first time produce accurate risetime measurements for a large sample of sesne in both optical and infrared passbands by modeling multiband light curves we find that sne ic tend to have lower ejecta masses and lower ejecta velocities than sneib and iib but higher ni masses less,The Lick Observatory Supernova Search follow-up program: photometry data release of 70 stripped-envelope supernovae,"10 March, 2022"
1420,Michael Hoffman,icecube has performed several allsky searches for pointlike neutrino sources using tracklike events including a recent timeintegrated analysis using years of icecube data this paper accompanies the public data release of these neutrino candidates detected by icecube between april and july the selection includes throughgoing tracks primarily due to muon neutrino candidates that reach the detector from all directions as well as neutrino track events that start within the instrumented volume an updated selection and reconstruction for data taken after april slightly improves the sensitivity of the sample while more than of the sample overlaps between the old and new versions differing events can lead to changes relative to the previous year event selection an a posteriori estimate of the significance of the txs flare is reported with an explanation of observed discrepancies with previous results this public data release which includes years of data and binned detector response functions for muon neutrino signal events shows improved sensitivity in generic timeintegrated point source analyses and should be preferred over previous releases less,IceCube Data for Neutrino Point-Source Searches Years 2008-2018,"27 January, 2021"
1421,Michael Hoffman,the treatment of malaria is a global health challenge that stands to benefit from the widespread introduction of a vaccine for the disease a method has been developed to create a live organism vaccine using the sporozoites spz of the parasite plasmodium falciparum pf which are concentrated in the salivary glands of infected mosquitoes current manual dissection methods to obtain these pfspz are not optimally efficient for largescale vaccine production we propose an improved dissection procedure and a mechanical fixture that increases the rate of mosquito dissection and helps to deskill this stage of the production process we further demonstrate the automation of a key step in this production process the picking and placing of mosquitoes from a staging apparatus into a dissection assembly this unit test of a robotic mosquito pickandplace system is performed using a customdesigned microgripper attached to a four degree of freedom dof robot under the guidance of a computer vision system mosquitoes are autonomously grasped and pulled to a pair of notched dissection blades to remove the head of the mosquito allowing access to the salivary glands placement into these blades is adapted based on output from computer vision to accommodate for the unique anatomy and orientation of each grasped mosquito in this pilot test of the system on mosquitoes we demonstrate a grasping accuracy and a accuracy in placing the mosquito with its neck within the blade notches such that the head can be removed this is a promising result for this difficult and nonstandard pickandplace task less,A Mosquito Pick-and-Place System for PfSPZ-based Malaria Vaccine Production,"12 April, 2020"
1422,Michael Hoffman,in their study mckinney et al showed the high potential of artificial intelligence for breast cancer screening however the lack of detailed methods and computer code undermines its scientific value we identify obstacles hindering transparent and reproducible ai research as faced by mckinney et al and provide solutions with implications for the broader field less,The importance of transparency and reproducibility in artificial intelligence research,"7 March, 2020"
1423,Michael Hoffman,as artificial intelligence and machine learning algorithms make further inroads into society calls are increasing from multiple stakeholders for these algorithms to explain their outputs at the same time these stakeholders whether they be affected citizens government regulators domain experts or system developers present different requirements for explanations toward addressing these needs we introduce ai explainability httpaixmybluemixnet an opensource software toolkit featuring eight diverse and stateoftheart explainability methods and two evaluation metrics equally important we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods not only those in the toolkit but also in the broader literature on explainability for data scientists and other users of the toolkit we have implemented an extensible software architecture that organizes methods according to their place in the ai modeling pipeline we also discuss enhancements to bring research innovations closer to consumers of explanations ranging from simplified more accessible versions of algorithms to tutorials and an interactive web demo to introduce ai explainability to different audiences and application domains together our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed less,One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques,"14 September, 2019"
1424,Michael Hoffman,we present the catalog of extragalactic hi line sources detected by the completed alfalfa survey out to z including both high signaltonoise ratio detections and ones of lower quality which coincide in both position and recessional velocity with galaxies of known redshift we review the observing technique data reduction pipeline and catalog construction process focusing on details of particular relevance to understanding the catalogs compiled parameters we further describe and make available the digital hi line spectra associated with the catalogued sources in addition to the extragalactic hi line detections we report nine confirmed oh megamasers and ten oh megamaser candidates at z whose oh line signals are redshifted into the alfalfa frequency band because of complexities in data collection and processing associated with the use of a feedhorn array on a complex singledish antenna in the terrestrial radio frequency interference environment we also present a list of suggestions and caveats for consideration by users of the alfalfa extragalactic catalog for future scientific investigations less,The Arecibo Legacy Fast ALFA Survey: The ALFALFA Extragalactic HI Source Catalog,"29 May, 2018"
1425,Michael Hoffman,the advanced ligo and advanced virgo observatories recently discovered gravitational waves from a binary neutron star inspiral a short gammaray burst grb that followed the merger of this binary was also recorded by the fermi gammaray burst monitor fermigbm and the anticoincidence shield for the spectrometer for the international gammaray astrophysics laboratory integral indicating particle acceleration by the source the precise location of the event was determined by optical detections of emission following the merger we searched for highenergy neutrinos from the merger in the geveev energy range using the antares icecube and pierre auger observatories no neutrinos directionally coincident with the source were detected within pm s around the merger time additionally no mev neutrino burst signal was detected coincident with the merger we further carried out an extended search in the direction of the source for highenergy neutrinos within the day period following the merger but found no evidence of emission we used these results to probe dissipation mechanisms in relativistic outflows driven by the binary neutron star merger the nondetection is consistent with model predictions of short grbs observed at a large offaxis angle less,"Search for High-energy Neutrinos from Binary Neutron Star Merger GW170817 with ANTARES, IceCube, and the Pierre Auger Observatory","9 November, 2017"
1426,Michael Hoffman,we study a discrete version of a geometric stable marriage problem originally proposed in a continuous setting by hoffman holroyd and peres in which points in the plane are stably matched to cluster centers as prioritized by their distances so that each cluster center is apportioned a set of points of equal area we show that for a discretization of the problem to an ntimes n grid of pixels with k centers the problem can be solved in time on log n and we experiment with two slower but more practical algorithms and a hybrid method that switches from one of these algorithms to the other to gain greater efficiency than either algorithm alone we also show how to combine geometric stable matchings with a kmeans clustering algorithm so as to provide a geometric politicaldistricting algorithm that views distance in economic terms and we experiment with weighted versions of stable kmeans in order to improve the connectivity of the resulting clusters less,Algorithms for Stable Matching and Clustering in a Grid,"7 April, 2017"
1427,Andrea Tagliasacchi,given a monocular video segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence existing solutions usually approach this problem in the image domain limiting their performance and understanding of the environment we introduce decoupled dynamic neural radiance field dnerf a selfsupervised approach that takes a monocular video and learns a d scene representation which decouples moving objects including their shadows from the static background our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes a naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting to this end we propose a novel loss to promote correct separation of phenomena we further propose a shadow field network to detect and decouple dynamically moving shadows we introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than stateoftheart approaches in decoupling dynamic and static d objects occlusion and shadow removal and image segmentation for moving objects less,D$^2$NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video,"1 June, 2022"
1428,Andrea Tagliasacchi,we present panoptic neural fields pnf an objectaware neural scene representation that decomposes a scene into a set of objects things and background stuff each object is represented by an oriented d bounding box and a multilayer perceptron mlp that takes position direction and time and outputs density and radiance the background stuff is represented by a similar mlp that additionally outputs semantic labels each object mlps are instancespecific and thus can be smaller and faster than previous objectaware approaches while still leveraging categoryspecific priors incorporated via metalearned initialization our model builds a panoptic radiance field representation of any scene from just color images we use offtheshelf algorithms to predict camera poses object tracks and d image semantic segmentations then we jointly optimize the mlp weights and bounding box parameters using analysisbysynthesis with selfsupervision from color images and pseudosupervision from predicted semantic segmentations during experiments with realworld dynamic scenes we find that our model can be used effectively for several tasks like novel view synthesis d panoptic segmentation d scene editing and multiview depth prediction less,Panoptic Neural Fields: A Semantic Object-Aware Neural Scene Representation,"9 May, 2022"
1429,Andrea Tagliasacchi,a classical problem in computer vision is to infer a d scene representation from few images that can be used to render novel views at interactive rates previous work focuses on reconstructing predefined d representations eg textured meshes or implicit representations eg radiance fields and often requires input images with precise camera poses and long processing times for each novel scene in this work we propose the scene representation transformer srt a method which processes posed or unposed rgb images of a new area infers a setlatent scene representation and synthesises novel views all in a single feedforward pass to calculate the scene representation we propose a generalization of the vision transformer to sets of images enabling global information integration and hence d reasoning an efficient decoder transformer parameterizes the light field by attending into the scene representation to render novel views learning is supervised endtoend by minimizing a novelview reconstruction error we show that this method outperforms recent baselines in terms of psnr and speed on synthetic datasets including a new dataset created for the paper further we demonstrate that srt scales to support interactive visualization and semantic segmentation of realworld outdoor environments using street view imagery less,Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations,"29 March, 2022"
1430,Andrea Tagliasacchi,we present neural descriptor fields ndfs an object representation that encodes both points and relative poses between an object and a target such as a robot gripper or a rack used for hanging via categorylevel descriptors we employ this representation for object manipulation where given a task demonstration we want to repeat the same task on a new object instance from the same category we propose to achieve this objective by searching via optimization for the pose whose descriptor matches that observed in the demonstration ndfs are conveniently trained in a selfsupervised fashion via a d autoencoding task that does not rely on expertlabeled keypoints further ndfs are seequivariant guaranteeing performance that generalizes across all possible d object translations and rotations we demonstrate learning of manipulation tasks from few demonstrations both in simulation and on a real robot our performance generalizes across both object instances and dof object poses and significantly outperforms a recent baseline that relies on d descriptors project website httpsyilundugithubiondf less,Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation,"9 December, 2021"
1431,Andrea Tagliasacchi,we present nesf a method for producing d semantic fields from posed rgb images alone in place of classical d representations our method builds on recent work in implicit neural scene representations wherein d structure is captured by pointwise functions we leverage this methodology to recover d density fields upon which we then train a d semantic segmentation model supervised by posed d semantic maps despite being trained on d signals alone our method is able to generate dconsistent semantic maps from novel camera poses and can be queried at arbitrary d points notably nesf is compatible with any method producing a density field and its accuracy improves as the quality of the density field improves our empirical analysis demonstrates comparable quality to competitive d and d semantic segmentation baselines on complex realistically rendered synthetic scenes our method is the first to offer truly dense d scene segmentations requiring only d supervision for training and does not require any semantic input for inference on novel scenes we encourage the readers to visit the project website less,NeSF: Neural Semantic Fields for Generalizable Semantic Segmentation of 3D Scenes,"2 December, 2021"
1432,Andrea Tagliasacchi,in the era of deep learning human pose estimation from multiple cameras with unknown calibration has received little attention to date we show how to train a neural model to perform this task with high precision and minimal latency overhead the proposed model takes into account joint location uncertainty due to occlusion from multiple views and requires only d keypoint data for training our method outperforms both classical bundle adjustment and weaklysupervised monocular d baselines on the wellestablished humanm dataset as well as the more challenging inthewild skipose ptz dataset less,MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision,"25 November, 2021"
1433,Andrea Tagliasacchi,we propose a selfsupervised capsule architecture for d point clouds we compute capsule decompositions of objects through permutationequivariant attention and selfsupervise the process by training with pairs of randomly rotated objects our key idea is to aggregate the attention masks into semantic keypoints and use these to supervise a decomposition that satisfies the capsule invarianceequivariance properties this not only enables the training of a semantically consistent decomposition but also allows us to learn a canonicalization operation that enables objectcentric reasoning to train our neural network we require neither classification labels nor manuallyaligned training datasets yet by learning an objectcentric representation in a selfsupervised manner our method outperforms the stateoftheart on d point cloud reconstruction canonicalization and unsupervised classification less,Canonical Capsules: Self-Supervised Capsules in Canonical Pose,"24 November, 2021"
1434,Andrea Tagliasacchi,polygonal meshes are ubiquitous but have only played a relatively minor role in the deep learning revolution stateoftheart neural generative models for d shapes learn implicit functions and generate meshes via expensive isosurfacing we overcome these challenges by employing a classical spatial data structure from computer graphics binary space partitioning bsp to facilitate d learning the core operation of bsp involves recursive subdivision of d space to obtain convex sets by exploiting this property we devise bspnet a network that learns to represent a d shape via convex decomposition without supervision the network is trained to reconstruct a shape using a set of convexes obtained from a bsptree built over a set of planes where the planes and convexes are both defined by learned network weights bspnet directly outputs polygonal meshes from the inferred convexes the generated meshes are watertight compact ie lowpoly and well suited to represent sharp geometry we show that the reconstruction quality by bspnet is competitive with those from stateoftheart methods while using much fewer primitives we also explore variations to bspnet including using a more generic decoder for reconstruction more general primitives than planes as well as training a generative model with variational autoencoders code is available at httpsgithubcomczqbspnetoriginal less,Learning Mesh Representations via Binary Space Partitioning Tree Networks,"1 July, 2021"
1435,Andrea Tagliasacchi,implicit representations of geometry such as occupancy fields or signed distance fields sdf have recently regained popularity in encoding d solid shape in a functional form in this work we introduce medial fields a field function derived from the medial axis transform mat that makes available information about the underlying d geometry that is immediately useful for a number of downstream tasks in particular the medial field encodes the local thickness of a d shape and enables o projection of a query point onto the medial axis to construct the medial field we require nothing but the sdf of the shape itself thus allowing its straightforward incorporation in any application that relies on signed distance fields working in unison with the o surface projection supported by the sdf the medial field opens the door for an entirely new set of efficient shapeaware operations on implicit representations we present three such applications including a modification to sphere tracing that renders implicit representations with better convergence properties a fast construction method for memoryefficient rigidbody collision proxies and an efficient approximation of ambient occlusion that remains stable with respect to viewpoint variations less,Deep Medial Fields,"7 June, 2021"
1436,Andrea Tagliasacchi,capsule networks aim to parse images into a hierarchy of objects parts and relations while promising they remain limited by an inability to learn effective low level part descriptions to address this issue we propose a way to learn primary capsule encoders that detect atomic parts from a single image during training we exploit motion as a powerful perceptual cue for part definition with an expressive decoder for part generation within a layered image model with occlusion experiments demonstrate robust part discovery in the presence of multiple objects cluttered backgrounds and occlusion the part decoder infers the underlying shape masks effectively filling in occluded regions of the detected shapes we evaluate flowcapsules on unsupervised part segmentation and unsupervised image classification less,Unsupervised part representation by Flow Capsules,"19 February, 2021"
1437,Andrea Tagliasacchi,we introduce a technique for d human keypoint estimation that directly models the notion of spatial uncertainty of a keypoint our technique employs a principled approach to modelling spatial uncertainty inspired from techniques in robust statistics furthermore our pipeline requires no d ground truth labels relying instead on possibly noisy d imagelevel keypoints our method achieves near stateoftheart performance on humanm while being efficient to evaluate and straightforward to less,Human 3D keypoints via spatial uncertainty modeling,"18 December, 2020"
1438,Andrea Tagliasacchi,we propose a deep network that can be trained to tackle image reconstruction and classification problems that involve detection of multiple object instances without any supervision regarding their whereabouts the network learns to extract the most significant topk patches and feeds these patches to a taskspecific network eg autoencoder or classifier to solve a domain specific problem the challenge in training such a network is the nondifferentiable topk selection process to address this issue we lift the training optimization problem by treating the result of topk selection as a slack variable resulting in a simple yet effective multistage training our method is able to learn to detect recurrent structures in the training dataset by learning to reconstruct images it can also learn to localize structures when only knowledge on the occurrence of the object is provided and in doing so it outperforms the stateoftheart less,MIST: Multiple Instance Spatial Transformer Network,"4 December, 2020"
1439,Andrea Tagliasacchi,with the advent of neural radiance fields nerf neural networks can now render novel views of a d scene with quality that fools the human eye yet generating these images is very computationally intensive limiting their applicability in practical scenarios in this paper we propose a technique based on spatial decomposition capable of mitigating this issue our key observation is that there are diminishing returns in employing larger deeper andor wider networks hence we propose to spatially decompose a scene and dedicate smaller networks for each decomposed part when working together these networks can render the whole scene this allows us nearconstant inference time regardless of the number of decomposed parts moreover we show that a voronoi spatial decomposition is preferable for this purpose as it is provably compatible with the painters algorithm for efficient and gpufriendly rendering our experiments show that for realworld scenes our method provides up to x more efficient inference than nerf with the same rendering quality or an improvement of up to db in psnr for the same inference cost less,DeRF: Decomposed Radiance Fields,"24 November, 2020"
1440,Andrea Tagliasacchi,in this technical report we investigate extending convolutional neural networks to the setting where functions are not sampled in a grid pattern we show that by treating the samples as the average of a function within a cell we can find a natural equivalent of most layers used in cnn we also present an algorithm for running inference for these models exactly using standard convex geometry algorithms less,Voronoi Convolutional Neural Networks,"21 October, 2020"
1441,Andrea Tagliasacchi,efficient representation of articulated objects such as human bodies is an important problem in computer vision and graphics to efficiently simulate deformation existing approaches represent d objects using polygonal meshes and deform them using skinning techniques this paper introduces neural articulated shape approximation nasa an alternative framework that enables efficient representation of articulated deformable objects using neural indicator functions that are conditioned on pose occupancy testing using nasa is straightforward circumventing the complexity of meshes and the issue of watertightness we demonstrate the effectiveness of nasa for d tracking applications and discuss other potential extensions less,NASA: Neural Articulated Shape Approximation,"28 July, 2020"
1442,Andrea Tagliasacchi,any solid object can be decomposed into a collection of convex polytopes in short convexes when a small number of convexes are used such a decomposition can be thought of as a piecewise approximation of the geometry this decomposition is fundamental in computer graphics where it provides one of the most common ways to approximate geometry for example in realtime physics simulation a convex object also has the property of being simultaneously an explicit and implicit representation one can interpret it explicitly as a mesh derived by computing the vertices of a convex hull or implicitly as the collection of halfspace constraints or support functions their implicit representation makes them particularly well suited for neural network training as they abstract away from the topology of the geometry they need to represent however at testing time convexes can also generate explicit representations polygonal meshes which can then be used in any downstream application we introduce a network architecture to represent a low dimensional family of convexes this family is automatically derived via an autoencoding process we investigate the applications of this architecture including automatic convex decomposition image to d reconstruction and partbased shape retrieval less,CvxNet: Learnable Convex Decomposition,"12 April, 2020"
1443,Andrea Tagliasacchi,voronoi diagrams are highly compact representations that are used in various graphics applications in this work we show how to embed a differentiable version of it via a novel deep architecture into a generative deep network by doing so we achieve a highly compact latent embedding that is able to provide much more detailed reconstructions both in d and d for various shapes in this tech report we introduce our representation and present a set of preliminary results comparing it with recently proposed implicit occupancy networks less,VoronoiNet: General Functional Approximators with Local Support,"8 December, 2019"
1444,Andrea Tagliasacchi,we extend the formulation of positionbased rods to include elastic volumetric deformations we achieve this by introducing an additional degree of freedom per vertex isotropic scale and its velocity including scale enriches the space of possible deformations allowing the simulation of volumetric effects such as a reduction in crosssectional area when a rod is stretched we rigorously derive the continuous formulation of its elastic energy potentials and hence its associated positionbased dynamics pbd updates to realize this model enabling the simulation of up to dofs at hz in our gpu implementation we further show how rods can provide a compact alternative to tetrahedral meshes for the representation of complex muscle deformations as well as providing a convenient representation for collision detection this is achieved by modeling a muscle as a bundle of rods for which we also introduce a technique to automatically convert a muscle surface mesh into a rodsbundle finally we show how rods andor bundles can be skinned to a surface mesh to drive its deformation resulting in an alternative to cages for realtime volumetric deformation less,VIPER: Volume Invariant Position-based Elastic Rods,"12 June, 2019"
1445,Andrea Tagliasacchi,we propose an automatic method for generating highquality annotations for depthbased hand segmentation and introduce a largescale hand segmentation dataset existing datasets are typically limited to a single hand by exploiting the visual cues given by an rgbd sensor and a pair of colored gloves we automatically generate dense annotations for two hand segmentation this lowers the costcomplexity of creating high quality datasets and makes it easy to expand the dataset in the future we further show that existing datasets even with data augmentation are not sufficient to train a hand segmentation algorithm that can distinguish two hands source and datasets will be made publicly available less,HandSeg: An Automatically Labeled Dataset for Hand Segmentation from Depth Images,"2 August, 2018"
1446,Andrea Tagliasacchi,when representing a solid object there are alternatives to the use of traditional explicit surface meshes or implicit zero crossing of implicit functions methods skeletal representations encode shape information in a mixed fashion they are composed of a set of explicit primitives yet they are able to efficiently encode the shapes volume as well as its topology i will discuss in two dimensions how symmetry can be used to reduce the dimensionality of the data from a d solid to a d curve and how this relates to the classical definition of skeletons by medial axis transform while the medial axis of a d shape is composed of a set of curves in d it results in a set of sheets connected in a complex fashion because of this complexity medial skeletons are difficult to use in practical applications curve skeletons address this problem by strictly requiring their geometry to be one dimensional resulting in an intuitive yet powerful shape representation in this report i will define both medial and curve skeletons and discuss their mutual relationship i will also present several algorithms for their computation and a variety of scenarios where skeletons are employed with a special focus on geometry processing and shape analysis less,Skeletal Representations and Applications,"25 June, 2014"
1447,Adrian Butscher,we present a method for enforcing manufacturability constraints in generated parts such that they will be automatically ready for fabrication using a subtractive approach we primarily target multiaxis cnc milling approaches but the method should generalize to other subtractive methods as well to this end we take as user input the radius of curvature of the tool bit a coarse model of the tool head and optionally a set of milling directions this allows us to enforce the following manufacturability conditions surface smoothness such that the radius of curvature of the part does not exceed the milling bit radius orientation such that every part of the surface to be milled is visible from at least one milling direction accessibility such that every surface patch can be reached by the tool bit without interference with the tool or head mount we will show how to efficiently enforce the constraint during level setbased topology optimization modifying the advection velocity such that at each iteration the topology optimization maintains a descent optimization direction and does not violate any of the manufacturability conditions this approach models the actual subtractive process by carving away material accessible to the machine at each iteration until a local optimum is achieved less,A subtractive manufacturing constraint for level set topology optimization,"19 February, 2020"
1448,Adrian Butscher,we present a generalization of the bilateral filter that can be applied to featurepreserving smoothing of signals on images meshes and other domains within a single unified framework our discretization is competitive with stateoftheart smoothing techniques in terms of both accuracy and speed is easy to implement and has parameters that are straightforward to understand unlike previous bilateral filters developed for meshes and other irregular domains our construction reduces exactly to the image bilateral on rectangular domains and comes with a rigorous foundation in both the smooth and discrete settings these guarantees allow us to construct unconditionally convergent meanshift schemes that handle a variety of extremely noisy signals we also apply our framework to geometric edgepreserving effects like feature enhancement and show how it is related to local histogram techniques less,A General Framework for Bilateral and Mean Shift Filtering,"30 April, 2014"
1449,Adrian Butscher,the gluing technique is used to construct hypersurfaces in euclidean space having approximately constant prescribed mean curvature these surfaces are perturbations of unions of finitely many spheres of the same radius assembled endtoend along a line segment the condition on the existence of these hypersurfaces is the vanishing of the sum of certain integral moments of the spheres with respect the prescribed mean curvature function less,A Gluing Construction for Prescribed Mean Curvature,"19 February, 2009"
1450,Chris Landreth,we present rignet an endtoend automated method for producing animation rigs from input character models given an input d model representing an articulated character rignet predicts a skeleton that matches the animator expectations in joint placement and topology it also estimates surface skin weights based on the predicted skeleton our method is based on a deep architecture that directly operates on the mesh representation without making assumptions on shape class and structure the architecture is trained on a large and diverse collection of rigged models including their mesh skeletons and corresponding skin weights our evaluation is threefold we show better results than prior art when quantitatively compared to animator rigs qualitatively we show that our rigs can be expressively posed and animated at multiple levels of detail and finally we evaluate the impact of various algorithm choices on our output rigs less,RigNet: Neural Rigging for Articulated Characters,"5 July, 2020"
