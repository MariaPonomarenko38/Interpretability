id,title,abstract,supervisor
cc9400fc-9c76-48ab-9120-e13c12566f6a,adapting contextual bandit algorithms to adserving platforms,contextual bandit algorithms are developed to gain maximum rewards from an unknown distribution in limited trials by exploiting contextual information they can be adapted to yield solutions for adserving systems with a short campaign time several new contextual bandit algorithms have been proposed in recent years but their performance is poorly understood in realworld scenarios this study tests these algorithms particularly the adaptivegreedy algorithm david cortes  in multiple simulated and realworld datasets we found that the adaptivegreedy algorithm can achieve up to  clickthrough rate improvement compared to the widely used bootstrappedthompson sampling algorithm bts at the same time the adaptivegreedy algorithm can be up to  times faster in training and predicting than the bts algorithm offpolicy evaluation ope which allows us to evaluate our new models from the log data generated by other policies provides vital backtesting results to inform new policy decisions to this end we study and compare different ope methods and use them to evaluate our proposed algorithms  reference david cortes adapting multiarmed bandits policies to contextual bandits scenarios aikaijian zhongabhishek ghose emma n,Leonard Wong
7e2ff01d-5a39-4a23-a14f-9e1c603b107d,defending white box adversarial examples with a guided denoiser,as deep neural networks become increasingly common in security conscious applications such as selfdriving cars or malware detection protecting networks from manipulation by an attacker has become increasingly important however neural networks trained to classify images are vulnerable to mistakes caused by images imperceptibly altered by an adversary existing defences against adversarial examples either require extensive retraining as in adversarial training or have been shown to be ineffective against an adversary that is aware of the defense a white box adversary as in the case of many preprocessing defenses we show that a denoising neural network can be trained to provide white box defense extending previous work which showed effectiveness only against an adversary unaware of the defense our defense provides comparable clean and robust accuracy to adversarial training when applied to the model used to guide the denoiser during training additionally the denoiser was transferred to other models trained on the same dataset where it provided a  increase in robust accuracy on average when compared to the same models with no defense the defenseamdalexander cannihab amer,Gennady Pekhimenko
b2fefba8-a7d4-4998-ac38-e1e761692ea2,modern gameplay test automation with reinforcement learning,gameplay testing on computer games to help ensure high per formance for graphics cards is often per formed manually resulting in substantial time and cost expenditures though many reinforcement learning rl studies have been conducted on gameplay most tend to focus on older game titles and have access to internal game information through apis this projectxexxas objective is to develop a solution capable of automating a limited time  minutes playthrough on a modern game title with rl without relying on access to internal game information  the solution focuses on automating player navigation to make progress in singleplayer games we collect screenshots during realtime gameplay as the rl agent traverses the game environment which we then put through image analysis and computer vision tools for processing to serve as observations for the agent a neural network then handles processing of policy updates during training timesteps using the proximal policy optimization ppo method the model is qualitatively evaluated by observing fully automated gameplay which has shown the current solution to effectively maneuver through navigationfocused areas as well as some instances when it is necessary for the agent to correctly react to ingame interaction prompts future work will involve training the model on different types of gameplay and on transfer learning to new titles the results from this project will ultimately demonstrate the ability of rl to perform well in modern gameplay amdallen baoyonas bedasso max kiehn vinay panditamir,Amir-massoud Farahmand
ab8e4985-bf0e-4486-9fa2-0401915b3bff,reducing data loading time in machine learning by reusing minibatches,training of models in machine learning is typically done by dividing a dataset into equally sized subsets called minibatches at each step of training a minibatch is loaded to the gpu where it is used once to determine updates to parameters if data loading is the bottleneck then this process will lead to idle time for the gpu as it waits for the next minibatch wasting precious gpu compute resources thus we propose a change to the training procedure and suggest reusing minibatches that have already been loaded into the gpu to perform additional parameter updates while waiting for the next minibatch if the number of times each minibatch is used is kept the same as regular training then this idea will lead to a speed up in training time an unintended consequence of minibatch to remedy this we modify the technique to adaptively adjust the amount of reuse based on how the model performs on unseen data we have evaluated our technique on some example models with data loading bottleneck and found that on average we get around xcx speed up with no loss in model accuracy some next steps include trying out more examples to see if the results we can get more speed upamdjafeer uddinabhinav vishnuma,Maryam Mehri Dehnavi
ef4c61ef-ed8a-475e-849e-ac9d063318e8,hybrid recommender system for applications based on heterogeneous commercial data,currently developed recommender system in the application marketplaces focuses only on the surface interactions between users and items which limits the use of information from various sources to further improve the existing recommendation engine we use a wider range of user information such as the browsing history in the website moreover the model can take advantage of applicationsxexxa information including their properties and descriptionsthe productsxexxa information is embedded by a pretrained language model the embeddings provide a better understanding for our model to recommend machine with the neural network to create an endtoend model this model can learn combined implicit features as well as corresponding low and highorder interactions simultaneously proposed model compared with existing baseline models the results demonstrate that our proposed advisor team to provide an interpretation of our results which also proved that our improved system is more effective from usersxexxa perspective as we aim to continually enhance our recommender system the next steps would involve researching how to represent and integrate more data such as utilizing a knowledge graph or the sequential interactions in our modelappdirectyiwen fengdominic,Peter Marbach
8419c27f-5df9-4599-9fd5-e1d3c26ba5e5,apifuzz introspection assisted fuzzer for qn x resource managers,blackberry qnx is a microkernel realtime operating system built for the worldxexxas most critical embedded systems resource managers are widely used for local communication between processes in qnx system these critical processes are usually stateful and accessed by other applications using portable operating system interface fuzzing is a dynamic analysis security testing technique that involves providing random or invalid data as inputs to programs a greybox fuzzer leverages coveragefeedback to learn how to reach deeper execution states of the program it often uses instrumentation to get runtime coverage for guiding the test case generation which  examines the state of the binary without modifying it by introducing a new introspectionbased graybox fuzzing tool targeted at resource managers in both kernel and user mode the experiments show that the tool using introspection explores more program states than a corresponding blackbox fuzzer  the next step involves experimenting with more complex resource managers and the kernel           blackberrycongwei chenglenn,David Lie
a8c3717b-f93f-404a-8297-4953e8675a1b,taking the first steps towards building a hardware bill of materials,cost of increasing complexity vastness and reliance on a plethora of components and libraries not all of which are known intimately to developers testers and consumers of these software systems leaving them exposed to risks and vulnerabilities  blackberryxexxas jarvis a software compositional analysis platform breaks down software binaries into a software bill of materials and highlights potential vulnerabilities by severity category that help customers ensure that their software supply chains are secure in an effort to rise against competition and to lead the market in terms of capability and per formance our project attempted to the hardware and processor from the software binaries uploaded by customers providing a more comprehensive picture of the potential threat landscape surrounding the software being used by them  lineage closely inspecting various properties and features across generations to build a ladder of potential detection strategies for arm families and processors having synthesized a working list of differences for most used arm architectures we disassembled sample binaries and wrote pattern matching rules to detect the variant versions identify them and map them to known cves surrounding the processor and architecture  arming the jarvis tool with this functionality will result in a broader and deeper understanding of the security and risk architecture of a customerxexxas software and hardware infrastructure and consequently grant blackberry the ability to assess the security posture more holistically by mapping more relevant vulnerabilities and patches continuing this project and research will enable jarvis to be a market leader in its class and empower users to know more about the software they interact with use and sellblackberrydivas kapurcris sinnott billy mccourt paul hirstishtiaque ahme,"Ishtiaque Ahmed, Courtney Gibson"
97db2995-fb89-4e7d-9ac0-31d2bec954c8,automated domain ,the problem of automated essay scoring aes is complex and solving the problem will have for their learning the goal of this project is to provide a complete version of the software platform to the educators to assist their essay grading to achieve this goal the company automates the process of grading studentsxexxa essays and instantly provides consistent and personalized feedback at scale the main task is to analyze the performance of the machine learning ai models and create a road map to enhance the model the ai models do not evaluate sentence structures or grammar format but evaluate the context of the student responses and look for the key concepts that educators are looking for  to solve this problem the bert model is used with data from stack exchange which primarily serves as a platform for users to ask and answer questions meanwhile other methods such as lstm and wordvec are used as baseline scores outcomes of the bert model is very promising with  out of  tags can achieve accuracy higher than  which are far better than other approaches in the future we are going to build automated personalized feedback modules for students to further assist their studyblees aiyuanhao lousojin lee alexandre bo,Linbo Wang
d790a0b5-91ef-47da-8649-cbc8729d0fba,secure crossservice genomic data federated analysis with graphql,extremely diverse data types such as genomics transcriptomics imaging clinical records and more performing complex analysis across these health data in different data stores can be challenging the health data research we propose that graphql can be used in the combination of a single decision data services to the policy point our graphql interface can only query the authorized data from different services  we demonstrated that secure and private data analysis and machine learning model training can be achieved with graphql our results show that our graphql interface is able to return machine learning convenient way to query here we illustrate that querying from our graphql interface transfer fewer bytes than querying from the traditional restful api interfaces to the client our graphql interface provides a sturdy ground for federated learning in the future we hope to enable different research institutions to perform federated learning by using our graphql interfacexexxas machine learning querycandigsiyue wangjonathan du,Yun William Yu
ae6f783e-555a-40b0-8aec-163d6b8e9a0a,forecasting future fundamental metrics from earnings call transcripts using grml to guide machine learning solutions,an earnings call is a conference call between the management of a public company analysts xxexxin this report we explore various nlp and deep learning algorithmsmodels that can be used to extract features from the earnings calls in our dataset and later use them to predict the companyxexxas future fundamentals the noisy nature of our target and the extremely long length of the transcripts make training existing deep learning models challenging even though bilstm with attention is considered to be the best model for this task we show that making certain changes to the data and model architecture and using lower complexity models like lda and lstms with fewer parameters helps generalize well to outofsample data points finally we use stateoftheart transformerbased models designed to handle long sequence lengths for the same task and present our resultscpp investmentsaneek dasbrend,Eric Yu
a2dcfdce-78f9-4a1c-8cd1-aa162082bb1e,asset vision a generalized approach to asset tag extraction,organizations seeking to optimize their maintenance and procurement strategies have realized that the necessary contextual data for their core assets rest in physical piping and instrumentation diagrams pids hiring engineers to manually identify these assets called asset tags is timethis way and prohibitively expensive  deloitte has developed a technology that can use machine learning and computer vision techniques to extract the asset tags present in a document a por tion of the technology involves a xefxacxmerge we have enhanced the model to learn when to merge the appropriate texts from the data without delivery to new clients  by leveraging a new merge candidate proposal algorithm and creating computer vision features achieve superior precision and recall scores on the test dataset compared to the corresponding models in the prototype pipeline the number of asset tags accurately extracted on the test dataset by the new pipeline is similar to that of engineers while only taking a fraction of the time future work will focus on minimizing the number of optical character recognition errorsdeloittebrendan kolisnikjerod,Radu Craiu
1c32f33e-ffbc-4ac5-b8f0-cc751f55caa4,using a temporal convolutional network for demand forecasting in retail,retailers face crucial replenishment decisions such as which products to restock and when stocking too few items will lead to stockouts and revenue loss while stocking too many items will result in decreased working capital higher costs and stale inventory to inform replenishment decisions many retailers predict future product demand using a rolling yearly average of product sales while the yearly average is a reasonable demand indicator for products that sell frequently it fails to capture demand for infrequent purchases and does not account for seasonality or the effects of promotional price changes on demand current ai demand forecasting solutions improve upon industry standards by capturing seasonality and promotional changes however these solutions are not effective for products that sell infrequently these products contribute are also laborious expensive and timeconsuming to implement due to the vast feature engineering required deep learning solutions are capable of circumventing lengthy feature engineering processes hence a temporal convolutional network tcn was built to perform feature extraction the tcn model architecture was designed and adapted to capture both shortterm and longterm seasonal trends in the sales data forecasting one week into the future on products that frequently sell the retail accuracy of the tcn model was  compared to  for the yearly average and  for the current ai solution mlp forecasting one week into the future on items that do not sell frequently the retail accuracy of the tcn model was  compared to  accuracy for the yearly average and  for the mlp although the tcn closely matches the performance of the mlp for frequently selling items the model outperforms both the yearly average and the mlp for infrequently selling products the results exhibit that the tcn which requires far less engineering than the mlp does shows much promise as a demand forecasting solution particularly proving strong for sparse data where features that capture longrange seasonality are extracted effectively open research questions remain such as whether the model is capable of learning the price elasticity of products given past demand and pricing informationdeloittesasha nandajoe roussyarvind gup,"Arvind Gupta, Huaxiong Huang"
8a75a6b3-26fa-4432-8a7c-f2c8a3ec6e99,evaluating and debiasing feature embeddings for condensates,drug discovery is an involved task including close collaboration between biologists data scientists and engineers unfortunately this process can take several years to develop a single drug due to the  we aim to see if deep convolutional neural networks dcnns can be used to learn morphologically distinguish a biological phenomenon called condensates  condensates are liquidlike compartments of molecules for key biochemical processes they play a screens hts labels related to condensates are rare fortunately condensates are uniquely a weakly supervised manner on these signals to develop weakly supervised embeddings wse  we conduct our experiments on the human protein atlas hpa an opensourced dataset that contains signals weakly related to condensates we show that a model can distinguish phenotypically different condensates whilst also separating images that are phenotypically dissimilar finally models that can recognize condensate embeddings well also scored generally higher in our quantitative metrics in our next steps we aim to evaluate on additional datasets and employ modern machine learning techniques to the scarce label problem such as selfsupervised learningdewpoint therapeuticslalit lalxcxtienne dumoulin grant watson kshiti,Alan Moses
de5b92e6-5079-4074-8335-0e310adee4ea,weakself supervised learning for drug discovery,weakly supervised learning is a relatively new technique used in drug discovery high content  tasks hence xefxacxweak supervisionxefxacx helping to discover novel mechanisms of action and localizations patterns within cells etc while these characteristics are not presented to the model during training they are extremely important for understanding the core biology and function of a drug and greatly accelerate the drug discovery process in our study we used several variations of selfsupervised models which leverage the contrastive learning framework to learn appropriate representations from  the proposed work is evaluated on the annotated bbbc dataset which is a publicly available treated with chemical compounds of known activities we observed that without using any labels or postprocessing the selfsuper vised model achieved similar performance as the weakly supervised modeldewpoint therapeuticszhihuan yuxcxtienne dumoulin kshitij gupta grant wa,Igor Jurisica
519991fb-4d8b-4dab-971f-1d55e1c3c0ee,assessing driving risk with enhanced telematics data,insurance understanding and assessing driving behavior and driversxexxa risks are drawing more and more attention most telematicsbased evaluation metrics reduce millions of rows of data into a few scalar features losing useful information in the process in this project we adopt the xefxacxspeedintroducing a similar speedacceleration heatmap for sideways acceleration and a contextualsmall percentage of whom had incurred damages to the car we show that compared to models with the usual scalar features the introduced heatmaps allow models to achieve higher prediction maneuvers are riskier than others the outputs of the model can also be used to produce xefxacxdriver safety scoresxefxacx with very reasonable distribution finally we look forward to improving this work in the future by utilizing more data and more sophisticated model structures such as selfattention modules  reference wxcxbcthrich mario  covariate selection from telematics car driving data european actuarial journal  szgeotabjiawei yuwillem petersenandrei ba,"Andrei Badescu, Sheldon Lin"
b404dbb7-1541-49c5-9bd2-abe46c41a6ef,collision detection with highrate telematics data,road safety affects all drivers passengers and pedestrians not just geotab customers telematics data is typically heavily compressed however geotab has a data capture logic that can capture events the objective is to use this to detect vehicular collisions the methodology behind this project consisted of  major stages first a complete background research on telematics data and collision detection was completed this includes learning from domain experts what the telematics data for potential collisions may appear as next features were extracted from the raw telematics was calculated by using hundreds of handlabeled and known collisionsnear misses the best performing machine learning model was a deep autoencoder with kmeans clustering on the encoded space clear separation between types of events were formed from the encoding with higher sampling frequency on the telematics data it is easier to accurately classify both collisions and near misses this is evident in the ability to capture low g events in terms of next steps more labels would be necessary to have promising models using supervised learning on timeseries data which may have potential to outperform the deep autoencodergeotabjack elliswillem petersenandrei ba,"Andrei Badescu, Sheldon Lin"
c59a98a9-3684-4c6d-bc9d-0423ca334d94,evaluation of ifit as an oam tool,operations administration and maintenance oam methods are necessar y for internet service performance measurement protocols developed by huawei to monitor network quality unlike traditional performance measurement tools such as ping test that make indirect measurements by sending extra packets ifit uses a new type of performance measurement mechanism it tracks the to make measurements with higher precision and smaller granularity  studied in this project we built a small test environment to evaluate ifit and compared it with other commonly used measurement protocols in measurement precisions and impacts on network throughput current results show ifit can reach a constant  deviation from the true value when measuring frame loss but it causes almost  of throughput degradation while other protocols nearly have no effect these results show directions for future improvement of ifithuawei technologies chinatianyu wangf,Baochun Li
660acfe3-c709-4cab-9abc-acb7ca79136a,online learning and teaching math editor,creating effective tools to enhance learning online has been a goal many are attempting to enhance hypatia system is one of those company xexxas whose mission is to create an innovative platform that enhances the online learning experience for students and for instructors hypatia learn is a tool for teachers to design problems for students and for students to work through the problem with the ability to check their work along the way during the covid pandemic academic institutions have had to quickly move to online instruction delivery to comply with public health guidelines as a result the learning experience has dramatically changed increasing the need for effective tools for online learning the goal of our project is to extend a powerful math editor that allows teachers to design elaboratecomplexrich mathematical problems to expand the capabilities of the system to allow teachers to create problems in calculus furthermore we expand the software to support the correctness checking and student feedback for various calculus computation and provide hints to students when they make mistakes  in order to expand parsing we make use of data structures such as abstract syntax trees and design a context free grammar and use an opensource framework which can perform various mathematical computation that is used to perform mathematical validation our approach enables the creation of a rich set of calculus problems such as related rates problems and implicit differentiation we also enhanced the errorhint reporting system to provide students with warning on their computations for example we provide warning when students use variables that are not instantiated or unnecessary for solving the problem this feedback motivates students to write clean and precise solutions to problems  for future work we look to investigate how to provide students with insight on how to solve problems by providing hints on next steps they can take to obtain the solutionthis would better mimic how a teacher would approach helping a student in class one way to approach this is to examine the correct solutions and identify where the studentxexxas solution deviated from the correct hypatia systemsram gurramladislav stacho jan manu,Fanny Chevalier
df5ba66e-7c0e-4a3d-84a1-b574eaba475a,designing a multimodal prediction model through voice data mining for retail banking,a large majority of organization cutting across industries and domains rely heavily on customer care agents to help them pitch their products to the customers and drive sales at icici bank these agents are referred to as virtual relationship managers vrms currently every vrm is scored based on various quantitative metrics like number of calls every day number of tickets generated etc the primary objective of this project is to develop a machine learning model which incorporates voice call transcripts and the associated key performance indicator kpi metadata to evaluate the performance of the vrms some of the kpis are xcx a the conversational pace b sentiment tagging and c greetings at the beginningend of the call this model trains over the freeform call transcripts and the categorical and numerical kpi metadata and gives a binary rating to the vrms by  percentage points in predicting the performance of the vrm we intend to keep adding onto the kpi metadata to further improve the prediction model and also increase the number of prediction classes in the futureicici banksaarthak sangamnerkarsandipan,Mark Chignell
11c2c129-81d8-48e6-9a8f-1684aed2ada8,dampingbot learning to stabilize objects inhand during highspeed manipulation,in kindred nonrigid items such as clothing apparels are being handled frequently by the robotic arms the problem of dangling can happen when they are being manipulated at high speed this can tremendously affect any downstream task like scanning and placement in this work we present a reinforcement learning approach to learn an active damping policy the policy controls a relatively slow robotic arm to stabilize a rapidly moving and underactuated object with a target position training is carried out in a simulation that is domain randomized with multiple parallel simulator we are able to train a policy network using thousands of concurrent environments in one consumerlevel gpu within an hour we demonstrate that the policy can be transferred to reality intervention currently we are planning to replace the pendulum with a polybag in a gripper so that we can design some downstream tasks closer to practical use cases such as placement we will then measure the success rate in those tasks to evaluate the performance of the algorithmkindred aiyip sang leungbr yan,Animesh Garg
2bb95c92-fb2b-433e-a41c-a0a610a31713,machine learning techniques to optimize the performance of an automated robotic arm,the main theme of this project is improving a system that controls a robotic arm working in an automated warehouse primarily trying to resolve some common failure modes like double picks previous approaches to this problem involved expensive and errorprone handlabeling leading to unsatisfying performance of the supervised methods we have shown that a weakly supervised training method for a double pick detector is a viable alternative that doesnxexxat require any labeled the realworld tests to be carried out also we worked on an average reward reinforcement learning approach that would improve the coordination of various systems involved in the operation of the robotic arm which is under developmentkindred aiborys bryndakjan,Animesh Garg
ee8ea035-a41d-4260-bac8-2b8e22621aba,multiobject tracking in production environments,today the problem of tracking multiple objects is typically solved using deep networks that require framebyframe annotations during the training step this project explores an approach to perform multiobject tracking applied to robotic picking in cases where realworld framebyframe annotations are unavailable  instead of training a deep tracker tracking is done by the paradigm of trackingbydetection trackingbydetection is performed by having a pretrained object detector predict objects at each frame and the detections are matched with detections in the subsequent frame to perform matching similarity measures between detections are obtained from object characteristics in this project an instance segmentation network was used instead of an object detector as instance segmentation provides a mask for each object detected objects can be matched based not only on the characteristics of their bounding boxes but also on the characteristics of their masks  although this tracker cannot be directly evaluated using standard tracking metrics as framebyframe annotations are unavailable evaluations of downstream applications of this tracker can be kindred aiwilliam ngolavi shpigelm,Florian Shkurti
a873a1cf-d2f7-4d64-b8bd-bc39cc58bb3c,exploring visualization and analysis methods of manufacturing data,the kraft heinz company generates massive volumes of rich and complex data through the process of manufacturing food products it usually takes the management team a great amount of time to collect and analyze these data in order to improve their management this project aims to design a visualization dashboard which conveys key information to the management team helping them to compare the manufacturing cost at different times and analyze the sources of cost increase we xxexxthe factors associated with total cost increase informed by user requirements that we gathered from members of the management team we designed an initial static dashboard that contains all the necessary tables and charts to support their analytical needs through informal regular we integrated usersxexxa feedback in a revised interactive and dynamic dashboard design that includes additional features that better support usersxexxa analytical needs the user experience has also improved according to usersxexxa feedback as an immediate next step we plan to conduct a formal evaluation of our prototype through a controlled experiment or deployment study possible future directions to expand our work include supporting prediction by machine learning using additional data to enrich the analysis or leveraging computation to recommend strategies automaticallykraft heinzpeifeng zhuhumberto consolo holan,Fanny Chevalier
66bafe4c-bfdf-4d31-90aa-9e2368505dcb,bag of little bootstraps and its subsample construction,the bootstrap is a commonly used resampling method to compute quality measures such as machine learning models where the quality measure is often hard to compute in closed forms by randomly sampling with replacement and train ml models for each of the bootstrap samples then the predictions from these models estimate the sampling distribution of the prediction a major drawback of this method is that it is too timeconsuming to train multiple ml models for those resamples when the sample size grows large the bag of little bootstraps blb is a recently during resampling however the blb draws the subsets completely randomly and it often requires many resamples to converge to the ground truth boosted decision trees next we studied the scenario where one has access to a reliable metric that characterizes similarities and distinctions between datapoints we found out the number of distinct data points in the subsets of blb is proportional to the accuracy of blb to sample more distinct data points in the subset we use kmeans to group similar data points together in a cluster intervals acquired by our method achieve a  higher intersectionoverunion rate on binary taewoo kimkin kwan leung barum rho,Rahul G. Krishnan
e6abe4c1-237e-4989-a2d9-2a4fb6a3e34d,knowledge modulated multimodal sentiment analysis,our world is inherently multimodal and recent work has highlighted the importance of machine learning models leveraging multiple streams of information in making decisions multimodal sentiment analysis has been an active area of research that requires models to take advantage of the linguistic acoustic and visual signals available in an utterance however most current models do not consider any social commonsense knowledge which is crucial in how we perceive sentiment in a conversation to address that in this project we modulate modality representations with commonsense knowledge obtained from a generative social commonsense knowledge base we provide a novel way to modulate the linguistic acoustic and visual features corresponding to an utterance by scaling and shifting these representations we use the knowledge base to obtain knowledge latent representations for an utterance corresponding to different states of the speaker such as the intent and the reaction and use it to shift and scale the three modalities our experiments on popular multimodal sentiment analysis benchmark datasets show that our proposed method is on par and often surpasses the current stateoftheart modelssourav bhattacharjeefelipe p,Frank Rudzicz
9670f02b-fc6a-4ab3-b860-36eaa7d274c9,nonparametric reinforcement learning,method allows us to modify the number of options in a datadriven manner by using bayesian nonparametric methods with a gem prior and variational methods we demonstrate empirically that our valentin villecrozeharry braviner gabriel loa,Chris Maddison
0d4d8474-42bb-4edd-9406-468d5c31f2db,texttovideo crossmodal attention in retrieval,in textvideo retrieval the objective is to learn a crossmodal similarity function between a text and a video the purpose is to use similarity to rank videos given text queries or to rank texts given video queries however videos inherently express a much wider gamut of information than texts so a text generally cannot fully capture the entire contents of a video instead texts are most semantically similar to subsets of videos which means there are multiple equally valid texts that can plausibly match a particular videoto mimic this behaviour in retrieval a model should only examine the most similar video regions described in a text to extract the most relevant information yet most existing works aggregate video frames in time without directly considering text such as through meanpooling or selfattention as such we have designed a crossmodal attention model to learn to reason between a text and video frames we adapted a scaled dot product attention for a text to attend to its most similar frames and then we generated an aggregated video representation conditioned on those frames our method was built on top of a pretrained clip model designed to match texts and images following the success of recent works we evaluated our method on the benchmark retrieval datasets of msrvtt msvd and lsmdc and obtained stateoftheart results extract important visual cues according to text our next steps are to examine the robustness of our model to noisy and irrelevant frames through experimental analysis as par t of future work we plan on applying our method to other textvideo tasks such as visual question answeringnoel vouitsisguangw,Animesh Garg
fe082792-58d6-4c0c-89af-a1bdd6efe9a7,reducing popularity bias in recommender system,in online shopping recommender systems are algorithms that recommend relevant products to customers based on their purchase history many works of literature have pointed out that a recommender system tends to recommend popular products however an ideal system should not only consider popularity bias but also provide personalized recommendations to customers to provide better email recommendations at loblaws we aim to depopularize our recommendations and help customers discover new products  in this project we implemented a new recommender system called nceplrec noise contrastive estimationprojected linear recommendation the model has two components noise contrastive estimation and projected linear recommendation to deal with popularity bias noise contrastive estimation reweights the useritem interaction matrix and explicitly depopularize item embeddings to scale the recommender system to millions of users projected linear recommendation is implemented which provides a highly scalable closed form solution  we compared the quality of the top items recommended by nceplrec and the current model in production in terms of map mean average precision precision recall and ndcg normalized  ctr clickthrough rate and weekly gmv gross merchandise volume between the two models the ab testing showed a  increase in ctr and an increase in weekly gmv as a result nceplrec provides a better recommendation experience for customers to discover new products and handles the coldstart problem for products with little historical data in the next step wexexxall enhance nceplrec by incorporating side information of products and replenishment behavior of customersloblaw digitalruijian anrichard ,Scott Sanner
dadb48dc-c722-42a8-ac81-2537617fc2e9,application of deep learning algorithm for medical images based on lowdose computed tomography for lung cancer,lowdose computed tomography ldct screening of lung cancer has demonstrated  to  lung cancer mortality reduction hence there are many management protocols for these pulmonary nodules with indeterminate malignant potential from watchful waiting biopsy to surgical detect the pulmonary nodules on the ldct scans and conduct segmentations to facilitate the downstream analysis that includes patient factors for nodule detection we have the data marked by professional radiologists and propose a d nodule detector framework using d unet as the for the future we want to build a platform for radiologists to quickly apply the model in real use and apply new models to the tasklunenfeldtanenbaum research institute sinai healthchenyang lurayjean j,Babak Taati
42184870-2602-4a84-8954-6410a2d543b8,enterprise knowledge mining with multitask selfsupervised graph neural networks,graph neural networks gnns have demonstrated the ability to leverage the structural information in graph relational data for both selfsupervised and semisupervised learning access to this semantic information to produce representations for documents users and topics that occur in enterprise data these representations are trained in a multitask selfsupervised setting to capture generalpurpose information about each node for use in various downstream applications using a custom automatic evaluation setup this report shows that these representations yield higher related topic document and user retrieval metrics than a strong baseline averaging algorithm since representations can be leveraged for additional downstream tasksmicrosoft turinghao zhangjames vuckovi,Sushant Sachdeva
0fdf34d5-116e-4f06-a0b3-eab86a75e201,model compression for transformerbased nlg models,the recent success of largescale transformerbased language models has exacerbated the need for neural network model compression most proposed methods for nlp are not examined on natural language generation nlg models despite the structural similarity transformerbased nlg models tends to be more sensitive to compression than their variants for natural language understanding nlu due to the higher complexity of the task in this project we discover several challenges while applying quantization and pruning for quantization naxcxafve quantizationawaretraining introduce excessive noise to the model and prevent convergence for sparsity unstructured pruning suffers from huge storage overhead while structured pruning results in severely degraded performance we developed a oneshot compression procedure integrating variational quantization sparsity introducing the information of token frequency improves the quality of sparsity pattern we evaluated our method on an internal largescale nlg model in microsoft turing with a structure like gpt the proposed method greatly enhances the compressed performance by approximately  nearly lossless compared to the previous best baseline model with dynamic qat our ongoing works focus on automatizing the design of sparsity patternmicrosoft turingzhumu chenwilliam buchwalter eri,Eyal de Lara
232e2083-51d4-4696-8e90-9c064bd06685,productionready federated learning,federated learning fl is a new and rapidly developing area of private data analysis taking advantage of distributed optimization without any centralized data collection even though the current stateoftheart has already achieved some level of maturity it is still far from being universally applicable in the production setting the major causes of that are high unpredictability have limited ways to ensure highquality result and robustness against outlying possibly malicious model updates  allowing data scientists to experiment with the federated environment and the training process by varying the number of available participants their behaviour and the heterogeneity of their local data the framework can assess the tradeoffs of introducing privacypreserving techniques such as differential privacy and secure multiparty computation as well making use of our framework and the microsoft news dataset mind we have explored the feasibility of training an ar ticle productionready prototype and integrated it into one of the companyxexxas products  we believe data poisoning prevention to be the next step in increasing applicability of fl we are interested in preventing model performance degradation resulting from usersxexxa malicious activity using computational techniques and incentivizationmicrosoft turinggrigory dorodnovjustin harris,Aleksandar Nikolov
d1c27bca-218a-4ea5-9be8-3e24b916fa19,simultuning simultaneous finetuning for contradiction dialogue generation,in recent years we have seen tremendous progress in opendomain dialogue systems through the use of large pretrained transformers however stateoftheart natural language generation nlg models used for dialogue systems still struggle to remain logically consistent on the other hand natural language understanding nlu models fare quite well at identifying contradiction indicating whether the modelxexxas response should contradict the context in this way the model can leverage its understanding of contradiction at the time of generation to reduce the occurrence of contradictory responses we demonstrate that simultuning is an effective approach for both nlu and nlg as we achieve stateoftheart results on the dialogue contradiction detection task decode while maintaining the ability to generate high quality responses we also demonstrate that this method can be used to prompt models to generate responses in a cer tain style as we were able to generate contradictory responses with up to  accuracy judged by humans work is ongoing to determine whether this method can successfully reduce the occurrence of contradiction at the time of generationmicrosoft turingkyle oppenheimeradam atki,Frank Rudzicz
a1c43589-f65b-4155-99b4-b1a983099256,design choices for fast and accurate d face reconstruction,d face reconstruction from a single image is an illposed problem due to the depth ambiguity in the face shape state of the art methods reduce this ambiguity by using priors on the face geometry such as d morphable models dmms  lightweight regression models can be trained to predict the dmm parameters of a face from an image in real time however they require data labelled with facial landmarks or dmm parameters for training which is not easily available nor reliably accurate  other models encode the image to the latent space of face and image features then reconstruct the input image using differentiable renderers and minimize a formulation of photometric loss on the reconstructed image these models can be trained with more unlabelled data and achieve better reconstruction accuracy but they canxexxat provide realtime inference we aim to train a lightweight model using these selfsupervision methods while also providing realtime inference  we have observed that state of the art accuracy is achievable with the regression models next we will compare the accuracy and speed of photo reconstruction models to these results also we will study the effect of training with synthetic data on the performance of these modelsmodifacesaad saleembrendan d,Sven Dickinson
1fa1d7e4-4eb9-4e40-9739-58ce3f992668,hardware aware neural architecture search for semantic image segmentation,hair color simulation is an important augmented reality task at modiface it is fundamentally based deployed on platforms with limited computing capabilities inference speed and performance are key considerations during the model design process current methods involve tediously handcrafting convolutional neural networks to achieve desired metrics in this project we use neural architecture  smaller networks and maintaining performance this contrasts with early nas methods which require repeated training of sampled models for different scenarios thus being computationally prohibitive model performance is measured using the mean intersection over union miou metric in our experiments a baseline model architecture consists of a preset backbone and the same segmentation head as the supermodel on our inhouse hair dataset a searched model with  fewer flops and  fewer parameters than our deployed model almost matches its accuracy  versus  miou cityscapes is a public dataset comprising street scenes from various cities and a popular  of specialized models for different platforms at modifacemodifaceshamitra rohanvic,Lueder Kahrs
4e21ec53-8f2b-48be-ad64-eaa205cf8dc4,learning controllable directions of gan space,generative adversarial networks gans are capable of generating highly realistic images given random inputs from a latent space where meaningful directions and channels have been shown to exist and linearly interpolating in these directions or changing individual channel values result in interpretable transformations eg adding eyeglasses or smile to a personxexxas face however controls obtained by supervised approaches require large amounts of labelled data and are sometimes entangled yet unsupervised settings involve manual examination of many different manipulation  in this work we propose a gradientbased approach that learns controllable directions of gan latent space our approach utilizes the gradient directions of auxiliary networks to control semantics in the latent codes which requires minimal amounts of labelled data with sizes as small as  samples that can be obtained quickly with human supervision gradcam is a form of posthoc attention which visualizes where a convolutional neural network model is looking we propose to select important latent code channels with gradcam based masks during manipulation resulting in more disentangled controls flickrfaceshq ffhq is a benchmark dataset for gans consisting of  highquality png images of human faces at x resolution our qualitative experimental result for manipulating outputs of stylegan trained on the ffhq dataset shows that our approach achieves an overall better performance and disentanglement than the stateoftheart supervised method using support vector machines svms in the remaining terms of research we will focus on quantitative evaluations manipulation of real images using gan inversion and application of our method to a wider range of datasetsmodifacezikun shelly chenirene,Babak Taati
0ad770e1-beee-416a-baf3-2ef79ab9f793,atpm evaluation platform and advanced solutions for autonomous transportation leveraging g networks,autonomous driving has long been favored by most manufacturers and researchers in the automotive industr y on top of traditional selfdriving cars which rely on onboard detecting and computing the technology of connected and autonomous vehicles cav has recently been proposed with the assumption of ultrareliable lowlatency communications however whereas the upcoming g networks have set the stage for cellular vehicletoeverything cellularvx connections cav and autonomous transportation has not yet been widely investigated therefore in this work we build the system named autonomoustransportation planning and management atpm by adopting the cellularvx standard to connect vehicles pedestrians and roadside safety our system brings we also created an evaluation platform based on virtual street simulators which helps test and optimize the transportation planning at low experiment cost the result of a case study we did in toronto showed that our transportation planning increased the highway capacity by  with a reduced collision rate in future work we plan to develop smart apps for mobile devices of drivers pedestrians and all other road users to further enhance mobility and safetynew vision systems canadatao wuho,Bo Wang
a1609441-8d5d-45c5-a661-84702e9b4d38,ganversed single image to d object reconstruction,to infer d properties such as geometry texture and d segmentation map from one single d image datasets in practice therefore these approaches fail to perform well when testing on real images due to the noticeable domain gap between synthetic and real images we utilize the generative adversarial networks gans to generate realisticlooking images for nvidia by analyzing and disentangling the latent space we successfully transform stylegan into a multiview image generator we train the d prediction network using a differentiable renderer and the entire architecture is iteratively trained using cycle consistency losses  we fur ther improve our approach by collecting a new dataset for training stylegan resulting in information into latent space and adapt the architecture of stylegan which allows the stylegan network to generate controllable multiview images  we show that our model can per form well on both real images and synthetic images leveraging the dreconstructing network to another level as an ongoing project we aim to further leverage the from stylegan we also aim to train the pipeline on multiple categoriesnvidiaao tangmasha shugrina clement fuji,Shurui Zhou
8889c756-f6c7-46a6-93d3-5ecc854de5cc,explainable ai applied to pharmaceutical ml model,odaia is a startup dedicated to understanding customer journey from health care professions and enabling pharma sales to dynamically target the next best audiences using the cutting edge machine learning technique odaia makes a trend prediction and thus provides broader customer insights to better understand which customer features eg age region take leading components disassemble the blackbox inside machine learning models on our business scenario  the whole project has two parts  feature importance measures the relative contribution of in terms of drug prescription volume prediction our experimental approaches are trying to answer such question and based on two different methods local feature importance describes how customer features affect the prediction in individual level and gives us a better understanding to a on the whole prediction model we compare those two methods to generalize a conclusion for most important features  counterfactual explanations cf explores how the small change in the customer features can achieve desired prediction outcome for example how much income increase for this customer would lead to double market share prediction by constraining changeable and nonchangeable features we have successfully generated some useful cf examples with different feature settings  in the next step we hope to utilize more robust feature importance methods to further generalize top critical features also we will keep exploring potential business scenario under power ful support from xaiodaiacheng han hsiehzh,Dehan Kong
65bfff16-1d22-4250-8c77-890e3bd7f81d,speech enhancement and recognition with generative adversarial network,we measure the effectiveness of the generative adversarial network gan for noisy speech enhancement and recognition gan can work endtoend with the raw audio waveform as inputs and targets different from the prior work  by evaluating the quality and intelligibility of enhanced audios we investigate the recovery of the word error rate wer lost in the noisy speech recognition context audios and above we conduct the study on an  khz pearson internal audio dataset from nonnative english speakers first we train segan  by directly adding the realworld noise from the demand dataset  to clean audios with various speechtonoise ratios snr as input features and clean audios as targets inspired by  two pytorchkaldi asr systems  are trained model model the model uses both the clean and noisy audios as input for model we use the same audios from model and pass the noisy audios through segan to generate their enhanced version to train the asr system model reduces the wer from  to  compared to the model on the testing noisy speech downstream tasks like scoring at the current stage the model is built on a relatively small dataset h hours of audio more clean and noisy speech together with clipped and low volume audios will be introduced to further improve the asr system performance for bad quality speech in the future  references   s pascual a bonafonte and j serra xefxacxsegan speech enhancement generative adversarial networkxefxacx in interspeech    thiemann joachim ito nobutaka  vincent emmanuel  demand a collection of multichannel recordings of acoustic noise in diverse environments  data set st international congress on acoustics ica  montreal canada zenodo httpsdoiorgzenodo   ravanelli mirco et al xefxacxthe pytorchkaldi speech recognition toolkitxefxacx icassp    ieee international conference on   k kinoshita t ochiai m delcroix and t nakatani xefxacximproving noise robust automatic speech recognition with singlechannel timedomain enhancement networkxefxacx in  icassp  pp xcxpearson canadazibin yangrichar,Gerald Penn
c6773b04-5013-4aa1-b7a4-283912bbabbd,learning tone curves for local image enhancement,xxexxenhancement techniques aim to adjust these factors to achieve more aesthetically pleasing results many datadriven approaches formulate image enhancement as an imagetoimage translation task these methods directly output an edited image providing little insight on what transformations were performed we propose a new method to perform automatic local image enhancement for smartphone cameras instead of learning the output image itself our neural network predicts a grid of transfer functions specifying the mapping from each possible input pixel value to its corresponding output value these transfer functions are known as xefxacxtone curvesxefxacx the predicted tone curves are then applied on corresponding patches of the input image with tilebased interpolation to produce smooth and artifactfree results compared with imagetoimage translation models our model is advantageous in a smartphone camera setting because tone curves are commonly used within image signal processors isps thus can be easily integrated with them experiment results demonstrate that the proposed approach outper forms global image enhancement methods and reaches similar per formance as the stateoftheart local enhancement methods in terms of perceptual metricssamsung ai centreluxi zhaomichael s brown abdelrahman abdelham,Kyros Kutulakos
99fdbe2b-9271-4156-a66b-b54c457a95cf,learningbased obstruction removal,raindrops modern approaches often exploit multiple image frames taken with a slightly moving camera eg cell phone camera to reconstruct a clear obstructionfree version of the scene in the background compared to traditional multiframe approaches learningbased methods require less computationally expensive optimization but they often fail in realworld sequences our fenceremoval work builds on a recent learningbased approach that alternates between estimating dense generation process we fur ther improve the data synthesis pipeline by applying random color and outoffocus blur data augmentation our experimental results show that our model reconstructs clearer background and yields fewer artifacts compared to the pretrained baseline besides obstruction removal methodssamsung ai centreyouheng gealex levinshte,Kyros Kutulakos
af4ce5e8-3dd4-4ac1-8b28-a5a2432d1729,referring expression understanding in navigation,applications of robotics continue to grow rapidly especially with the advancement of machine on the table my glasses are placed above the booksxefxacx it is hard for an ai robot to understand this sequence of instructions because the system needs to ground the words with visual perception problem we treat it as a sequence of image retrieval problems we split a panorama into several image regions and use referring expressions to retrieve each corresponding target region we test two models and their variants with a panorama dataset to obtain the performance of the models further improve the success rate in order to build a fully functional robot assistant that can retrieve objects as requested by a humansamsung ai centreguanxiong liuafsaneh f isma h ran z federic,Frank Rudzicz
641bcd56-abd0-4963-9399-12f327d9168e,web page contextual subcomponents recognition and ,a web page typically contains many subcomponents such as navigation bar advertisement and informative contextual subcomponent is proposed representing a subcomponent that provides useful information to the user such as sentences or paragraphs in the main content the goal of the project is to implement a browser extension which can identify informative contextual web pages the designed extension contains two main features manually highlight and provide recommended highlights there are already some tools such as hypothesis and liner that can help user create highlights manually but there is no existing extension that can recognize informative subcomponents according to the content of the webpage to create recommended highlights to articles documents recipes wikipedia articles and educational articlesthe test result shows that the user can select any sentences on these web pages to create highlights and all recommended highlights created by heuristic rules provide helpful information to the user furthermore the proposed approach will be continuously improved when more and more web pages are involvedscrawlrkexin yanmatija boban jonathan ,Shurui Zhou
72485629-18ac-454a-92b1-17847900317a,structured information extraction in clinical texts using graph model,medical coding is a process of identifying diagnoses and procedures in medical notes and transforming them into universally recognizable alphanumeric codes with highly unstructured medical notes and thousands of possible codes the training of any deep learning algorithms will be noisy in this project we used a knowledge graph approach to improve the recall in this extreme linked with terms in an existing medical ontology to populate the patient graph with relationships provided by the ontology in the end the ontology codes were mapped to disease codes using the ontology native code conversion schema our model achieved  recall on full disease codes and  recall for the root interventions without restricting the label space and was similar to the bertbased baseline however the model did not achieve productionlevel precision leaving room for improvementsemantic healthkai wangmichal maly,Michael Brudno
bda5a0bb-359a-493d-8b5f-a946f839c161,enriched understanding of retail receipts for personalized financial insights,sensibill is offering an agile spend management platform that digitizes retail receipts and invoices through its proprietary optical character recognition ocr technology the extracted attributes from  to improve personalization there needs to be granular categorization of receipt line items the unique challenge lies in the fact that receipt line items are short abbreviated and prone to ocr errors making standardizing line items so that their most comprehensible form is displayed xcx is necessary embeddingbased model tuned for both enrichment objectives item normalization and categorization in this unsupervised approach the model extracts the most normalized form of an item by grouping historical line items from our database into clusters through various pruning criteria that reduce the pruning clusters using an embeddingbased approach in place of text similarity resulted in clusters that measure model performance we developed a set of custom quantitative and qualitative metrics the optimized model demonstrated a boost in item normalization accuracy with no decrease in item categorization accuracy on the benchmark dataset  the optimized model has been deployed in the production environment we are continuing to explore optimization strategies for the line item embedding model to increase the coverage of items enrichedsensibillelisa dudaniel wagnermichael guerzho,"Michael Guerzhoy, Rohan Alexander"
528a129d-20fe-4f38-aa8a-c3dec3dee769,object detection and image segmentation for receipt images,the goal of the project is to extract highquality receipt images from useruploaded photos to increase the accuracy of sensibillxexxas process for extracting purchase information from receipts sensibill provides customer data into actionable insights in order to know and serve their customers better  in this project a new dataset was collected and several neuralnetworkbased methods for receipt extraction were implemented and evaluated we improved the performance over the baseline and  and image segmentation frameworks based on mask rcnn u net and deeplab vextraction methods were based on our test set the metric was the proportion of the photos for which the bounding boxes or image masks were correct the unet model successfully detected receipt objects respectively besides that a future evaluation would be done for aws textract a commercial service that can be used for extracting text from receipts   he k gkioxari g dollxcxar p  girshick r  mask rcnn in proceedings of the ieee international conference on  ronneberger o fischer p  brox t  october unet convolutional networks for biomedical image segmentation in international conference on medical image computing and computerassisted intervention pp  springer cham  chen l c papandreou g kokkinos i murphy k  yuille a l  deeplab semantic image segmentation with deep convolutional nets atrous convolution and fully connected crfs ieee transactions on pattern analysis and machine intelligence    announcing specialized support for extracting data from invoices and receipts using amazontextract  amazon web services amazon web services  httpsawsamazoncomblogsmachinelearningannouncingexpanded supportforextractingdatafrominvoicesandreceiptsusingamazontextractsensibilldanting dongdaniel wagnerlueder kahrs,"Lueder Kahrs, Michael Guerzhoy"
34938033-5963-4d35-bfd6-65215e364024,achieving clinical automation in pediatric emergency medicine with machine learning  medical directives,in emergency departments increased patient volume wait times and long stays impose bad hospitalvisiting experiences to the patients as well as common medical challenges to the hospital in order to provide guidelines for nurses medical directives can request certain diagnostic tests at triage which expedite the process by providing test results to the physician at the initial encounter the goal of this project is to propose a machine learning medical directive approach to autonomously ordering downstream tests such as ultrasounds xrays etc  our solution used the triage testing and diagnosis data from patientsxexxa electronic health records to train the logistic regression random forest gradient boosting and deep neural network models then make predictions we also constructed a retrospective modeltraining pipeline to validate the safety of model performance by performing error analysis and biases assessments in doing so we found the best hyperparameter settings for each downstream test models obtained high areaunderoperatorcurve across each of the use cases the result demonstrated the feasibility of using our approach to achieve clinical automation the next step of the project is to deploy models into a realtime prospective pipeline and propose a selfauditing function to monitor performance metricssickkidsxinqi shendevin singh erik drysd,Yun William Yu
37095493-fcb2-4cda-985e-81f44739623e,hostseq covid data visualization,the cgen hostseq project is funded by the federal government to sequence approximately  canadians infected with sarscov the goal of this project is to build a data visualization tool for cgen hostseq databank data visualization is an active research area which focuses on methodologies and tools eg tableau to deliver the information from the datathe visualization tools referred to as data portals will deliver information to researchers and the public to facilitate understanding of what is available in the hostseq databank and enabling them to design and conduct future research studies the hostseq databank is comprised of both the clinical data and the genetic sequence of each participant therefore the team has implemented two data portals the portals are written using python and flask frame as of october  the clinical data portal has been developed with approximately seventy features available to be visualized and summarized the genetic data portal is still under development but will be comprised of displays and summary statistics from genomewide analysis of sarscov infection severity and other related outcomes displays will include manhattan plots of association statistics quantilequantile displays and others in the current project the backbone of the visualization tool has been developed as more data becomes available we will present more features and include larger sample sizes in the data portals lastly linking the data portals with the multiomics statistical integration tool developed by our team xefxacxlocusfocusxefxacx will enable an improved user experience for visualizing and interpreting the genetic resultssickkidsshikai liulisa st,Michael Brudno
eab5247a-dbf9-47c4-a06d-73efad7625b1,continuous mixout preventing forgetting problem of largescale language models in continual learning,just like the human brain large scale language models also suffer from catastrophic forgetting tasks often forget their pretrained knowledge due to the smaller number of training instances available the combination of this scenario with continual learning where the model learns new knowledge iteratively on a small number of training instances as they become available causes further degradation in per formance to mitigate this catastrophic forgetting problem in lms this paper proposes a new hybrid method where the combination of stochastically mixing the parameters of two models mixout regularization with geometric decay of learning rate through the layers of the model with the adamw optimizer mitigates the forgetting problem the results are shown for the distilbert model across three datasets which suggests that compared to a simple retraining method which results in an accuracy drop anywhere between  to  across various iterations the hybrid method not only prevents these accuracy drops but also increases the baseline accuracy in some cases the mitigation of the forgetting problem by this hybrid method facilitates the future use of large lms for multitask learning as well as continual learning setupssotivarun pandyahossein taghi,Scott Sanner
7f5c716d-7936-40bc-a5dd-ac896469f27c,creating and analyzing the nocode platform for generating restful apis for database manipulation,lowcode and nocode application development platforms lcaps and ncaps improve the ease with which business applications can be delivered requiring smaller and less specialized teams of developers than traditional codebased application development for example soti snap a ncap for crossplatform mobile application development provides a draganddrop interface to reduce the accessibility barrier for users without prior programming experience one of the issues in building a general purpose ncap is the integration of userfriendly data entr y and retrieval with corporate databases in this project we created an ncap crud api tool to create and manage restful api services that allow other platforms to perform create read update and delete crud operations on a wide variety of relational databases prior internal analysis estimated that integrating ncap crud api tool into soti snap will save roughly  of a typical application development time  throughout the project we have done multiple rounds of usability testing prior to the implementation several rounds of focus groups have been conducted within the organization to ensure usability among others participants emphasized the need for documentation ease of use functionality security consistency with related products simple navigation familiar terminology for a nontechnical audience effective onboarding error diagnosis and prevention and visibility of system status then a moderated remote usability study with  participants was conducted on a preliminary version of the user interface aimed to measure the above metrics the study had three phases explorative testing usability testing with simulated tasks and a poststudy questionnaire the analysis of the results is still in progress and we aim to use them to assess the usability of the tool and to suggest areas for improvementsotiyuan chensheldon da,Marsha Chechik
a05f8951-37ff-4862-8e80-8300783d47b7,mislabeled data detection pipeline for snap analytics,snap analytics allows users to perform database queries via natural language inputs to convert natural language to sql queries multiple bertbased models are used to learn the relationship between natural language questions and their corresponding sql queries however the presence of the correct texttosql mapping therefore as an important step towards good model performance my project focuses on ensuring data quality by detecting and removing mislabeled data for this purpose the pipeline detects mislabeled data by combining different metrics based on the modelxexxas training dynamics as well as performing a semantic match between input and label first the data sample additionally a hybrid approach of computational linguistics and universal sentence encoder is applied to look for any semantic match between the data and the label as a result the recall of this approach is  and the precision is    depending on the ratio of mislabeled data this means that the pipeline captures all the mislabeled data and the model can maintain its performance instead of suffering an accuracy drop of    moreover a conversation pipeline is also integrated so that users can interact with the snap analytics system a point for future sotixinming yehossein taghi,Scott Sanner
95054e68-0d9f-47d1-8705-744c66e2e5bb,uav onboard realtime scene ,soti is in researching advanced intelligent visual systems for unmanned aerial vehicles uav uavs distinguishing different scenes during navigation recently convolutional neural network cnn of the methods requires large computing to complete however to deploy on a uav a realtime method that is based on a compact model is desired  case the preparation and undergo training has been upgraded to a customized data augmentation pipeline subsequently the new method achieved a better testing result compared to the current deployed on the nvidia jetson xavier nx board  the next step will be optimizing and integrate this method into sotixexxas current uav vision system to further consolidate the models one potential approach is by using a transfer learningbased sotiyunze peter pandmitry sheste,Anthony Bonner
bade1be2-6c85-4259-9e24-0d775822e59c,of surgical videos,computerassisted surgery cas systems have been growing in impact in modern surgery such systems provide automated service to surgical education surgeon performance evaluation manually in the past which is timeconsuming people tried to automate the process but the proposed methods did not investigate the generalization over different surgery types to address the which learns the mapping from the surgical videos to the corresponding surgical steps furthermore we utilized the domain knowledge to further improve the prediction our method not only yields stateoftheart performance on a common surgical dataset known as cholec but also shows great generalization over multiple surgical types we deployed this model to amazon web services aws using docker containers which automate the process from uploading the videos to visualizing the prediction our current pipeline took a long time to process the whole video thus as a next step we surgical safety technologiesyi xiangfrank rudziczma,Maryam Mehri Dehnavi
efb7c339-c879-4f2b-92b3-af9de323ca97,anomaly detection and data using snorkel and autolabeling,customers rely on our data to help them discover new suppliers and to meet diversity and esg environmental social governance goals  most of our data is sourced from unstructured text through crawling the internet because of the is an excellent tool for webscale information extraction and anomaly detection but typically requires large amounts of welllabeled training data the cost of obtaining sizable humanlabeled datasets is frequently the biggest barrier to adoption of machine learning methods in industries  pipeline using snorkel an autolabeling framework developed at stanford snorkel helps to drastically lower the cost of labeling training data through the use of labeling functions and a learned label model in our experiments we used snorkel to create an autolabeled dataset that is x bigger than our handlabeled training dataset in pairing a large autolabeled training dataset with a pretrained language tealbookmarshall hoarnold liw,Mark Chignell
c5e3276d-051d-4061-a6f2-aaf4d3a6c9b6,alphaportfolio largescale automatic portfolio construction using reinforcement learning,portfolio management driven by machine learning has regained huge interest in the recent years followed by a plethora of success in many areas where machine learning excels eg alphago gpt etc following the latest trend in both academia and industry we propose a general framework for dynamic portfolio construction based on deep reinforcement learning drl our approach contrasts the traditional way of constructing a portfolio consolidating the twostep process that involves both estimation of assets returns and optimization of assets weights into a single endtoend pipeline our drl framework handles different portfolio construction needs by leveraging three functionality modules  a module for achieving excess alpha by maximizing the information ratio  a module for mitigating downside risk through maximizing maximum drawdownadjusted unique stocks and over  million records and the model is able to autoselect stocks from the metrics for each module we also perform perturbation analysis and feature selection using lasso and random forest which reveal the driving factors that contribute to each portfolio construction goal our analysis improves the interpretability of the drl models and provides novel insights to traditional factor investingthe vanguard group incruiwen tina wangjithin pradeep jerry chens,Sebastian Jaimungal
a0b95623-2694-4c0b-baf8-55e0c5edc401,controllable text generation with reweighed diverse beam search,recent advances in natural language processing have led to the availability of large pretrained language models lm with rich generative capabilities although these models are able to produce a majority of current approaches in controllable text generation attempt to select the best input subset of the lmxexxas parameters these methods suffer from a lack of interpretability inconsistency in generations and a lack of generalization to lowresource domains  output distribution of a lm with no changes to the original model we propose a new extension of beam search whereby an auxiliary attribute model is used to reweigh hypotheses provided by results support the hypothesis that a similar or better level of control can be achieved without the auxiliary attribute model improving the generalizability to lowresource domains moreover our approach offers an interpretable and transparent method for controllable generation as it operates directly on natural language hypotheses future work can explore multidimensional attribute control  the vanguard group incdavid landsmanhussain,Gerald Penn
d23dc0b3-dd05-4bca-bf8c-75044dc5f823,setc sentence and entity type clustering for unsupervised open relation extraction,with the abundance of unlabelled text available there is a need for techniques to extract meaningful information from these texts unsupervised open relation extraction uore is the process of extracting opendomain relations between named entities from a corpus of text without the use of labelled data or knowledge bases current stateoftheart unsupervised open relation extraction techniques either do not use entity types as input explicitly or fail to provide meaningful contextual embeddings that can supplement entity types  in this paper we propose an unsupervised open relation extraction technique sentence and entity type clustering setc that uses kmeans clustering on the embeddings produced by our sentence and entity type set encoder our method utilizes both entity type and entity context information results on a large benchmark dataset show that setc outperforms other stateoftheart approaches in terms of clustering metrics bprecision recall f using the majority relation of each cluster  that outperforms stateoftheart models while also utilizing both entity type and entity context its applications extend beyond unsupervised open relation extraction to other nlp tasks such as information retrieval knowledge base construction and natural language understandingthe vanguard group incstephen brockjerrod parker shi yu,En-Shiun Annie Lee
aeaf9e79-0075-4a2b-9ecb-16f286a0d365,realtime big data visualization pipeline with apache flink and realtime data warehouse,data had become the most valuable resource of commercial businesses as a company with a growing business timeplay was struggling on processing large amounts of user data in realtime processing tool that required more than  minutes to process the data generated by a show the goal of this research project was to help timeplay replace this tool with a realtime data visualization pipeline by reviewing and comparing existing methods this project compared a traditional datawarehousebased approach with an experimental realtime visualization approach called i as well as their associated data middleware including apache flink clickhouse amazontimestream and timescaledb the results showed that i per formed a little better than the traditional approach in terms of eventtime latency but that the traditional approach could provide users with a better of ecosystem and ease of use for timeplay the traditional approach appeared to be a better solution further investigation and additional software infrastructure were needed for applying i and other similar approaches in a production environmenttimeplaylinzhi fengfrancoismichel sch,Nick Koudas
2369ffe8-ec67-4d2d-b824-bf87a29e9d1c,datadriven approach to disorders and cognitive impairment from noisy data,acoustic and linguistic features are prominent markers of both mental disorders and cognitive symptomsthe goal of this project is to characterize these diseases by automatically identifying their symptom components as an aggregation of features derived from spoken language and associating those with the clinical symptoms to retrieve the datadriven symptom components we make use of unsupervised learning methods that include both dimensionality reduction and clustering to explore linear approaches we initially applied pca principal component analysis over an aggregated dataset given by picturedescription task consisting of subjects with ad alzheimerxexxas disease mci mild cognitive impairment depression and hc healthy control diagnosis labels additionally we used nonlinear dimensionality reduction techniques including umap uniform manifold approximation and projection and tsne tdistributed stochastic neighbor embedding as well as another linear method of lda linear discriminant analysis over the same dataset to compare their performance in terms of characterizing the diseases as well as separability level of distinct diagnosis labels after dimensionality reduction with pca  major symptom components appeared with a  total explained variance ratio in addition among all dimension reduction methods umap indicated the best performance according to its silhouette score after kmeans clustering as well as the optimal number of clusters which was the same as the number of disease labels next steps are to  work on the interpretability of the components by lime across different dimension reduction techniques to extract highly relevant features aggregated in each component  augment the dataset with various audio augmentation techniques to upsample minor classes and rerun the dimension reduction techniqueswinterlight labsmalikeh ehghaghijekaterina novikovafrank rudzi,"Frank Rudzicz, Andrei Badescu"
a2d8cabe-233d-4391-b645-e0e06bbc802d,fast biomechanical simulation with an iterative solver,anatomical tissue simulation is one of the most challenging areas within the scope of computer video game development and biomedical research zivaxexxas current tissue simulation algorithm is robust and accurate but not particularly fast the primary goal of this research project is to improve the speed of the simulator which will accelerate the iteration cycle for character artists to turn their from the direct solver because inverting the system or matrix decomposition is computationally expensivethe proposed method is to switch from the direct solver into iterative solvers the new algorithm starts with an initial guess and then iteratively moves towards the optimal until reaching meanwhile achieving the accurate solution thus preserving the original animation effects since sparse matrixvector multiplication is the most needed operation for iterative solvers the next goal conditioned input systems as the number of iterations required for convergence highly depends on the system condition numberziva dynamicsyue licrawford,David Levin
